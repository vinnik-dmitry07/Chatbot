{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPM10P3bMe3NV3h9Dtp76Qh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinnik-dmitry07/Chatbot/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rY3EisQ766aP",
        "outputId": "0cf8334a-0ef4-4fb9-f630-bf6ff317e93b"
      },
      "source": [
        "!pip install --quiet parlai"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.4MB 5.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 37.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 34.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 317kB 54.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 9.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.0MB 22.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.2MB 59.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 64.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 11.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 58.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 54.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 58.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 54.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.8MB 58.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 40kB 217kB/s \n",
            "\u001b[K     |████████████████████████████████| 552kB 57.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 143kB 62.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 11.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 12.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 11.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 12.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.8MB/s \n",
            "\u001b[?25h  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docformatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for websocket-server (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for untokenize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: torchtext 0.8.1 has requirement torch==1.7.1, but you'll have torch 1.7.0+cu101 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: requests 2.23.0 has requirement urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: myst-parser 0.13.3 has requirement sphinx<4,>=2, but you'll have sphinx 1.8.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fvcore 0.1.3.post20210204 has requirement pyyaml>=5.1, but you'll have pyyaml 3.13 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: sphinx-autodoc-typehints 1.11.1 has requirement Sphinx>=3.0, but you'll have sphinx 1.8.5 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrUpm9bM7rb0",
        "outputId": "9c8a5f05-d086-4e0f-ae24-6c1e9941e67d"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Feb 12 14:22:53 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vntU8btYNpcW",
        "outputId": "a9200834-8bb2-4722-9428-dcb5b24d7e5d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fr8wrt68ATZl",
        "outputId": "d6cd1231-7fc3-466a-a019-942e0660eaf0"
      },
      "source": [
        "from google.colab import drive\r\n",
        "\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCM-AVZhHKFK"
      },
      "source": [
        "import json\r\n",
        "\r\n",
        "with open('/content/drive/MyDrive/result.json', 'r', encoding='utf8') as f:\r\n",
        "    raw_messages = json.load(f)['messages']"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3CvK47rIujd"
      },
      "source": [
        "from datetime import datetime\r\n",
        "\r\n",
        "filtered_messages = []\r\n",
        "for msg in raw_messages:\r\n",
        "    if (\r\n",
        "            'from' in msg and\r\n",
        "            'from_id' in msg and\r\n",
        "            'mime_type' not in msg and\r\n",
        "            msg['text'] and\r\n",
        "            isinstance(msg['text'], str)\r\n",
        "    ):\r\n",
        "        msg['date'] = datetime.strptime(msg['date'], '%Y-%m-%dT%H:%M:%S')\r\n",
        "        filtered_messages.append(msg)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz06qsAJJLda"
      },
      "source": [
        "from datetime import timedelta\r\n",
        "\r\n",
        "episode_dt = timedelta(minutes=5)\r\n",
        "joined_messages = [filtered_messages[0]]\r\n",
        "for i in range(1, len(filtered_messages)):\r\n",
        "    if (\r\n",
        "            filtered_messages[i - 1]['from_id'] == filtered_messages[i]['from_id'] and\r\n",
        "            filtered_messages[i - 1]['date'] - filtered_messages[i]['date'] <= episode_dt\r\n",
        "    ):\r\n",
        "        joined_messages[-1]['text'] += ' ' + filtered_messages[i]['text']\r\n",
        "    else:\r\n",
        "        joined_messages.append(filtered_messages[i])"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUphzoX3K_FF"
      },
      "source": [
        "def partition(alist, indices):\r\n",
        "    return [alist[a:b] for a, b in zip([0] + indices, indices + [None])]\r\n",
        "\r\n",
        "time_diffs = [joined_messages[i + 1]['date'] - joined_messages[i]['date'] for i in range(len(joined_messages) - 1)]\r\n",
        "split_positions = [i + 1 for i in range(len(time_diffs)) if time_diffs[i] > episode_dt]\r\n",
        "episodes = partition(joined_messages, split_positions)\r\n",
        "assert len(split_positions) >= 3, 'Not enough episodes, try decrease \"episode_dt\"'"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXMAHZTxL-KN"
      },
      "source": [
        "def save_jsonl(episodes_part, suffix, human_readable=False):\r\n",
        "    with open(f'data_{suffix}.jsonl', 'w', **({'encoding': 'utf8'} if human_readable else {})) as outfile:\r\n",
        "        for episode in episodes_part:\r\n",
        "            dialog = [\r\n",
        "                {\r\n",
        "                    'id': i % 2,\r\n",
        "                    'text': msg['text'].replace('\\n', ' '),\r\n",
        "                } for i, msg in enumerate(episode)\r\n",
        "            ]\r\n",
        "\r\n",
        "            episode = {'dialog': [dialog]}\r\n",
        "            json.dump(episode, outfile, **({'ensure_ascii': False} if human_readable else {}))\r\n",
        "            outfile.write('\\n')"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApxWsw2omwax",
        "outputId": "9ce5ba5e-1142-44ac-e678-1a2f4f3f83d3"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "train_part, test_part, valid_part = 0.9, 0.05, 0.05\r\n",
        "assert train_part + test_part + valid_part == 1\r\n",
        "\r\n",
        "train, test, valid = np.split(episodes, [\r\n",
        "    int(train_part * len(episodes)),\r\n",
        "    int((train_part + test_part) * len(episodes)),\r\n",
        "])\r\n",
        "\r\n",
        "save_jsonl(train, 'train')\r\n",
        "save_jsonl(test, 'test')\r\n",
        "save_jsonl(valid, 'valid')\r\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HWEdG7hMboN",
        "outputId": "bcdff31d-963d-4d1c-e32d-4afdc8100349"
      },
      "source": [
        "!rm -rf from_pretrained\r\n",
        "!mkdir -p from_pretrained\r\n",
        "\r\n",
        "from parlai.scripts.train_model import TrainModel\r\n",
        "\r\n",
        "TrainModel.main(\r\n",
        "    task='jsonfile',\r\n",
        "    jsonfile_datapath='data',\r\n",
        "    jsonfile_datatype_extension=True,\r\n",
        "\r\n",
        "    model='transformer/generator',\r\n",
        "    model_file='from_pretrained/model',\r\n",
        "    \r\n",
        "    init_model='zoo:tutorial_transformer_generator/model',\r\n",
        "\r\n",
        "    n_heads=16, n_layers=8, n_positions=512, text_truncate=512,\r\n",
        "    label_truncate=128, ffn_size=2048, embedding_size=512,\r\n",
        "    activation='gelu', variant='xlm',\r\n",
        "    dict_lower=True, dict_tokenizer='bpe',\r\n",
        "    dict_file='zoo:tutorial_transformer_generator/model.dict',\r\n",
        "    learn_positional_embeddings=True,\r\n",
        "    \r\n",
        "    lr=1e-5, optimizer='adam',\r\n",
        "    warmup_updates=100,\r\n",
        "    validation_metric='ppl',\r\n",
        "    validation_every_n_secs=60 * 60,\r\n",
        "    save_every_n_secs=10 * 60,\r\n",
        "    \r\n",
        "    batchsize=12, fp16=True, fp16_impl='mem_efficient',\r\n",
        "    \r\n",
        "    skip_generation=True,\r\n",
        "    \r\n",
        "    dynamic_batching='full',\r\n",
        "\r\n",
        "    label_turns='both',\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14:39:11 | building dictionary first...\n",
            "14:39:11 | No model with opt yet at: from_pretrained/model(.opt)\n",
            "14:39:11 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: full,datapath: /usr/local/lib/python3.6/dist-packages/data,tensorboard_logdir: None,jsonfile_datapath: data,jsonfile_datatype_extension: True,label_turns: both,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,beam_block_full_context: True,beam_length_penalty: 0.65,topk: 10,topp: 0.9,beam_delay: 30,beam_block_list_filename: None,temperature: 1.0,compute_tokenized_bleu: False,interactive_mode: False,fp16_impl: mem_efficient,force_fp16_tokens: False,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,hf_skip_special_tokens: True,max_lr_steps: -1,invsqrt_lr_decay_gamma: -1,parlai_home: /usr/local/lib/python3.6/dist-packages\u001b[0m\n",
            "14:39:11 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--show-advanced-args False --task internal:new_reddit:presorted --datatype train:stream --numthreads 1 --batchsize 48 --num-epochs 5.0 --validation-every-n-secs 1800.0 --save-every-n-secs -1 --save-after-valid True --validation-max-exs 9920 --short-final-eval True --validation-patience 0 --validation-metric-mode min --dict-build-first True --numworkers 4 --pytorch-preprocess False --pytorch-teacher-batch-sort False --batch-sort-cache-type pop --batch-length-range 5 --shuffle False --batch-sort-field text --pytorch-context-length -1 --pytorch-include-labels True --log-every-n-secs 30.0 --distributed-world-size 64 --verbose False --port 61337 --dropout 0.1 --beam-size 8 --beam-min-n-best 3 --beam-min-length 10 --skip-generation False --inference beam --optimizer fused_adam --learningrate 0.0005 --gradient-clip 10.0 --adam-eps 1e-06 --betas 0.9,0.98 --weight-decay 0.01 --lr-scheduler invsqrt --warmup-updates 20000 --gpu 0 --beam-block-ngram 3 --beam-context-block-ngram 3\u001b[0m\n",
            "14:39:11 | Using CUDA\n",
            "14:39:11 | loading dictionary from /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "14:39:11 | num words = 54944\n",
            "14:39:12 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "14:39:12 | Loading existing model params from /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "14:39:20 | \u001b[33mNot loading optim state since optim class changed.\u001b[0m\n",
            "14:39:21 | Opt:\n",
            "14:39:21 |     activation: gelu\n",
            "14:39:21 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "14:39:21 |     adam_eps: 1e-08\n",
            "14:39:21 |     add_p1_after_newln: False\n",
            "14:39:21 |     aggregate_micro: False\n",
            "14:39:21 |     allow_missing_init_opts: False\n",
            "14:39:21 |     attention_dropout: 0.0\n",
            "14:39:21 |     batchsize: 12\n",
            "14:39:21 |     beam_block_full_context: True\n",
            "14:39:21 |     beam_block_list_filename: None\n",
            "14:39:21 |     beam_block_ngram: -1\n",
            "14:39:21 |     beam_context_block_ngram: -1\n",
            "14:39:21 |     beam_delay: 30\n",
            "14:39:21 |     beam_length_penalty: 0.65\n",
            "14:39:21 |     beam_min_length: 1\n",
            "14:39:21 |     beam_size: 1\n",
            "14:39:21 |     betas: '(0.9, 0.999)'\n",
            "14:39:21 |     bpe_add_prefix_space: None\n",
            "14:39:21 |     bpe_debug: False\n",
            "14:39:21 |     bpe_merge: None\n",
            "14:39:21 |     bpe_vocab: None\n",
            "14:39:21 |     compute_tokenized_bleu: False\n",
            "14:39:21 |     datapath: /usr/local/lib/python3.6/dist-packages/data\n",
            "14:39:21 |     datatype: train\n",
            "14:39:21 |     delimiter: '\\n'\n",
            "14:39:21 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "14:39:21 |     dict_endtoken: __end__\n",
            "14:39:21 |     dict_file: /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "14:39:21 |     dict_include_test: False\n",
            "14:39:21 |     dict_include_valid: False\n",
            "14:39:21 |     dict_initpath: None\n",
            "14:39:21 |     dict_language: english\n",
            "14:39:21 |     dict_loaded: True\n",
            "14:39:21 |     dict_lower: True\n",
            "14:39:21 |     dict_max_ngram_size: -1\n",
            "14:39:21 |     dict_maxexs: -1\n",
            "14:39:21 |     dict_maxtokens: -1\n",
            "14:39:21 |     dict_minfreq: 0\n",
            "14:39:21 |     dict_nulltoken: __null__\n",
            "14:39:21 |     dict_starttoken: __start__\n",
            "14:39:21 |     dict_textfields: text,labels\n",
            "14:39:21 |     dict_tokenizer: bpe\n",
            "14:39:21 |     dict_unktoken: __unk__\n",
            "14:39:21 |     display_examples: False\n",
            "14:39:21 |     download_path: None\n",
            "14:39:21 |     dropout: 0.0\n",
            "14:39:21 |     dynamic_batching: full\n",
            "14:39:21 |     embedding_projection: random\n",
            "14:39:21 |     embedding_size: 512\n",
            "14:39:21 |     embedding_type: random\n",
            "14:39:21 |     embeddings_scale: True\n",
            "14:39:21 |     eval_batchsize: None\n",
            "14:39:21 |     evaltask: None\n",
            "14:39:21 |     ffn_size: 2048\n",
            "14:39:21 |     force_fp16_tokens: False\n",
            "14:39:21 |     fp16: True\n",
            "14:39:21 |     fp16_impl: mem_efficient\n",
            "14:39:21 |     gpu: -1\n",
            "14:39:21 |     gradient_clip: 0.1\n",
            "14:39:21 |     hf_skip_special_tokens: True\n",
            "14:39:21 |     hide_labels: False\n",
            "14:39:21 |     history_add_global_end_token: None\n",
            "14:39:21 |     history_reversed: False\n",
            "14:39:21 |     history_size: -1\n",
            "14:39:21 |     image_cropsize: 224\n",
            "14:39:21 |     image_mode: raw\n",
            "14:39:21 |     image_size: 256\n",
            "14:39:21 |     inference: greedy\n",
            "14:39:21 |     init_model: /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "14:39:21 |     init_opt: None\n",
            "14:39:21 |     interactive_mode: False\n",
            "14:39:21 |     invsqrt_lr_decay_gamma: -1\n",
            "14:39:21 |     jsonfile_datapath: data\n",
            "14:39:21 |     jsonfile_datatype_extension: True\n",
            "14:39:21 |     label_truncate: 128\n",
            "14:39:21 |     label_turns: both\n",
            "14:39:21 |     learn_positional_embeddings: True\n",
            "14:39:21 |     learningrate: 1e-05\n",
            "14:39:21 |     load_from_checkpoint: True\n",
            "14:39:21 |     log_every_n_secs: 10\n",
            "14:39:21 |     loglevel: info\n",
            "14:39:21 |     lr_scheduler: reduceonplateau\n",
            "14:39:21 |     lr_scheduler_decay: 0.5\n",
            "14:39:21 |     lr_scheduler_patience: 3\n",
            "14:39:21 |     max_lr_steps: -1\n",
            "14:39:21 |     max_train_time: -1\n",
            "14:39:21 |     metrics: default\n",
            "14:39:21 |     model: transformer/generator\n",
            "14:39:21 |     model_file: from_pretrained/model\n",
            "14:39:21 |     model_parallel: False\n",
            "14:39:21 |     momentum: 0\n",
            "14:39:21 |     multitask_weights: [1]\n",
            "14:39:21 |     n_decoder_layers: -1\n",
            "14:39:21 |     n_encoder_layers: -1\n",
            "14:39:21 |     n_heads: 16\n",
            "14:39:21 |     n_layers: 8\n",
            "14:39:21 |     n_positions: 512\n",
            "14:39:21 |     n_segments: 0\n",
            "14:39:21 |     nesterov: True\n",
            "14:39:21 |     no_cuda: False\n",
            "14:39:21 |     num_epochs: -1\n",
            "14:39:21 |     nus: (0.7,)\n",
            "14:39:21 |     optimizer: mem_eff_adam\n",
            "14:39:21 |     output_scaling: 1.0\n",
            "14:39:21 |     override: \"{'task': 'jsonfile', 'jsonfile_datapath': 'data', 'jsonfile_datatype_extension': True, 'model': 'transformer/generator', 'model_file': 'from_pretrained/model', 'init_model': 'zoo:tutorial_transformer_generator/model', 'n_heads': 16, 'n_layers': 8, 'n_positions': 512, 'text_truncate': 512, 'label_truncate': 128, 'ffn_size': 2048, 'embedding_size': 512, 'activation': 'gelu', 'variant': 'xlm', 'dict_lower': True, 'dict_tokenizer': 'bpe', 'dict_file': '/usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model.dict', 'learn_positional_embeddings': True, 'learningrate': 1e-05, 'optimizer': 'adam', 'warmup_updates': 100, 'validation_metric': 'ppl', 'validation_every_n_secs': 3600.0, 'save_every_n_secs': 600.0, 'batchsize': 12, 'fp16': True, 'fp16_impl': 'mem_efficient', 'skip_generation': True, 'dynamic_batching': 'full', 'label_turns': 'both'}\"\n",
            "14:39:21 |     parlai_home: /usr/local/lib/python3.6/dist-packages\n",
            "14:39:21 |     person_tokens: False\n",
            "14:39:21 |     rank_candidates: False\n",
            "14:39:21 |     relu_dropout: 0.0\n",
            "14:39:21 |     save_after_valid: False\n",
            "14:39:21 |     save_every_n_secs: 600.0\n",
            "14:39:21 |     share_word_embeddings: True\n",
            "14:39:21 |     short_final_eval: False\n",
            "14:39:21 |     skip_generation: True\n",
            "14:39:21 |     special_tok_lst: None\n",
            "14:39:21 |     split_lines: False\n",
            "14:39:21 |     starttime: Feb12_14-39\n",
            "14:39:21 |     task: jsonfile\n",
            "14:39:21 |     temperature: 1.0\n",
            "14:39:21 |     tensorboard_log: False\n",
            "14:39:21 |     tensorboard_logdir: None\n",
            "14:39:21 |     text_truncate: 512\n",
            "14:39:21 |     topk: 10\n",
            "14:39:21 |     topp: 0.9\n",
            "14:39:21 |     truncate: -1\n",
            "14:39:21 |     update_freq: 1\n",
            "14:39:21 |     use_reply: label\n",
            "14:39:21 |     validation_cutoff: 1.0\n",
            "14:39:21 |     validation_every_n_epochs: -1\n",
            "14:39:21 |     validation_every_n_secs: 3600.0\n",
            "14:39:21 |     validation_max_exs: -1\n",
            "14:39:21 |     validation_metric: ppl\n",
            "14:39:21 |     validation_metric_mode: None\n",
            "14:39:21 |     validation_patience: 10\n",
            "14:39:21 |     validation_share_agent: False\n",
            "14:39:21 |     variant: xlm\n",
            "14:39:21 |     warmup_rate: 0.0001\n",
            "14:39:21 |     warmup_updates: 100\n",
            "14:39:21 |     weight_decay: None\n",
            "14:39:21 | creating task(s): jsonfile\n",
            "14:39:21 | [loading data from json file into task:data_train.jsonl]\n",
            "14:39:24 | \u001b[31mMetadata does not exist. Please double check your datapath.\u001b[0m\n",
            "14:39:27 | training...\n",
            "14:39:28 | Overflow: setting loss scale to 65536.0\n",
            "14:39:28 | Overflow: setting loss scale to 32768.0\n",
            "14:39:29 | Overflow: setting loss scale to 16384.0\n",
            "14:39:38 | time:10s total_exs:3772 epochs:0.01\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .8421  3108  5770 368.5 3772             19833  13.16    .5413 4.008 1.901e-06  1048  1946 55.05      .3111   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                     19 4156 7716 1.857\n",
            "\n",
            "14:39:48 | time:21s total_exs:6132 epochs:0.01\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  4704 11142 232.9 2360             16384  15.18    .5441 3.769 4.301e-06 554.8  1314 43.35      .3514   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                     43 5258 12456 2.369\n",
            "\n",
            "14:39:58 | time:31s total_exs:8060 epochs:0.01\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss      lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5631 13183 188.1 1928             16384  15.34    .4483 3.687 6.7e-06 518.7  1214 39.94      .3550   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                     67 6150 14398 2.341\n",
            "\n",
            "14:40:08 | time:41s total_exs:9780 epochs:0.01\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss      lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5702 13399 168.4 1720             16384  15.08    .4253 3.753 9.1e-06 443.3  1042 42.65      .3317   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                     91 6145 14441 2.35\n",
            "\n",
            "14:40:18 | time:51s total_exs:11080 epochs:0.02\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6406 15868 128.8 1300             16384   16.7    .3711 3.769 1e-05 327.5 811.3 43.35      .3311   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    116 6733 16679 2.477\n",
            "\n",
            "14:40:29 | time:62s total_exs:12336 epochs:0.02\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5936 13157   121 1256             16384  14.77    .5599 3.497 1e-05 346.5   768 33.03      .3614   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    139 6283 13925 2.216\n",
            "\n",
            "14:40:39 | time:72s total_exs:13644 epochs:0.02\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5912 13706 126.3 1308             16384  13.78    .5600 3.583 1e-05 343.1 795.5 35.99      .3524   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    163 6255 14501 2.319\n",
            "\n",
            "14:40:48 | Overflow: setting loss scale to 16384.0\n",
            "14:40:49 | time:82s total_exs:14672 epochs:0.02\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9583  6541 15562 101.9 1028             16384  14.42    .3900  3.62 1e-05 263.5 626.9 37.33      .3441   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    187 6804 16189 2.379\n",
            "\n",
            "14:40:59 | time:92s total_exs:15804 epochs:0.02\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5890 13368 111.7 1132             16384  13.46    .4309 3.408 1e-05 285.2 647.1 30.19      .3574   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    210 6176 14015 2.269\n",
            "\n",
            "14:41:10 | time:102s total_exs:16828 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6346 15162 101.9 1024             16384  15.61    .3795 3.519 1e-05   256 611.7 33.76      .3595   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    234 6602 15774 2.39\n",
            "\n",
            "14:41:20 | time:112s total_exs:17800 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6547 15730  97.3  972             16384  16.15    .4276 3.342 1e-05 207.8 499.1 28.29      .3566   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    258 6755 16229 2.403\n",
            "\n",
            "14:41:30 | time:123s total_exs:18668 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6247 14646 84.79  868             16384  16.12    .4424 3.546 1e-05 230.2 539.6 34.68      .3507   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    282 6477 15186 2.345\n",
            "\n",
            "14:41:36 | Overflow: setting loss scale to 16384.0\n",
            "14:41:40 | time:133s total_exs:19556 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9583  6205 14817 88.35  888             16384  14.74    .4424 3.508 1e-05 230.2 549.7 33.39      .3455   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    306 6436 15367 2.388\n",
            "\n",
            "14:41:50 | time:143s total_exs:20400 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6340 14740 81.76  844             16384  15.99    .6072 3.293 1e-05 217.3 505.2 26.93      .3724   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    330 6557 15245 2.325\n",
            "\n",
            "14:42:01 | Overflow: setting loss scale to 16384.0\n",
            "14:42:01 | time:153s total_exs:21084 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9583  6700 15534 66.08  684             16384  17.59    .4116 3.694 1e-05   214 496.1 40.22      .3346   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    354 6914 16031 2.319\n",
            "\n",
            "14:42:11 | time:164s total_exs:21848 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6614 15792    76  764             16384  17.87    .4242 3.452 1e-05 169.4 404.5 31.55      .3679   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    378 6783 16196 2.388\n",
            "\n",
            "14:42:21 | time:174s total_exs:22704 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6032 14256 84.29  856             16384  17.95    .3939 3.492 1e-05 210.6 497.8 32.84      .3577   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    402 6242 14754 2.364\n",
            "\n",
            "14:42:25 | Overflow: setting loss scale to 16384.0\n",
            "14:42:31 | time:184s total_exs:23416 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9583  6323 14740 69.16  712             16384  16.07    .4251 3.415 1e-05 194.2 452.7 30.41      .3648   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    426 6517 15193 2.331\n",
            "\n",
            "14:42:40 | Overflow: setting loss scale to 16384.0\n",
            "14:42:41 | time:194s total_exs:24168 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9583  6166 14433 73.34  752             16384  16.51    .4072 3.201 1e-05 169.8 397.5 24.55      .3867   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    450 6336 14830 2.341\n",
            "\n",
            "14:42:52 | time:205s total_exs:25020 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5964 13401 83.24  852             16384   15.3    .4188 3.403 1e-05 237.6 533.8 30.05      .3563   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    473 6201 13935 2.247\n",
            "\n",
            "14:43:02 | time:215s total_exs:25736 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6733 15722 69.66  716             16384  16.17    .3888 3.276 1e-05 151.2 353.1 26.48      .3764   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    497 6884 16075 2.335\n",
            "\n",
            "14:43:12 | time:225s total_exs:26500 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5941 13662 73.21  764             16384  16.12    .4169 3.286 1e-05 183.2 421.2 26.75      .3678   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    521 6124 14083  2.3\n",
            "\n",
            "14:43:17 | Overflow: setting loss scale to 16384.0\n",
            "14:43:17 | Overflow: setting loss scale to 16384.0\n",
            "14:43:20 | Overflow: setting loss scale to 16384.0\n",
            "14:43:23 | time:236s total_exs:27140 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "   .8750  6569 15388 62.46  640             16384  15.29    .4242 3.353 1e-05 164.7 385.7 28.6      .3674   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    545 6734 15774 2.343\n",
            "\n",
            "14:43:33 | time:246s total_exs:27796 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6128 14425 64.34  656             16384   16.6    .4116 3.199 1e-05   179 421.5 24.5      .3793   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    569 6307 14846 2.354\n",
            "\n",
            "14:43:40 | Overflow: setting loss scale to 16384.0\n",
            "14:43:43 | time:256s total_exs:28412 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9565  6459 14718 61.03  616             16384  16.43    .4335 3.374 1e-05   176 401.1 29.18      .3685   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    592 6635 15119 2.279\n",
            "\n",
            "14:43:53 | time:266s total_exs:29064 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6524 15177  63.2  652             16384  17.02    .4116 3.283 1e-05 142.9 332.5 26.65      .3758   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    616 6667 15510 2.326\n",
            "\n",
            "14:43:55 | Overflow: setting loss scale to 16384.0\n",
            "14:43:56 | Overflow: setting loss scale to 16384.0\n",
            "14:44:01 | Overflow: setting loss scale to 16384.0\n",
            "14:44:04 | time:277s total_exs:29696 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .8750  6463 15135 61.66  632             16384  15.64    .4040 3.238 1e-05 150.3   352 25.49      .3689   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    640 6613 15487 2.342\n",
            "\n",
            "14:44:14 | time:287s total_exs:30224 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6835 15676 52.65  528             16384  19.15    .4344 3.647 1e-05 153.6 352.2 38.35      .3364   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    663 6989 16028 2.294\n",
            "\n",
            "14:44:24 | time:297s total_exs:30840 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6407 15049 60.28  616             16384  17.27    .4072 3.243 1e-05 142.8 335.3 25.62      .3704   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    687 6550 15384 2.349\n",
            "\n",
            "14:44:34 | time:307s total_exs:31444 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6442 15022 58.68  604             16384  17.02    .4126 3.374 1e-05 150.6 351.2 29.2      .3485   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    711 6593 15373 2.332\n",
            "\n",
            "14:44:36 | Overflow: setting loss scale to 16384.0\n",
            "14:44:40 | Overflow: setting loss scale to 16384.0\n",
            "14:44:44 | time:317s total_exs:31940 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9130  6703 15291 49.19  496             16384  17.47    .4265 3.513 1e-05 139.5 318.3 33.56      .3524   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    734 6843 15609 2.281\n",
            "\n",
            "14:44:46 | Overflow: setting loss scale to 16384.0\n",
            "14:44:55 | time:327s total_exs:32440 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9583  6491 15416 49.47  500             16384  18.46    .4188 3.417 1e-05 132.2 314.1 30.49      .3639   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    758 6623 15730 2.375\n",
            "\n",
            "14:45:04 | Overflow: setting loss scale to 16384.0\n",
            "14:45:05 | Overflow: setting loss scale to 16384.0\n",
            "14:45:05 | time:337s total_exs:32900 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9091  6485 14198 45.78  460             16384  16.92    .6637 3.074 1e-05 130.1 284.9 21.63      .4129   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    780 6615 14482 2.19\n",
            "\n",
            "14:45:05 | Overflow: setting loss scale to 16384.0\n",
            "14:45:15 | time:348s total_exs:33456 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9565  5986 13675 55.23  556             16384  16.26    .4672 3.321 1e-05 141.6 323.4 27.68      .3716   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    803 6127 13999 2.285\n",
            "\n",
            "14:45:24 | Overflow: setting loss scale to 16384.0\n",
            "14:45:25 | time:358s total_exs:33932 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9565  6575 15011 47.25  476             16384  18.32    .4188 3.221 1e-05 122.8 280.4 25.06      .3696   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    826 6698 15291 2.283\n",
            "\n",
            "14:45:30 | Overflow: setting loss scale to 16384.0\n",
            "14:45:35 | time:368s total_exs:34400 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9583  6591 15191 44.94  468             16384  18.17    .4672 3.139 1e-05 121.5   280 23.09      .3875   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    850 6713 15471 2.305\n",
            "\n",
            "14:45:39 | Overflow: setting loss scale to 16384.0\n",
            "14:45:43 | Overflow: setting loss scale to 16384.0\n",
            "14:45:45 | time:378s total_exs:34888 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9130  6583 14963 48.23  488             16384  17.37    .4344  3.19 1e-05   118 268.2 24.28      .4001   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    873 6701 15231 2.273\n",
            "\n",
            "14:45:51 | Overflow: setting loss scale to 16384.0\n",
            "14:45:56 | time:389s total_exs:35380 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9565  6544 14736 48.17  492             16384  18.86    .4569 3.332 1e-05 132.2 297.6 27.99      .3740   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    896 6676 15033 2.252\n",
            "\n",
            "14:46:03 | Overflow: setting loss scale to 16384.0\n",
            "14:46:06 | time:399s total_exs:35928 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9583  6090 14134 52.99  548             16384  17.05    .4364 3.206 1e-05 130.2 302.3 24.68      .3868   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    920 6220 14436 2.321\n",
            "\n",
            "14:46:16 | time:409s total_exs:36412 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6572 14629 46.84  484             16384  18.99    .4509 3.108 1e-05   129 287.1 22.38      .3953   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    943 6701 14916 2.226\n",
            "\n",
            "14:46:19 | Overflow: setting loss scale to 16384.0\n",
            "14:46:27 | time:420s total_exs:36836 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9565  6549 14684 41.33  424             16384  19.59    .4536 3.092 1e-05    98 219.7 22.02      .4059   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    966 6647 14903 2.242\n",
            "\n",
            "14:46:34 | Overflow: setting loss scale to 16384.0\n",
            "14:46:37 | time:430s total_exs:37372 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9583  6143 14182 51.56  536             16384  17.31    .4448 3.246 1e-05 135.5 312.8 25.69      .3822   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    990 6278 14495 2.309\n",
            "\n",
            "14:46:42 | Overflow: setting loss scale to 16384.0\n",
            "14:46:44 | Overflow: setting loss scale to 16384.0\n",
            "14:46:47 | time:440s total_exs:37816 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9130  6541 15005 44.29  444             16384   18.2    .4569 3.539 1e-05 129.5 297.1 34.42      .3471   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1013 6670 15302 2.294\n",
            "\n",
            "14:46:48 | Overflow: setting loss scale to 16384.0\n",
            "14:46:57 | time:450s total_exs:38280 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9565  6404 14427 45.45  464             16384  16.83    .4448 3.173 1e-05   120 270.4 23.87      .3843   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1036 6524 14698 2.253\n",
            "\n",
            "14:47:07 | time:460s total_exs:38736 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6406 14571  45.1  456             16384  19.21    .4628 3.105 1e-05 109.4 248.9 22.31      .3933   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1059 6515 14820 2.275\n",
            "\n",
            "14:47:09 | Overflow: setting loss scale to 16384.0\n",
            "14:47:18 | time:470s total_exs:39128 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9565  6423 14593 38.72  392             16384  21.41    .4487 3.382 1e-05 109.8 249.5 29.42      .3769   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1082 6533 14842 2.272\n",
            "\n",
            "14:47:19 | Overflow: setting loss scale to 16384.0\n",
            "14:47:22 | Overflow: setting loss scale to 16384.0\n",
            "14:47:24 | Overflow: setting loss scale to 16384.0\n",
            "14:47:28 | time:481s total_exs:39540 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "   .8750  6525 15090  39.7  412             16384  17.89    .4537 3.104 1e-05 96.12 222.3 22.3      .4131   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1106 6621 15313 2.313\n",
            "\n",
            "14:47:31 | Overflow: setting loss scale to 16384.0\n",
            "14:47:33 | Overflow: setting loss scale to 16384.0\n",
            "14:47:35 | Overflow: setting loss scale to 16384.0\n",
            "14:47:36 | Overflow: setting loss scale to 16384.0\n",
            "14:47:38 | time:491s total_exs:39964 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "   .8261  6493 14626 41.53  424             16384  16.52    .4628 3.243 1e-05 98.17 221.1 25.6      .3733   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1129 6591 14847 2.253\n",
            "\n",
            "14:47:41 | Overflow: setting loss scale to 16384.0\n",
            "14:47:44 | Overflow: setting loss scale to 16384.0\n",
            "14:47:49 | time:502s total_exs:40356 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9130  6405 14236 37.88  392             16384  19.13    .4487 3.433 1e-05 119.6 265.8 30.97      .3596   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1152 6524 14502 2.223\n",
            "\n",
            "14:47:52 | Overflow: setting loss scale to 16384.0\n",
            "14:47:55 | Overflow: setting loss scale to 16384.0\n",
            "14:47:59 | time:512s total_exs:40784 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "   .9130  6612 14598 41.08  428             16384  17.91    .4628 3.161 1e-05   103 227.5 23.6      .3802   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1175 6715 14825 2.208\n",
            "\n",
            "14:48:09 | time:522s total_exs:41172 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6343 14302 38.03  388             16384  20.56    .4537 3.274 1e-05 113.7 256.2 26.41      .3936   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1198 6457 14558 2.255\n",
            "\n",
            "14:48:20 | time:532s total_exs:41564 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6315 14287 38.56  392             16384  20.82    .4537 3.276 1e-05 92.13 208.4 26.47      .3794   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1221 6407 14495 2.263\n",
            "\n",
            "14:48:24 | Overflow: setting loss scale to 16384.0\n",
            "14:48:25 | Overflow: setting loss scale to 16384.0\n",
            "14:48:28 | Overflow: setting loss scale to 16384.0\n",
            "14:48:30 | Overflow: setting loss scale to 16384.0\n",
            "14:48:30 | time:543s total_exs:41952 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .8261  6572 14577 37.41  388             16384   16.3    .4628 3.462 1e-05 118.4 262.7 31.87      .3675   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1244 6691 14840 2.218\n",
            "\n",
            "14:48:32 | Overflow: setting loss scale to 16384.0\n",
            "14:48:40 | Overflow: setting loss scale to 16384.0\n",
            "14:48:40 | time:553s total_exs:42364 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9130  6397 14432 40.41  412             16384  17.13    .4628 3.051 1e-05 108.2 244.1 21.13      .4182   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1267 6506 14676 2.256\n",
            "\n",
            "14:48:41 | Overflow: setting loss scale to 16384.0\n",
            "14:48:43 | Overflow: setting loss scale to 16384.0\n",
            "14:48:47 | Overflow: setting loss scale to 16384.0\n",
            "14:48:50 | time:563s total_exs:42704 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .8696  6336 14512 33.86  340             16384  17.93    .4628 3.311 1e-05 103.9 237.9 27.42      .3817   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1290 6439 14750 2.291\n",
            "\n",
            "14:48:51 | Overflow: setting loss scale to 16384.0\n",
            "14:48:52 | Overflow: setting loss scale to 16384.0\n",
            "14:49:01 | time:573s total_exs:43124 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9130  6009 13371 40.63  420             16384  18.13    .6766 3.321 1e-05 130.5 290.4 27.68      .3784   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1313 6140 13661 2.225\n",
            "\n",
            "14:49:04 | Overflow: setting loss scale to 16384.0\n",
            "14:49:11 | time:583s total_exs:43516 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9565  6065 13928 39.14  392             16384  19.36    .4537 3.406 1e-05 115.7 265.6 30.14      .3662   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1336 6181 14194 2.297\n",
            "\n",
            "14:49:18 | Overflow: setting loss scale to 16384.0\n",
            "14:49:21 | time:594s total_exs:43904 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9583  5927 13945 38.03  388             16384   18.1    .4448 3.466 1e-05 136.2 320.5 32.02      .3373   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1360 6064 14265 2.353\n",
            "\n",
            "14:49:26 | Overflow: setting loss scale to 16384.0\n",
            "14:49:27 | saving model checkpoint: from_pretrained/model.checkpoint\n",
            "14:49:27 | Saving dictionary to from_pretrained/model.checkpoint.dict\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "14:49:33 | time:606s total_exs:44176 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9375  6285  8317 22.49  272             16384  17.96    .4628 3.032 1e-05 138.2 182.9 20.73      .3879   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                   1376 6423 8500 1.323\n",
            "\n",
            "14:49:39 | Overflow: setting loss scale to 16384.0\n",
            "14:49:40 | Overflow: setting loss scale to 16384.0\n",
            "14:49:43 | time:616s total_exs:44540 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9130  6420 14539 35.84  364             16384  17.99    .4628 3.341 1e-05 104.5 236.7 28.24      .3669   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1399 6524 14776 2.265\n",
            "\n",
            "14:49:47 | Overflow: setting loss scale to 16384.0\n",
            "14:49:49 | Overflow: setting loss scale to 16384.0\n",
            "14:49:53 | time:626s total_exs:44936 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9167  6070 14067 38.23  396             16384  17.23    .4275 2.984 1e-05 92.33   214 19.77      .4057   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1423 6162 14281 2.318\n",
            "\n",
            "14:50:04 | time:636s total_exs:45308 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6227 14234 36.97  372             16384  20.57    .4570  3.22 1e-05 96.09 219.6 25.02      .3805   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1446 6323 14453 2.286\n",
            "\n",
            "14:50:14 | time:647s total_exs:45668 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6074 13813 35.59  360             16384  21.93    .4590 3.521 1e-05 111.8 254.2 33.83      .3438   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1469 6186 14067 2.274\n",
            "\n",
            "14:50:24 | Overflow: setting loss scale to 16384.0\n",
            "14:50:24 | time:657s total_exs:46036 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9565  6123 14021 36.64  368             16384  17.64    .4487 3.418 1e-05 122.1 279.6 30.51      .3636   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   1492 6245 14300 2.29\n",
            "\n",
            "14:50:32 | Overflow: setting loss scale to 16384.0\n",
            "14:50:34 | Overflow: setting loss scale to 16384.0\n",
            "14:50:34 | time:667s total_exs:46412 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9167  5973 14029 36.79  376             16384  18.34    .4362 3.245 1e-05 103.2 242.5 25.66      .3967   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1516 6076 14271 2.349\n",
            "\n",
            "14:50:44 | time:677s total_exs:46724 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6238 14505 30.23  312             16384  20.95    .4628 3.257 1e-05 86.21 200.4 25.96      .3751   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1540 6324 14705 2.325\n",
            "\n",
            "14:50:54 | time:687s total_exs:47064 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6133 14007 33.76  340             16384  20.27    .4555 3.373 1e-05 114.3   261 29.15      .3577   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1563 6247 14268 2.284\n",
            "\n",
            "14:50:55 | Overflow: setting loss scale to 16384.0\n",
            "14:51:05 | time:697s total_exs:47420 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9565  6407 14598 35.27  356             16384  18.86    .4629 3.142 1e-05 96.48 219.8 23.16      .3966   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1586 6503 14818 2.279\n",
            "\n",
            "14:51:09 | Overflow: setting loss scale to 16384.0\n",
            "14:51:15 | time:708s total_exs:47796 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9565  6219 14056 36.94  376             16384  18.39    .4449 3.244 1e-05 98.78 223.2 25.63      .3675   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   1609 6318 14279 2.26\n",
            "\n",
            "14:51:18 | Overflow: setting loss scale to 16384.0\n",
            "14:51:25 | time:718s total_exs:48200 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9565  5922 13373 39.66  404             16384  16.85    .4590 3.168 1e-05   117 264.3 23.77      .3759   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1632 6039 13638 2.258\n",
            "\n",
            "14:51:33 | Overflow: setting loss scale to 16384.0\n",
            "14:51:34 | Overflow: setting loss scale to 16384.0\n",
            "14:51:35 | Overflow: setting loss scale to 16384.0\n",
            "14:51:35 | time:728s total_exs:48536 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "   .8696  6162 14069 33.35  336             16384  16.88    .4395  3.17 1e-05 102.8 234.7 23.8      .3972   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1655 6265 14304 2.283\n",
            "\n",
            "14:51:45 | time:738s total_exs:48916 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6067 14142 36.91  380             16384  19.16    .4324 3.011 1e-05 102.9 239.8 20.31      .3937   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1679 6170 14381 2.331\n",
            "\n",
            "14:51:55 | time:748s total_exs:49256 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6162 14176 34.01  340             16384  19.84    .4537 3.199 1e-05 100.3 230.7 24.52      .3801   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1702 6263 14407 2.301\n",
            "\n",
            "14:52:06 | time:758s total_exs:49560 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6216 14169 30.13  304             16384  19.74    .4449 3.263 1e-05 91.22 207.9 26.13      .3756   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   1725 6307 14377 2.28\n",
            "\n",
            "14:52:16 | time:769s total_exs:49888 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6102 13872 32.42  328             16384  19.17    .4537 3.273 1e-05 108.1 245.8 26.4      .3691   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1748 6210 14117 2.273\n",
            "\n",
            "14:52:22 | Overflow: setting loss scale to 16384.0\n",
            "14:52:26 | time:779s total_exs:50212 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9583  6343 14580 31.03  324             16384   20.8    .4629 3.126 1e-05 74.54 171.4 22.79      .4058   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1772 6417 14752 2.299\n",
            "\n",
            "14:52:31 | Overflow: setting loss scale to 16384.0\n",
            "14:52:36 | time:789s total_exs:50552 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9565  6043 13721 33.57  340             16384  18.68    .4555 3.168 1e-05 109.6 248.9 23.75      .3895   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1795 6152 13970 2.271\n",
            "\n",
            "14:52:46 | time:799s total_exs:50888 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6155 14035 33.31  336             16384  19.61    .4629 3.012 1e-05 87.04 198.5 20.33      .4226   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   1818 6242 14234 2.28\n",
            "\n",
            "14:52:49 | Overflow: setting loss scale to 16384.0\n",
            "14:52:57 | time:810s total_exs:51232 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9565  6171 13733 33.28  344             16384  18.75    .5992 2.738 1e-05 84.91   189 15.45      .4537   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1841 6256 13922 2.226\n",
            "\n",
            "14:53:07 | time:820s total_exs:51568 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6251 14128 33.02  336             16384  21.29    .4656 2.959 1e-05 77.48 175.1 19.28      .4052   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   1864 6328 14303 2.26\n",
            "\n",
            "14:53:17 | time:830s total_exs:51892 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6228 14111 31.92  324             16384  19.09    .4363 3.211 1e-05   101 228.7 24.8      .3730   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1887 6329 14339 2.266\n",
            "\n",
            "14:53:27 | time:840s total_exs:52268 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6046 13840 37.42  376             16384  18.05    .4590  2.94 1e-05 101.3 231.9 18.91      .4232   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1910 6147 14072 2.289\n",
            "\n",
            "14:53:29 | Overflow: setting loss scale to 16384.0\n",
            "14:53:30 | Overflow: setting loss scale to 16384.0\n",
            "14:53:37 | time:850s total_exs:52580 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9130  6142 14044 31.01  312             16384  17.71    .4656 3.152 1e-05  93.3 213.3 23.39      .3910   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1933 6236 14257 2.287\n",
            "\n",
            "14:53:48 | time:861s total_exs:52916 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5986 13837 32.36  336             16384  20.15    .4629 2.982 1e-05 89.17 206.1 19.72      .4075   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1957 6075 14043 2.312\n",
            "\n",
            "14:53:52 | Overflow: setting loss scale to 16384.0\n",
            "14:53:56 | Overflow: setting loss scale to 16384.0\n",
            "14:53:58 | time:871s total_exs:53232 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9167  6180 14313  30.5  316             16384   18.4    .4487 3.158 1e-05 97.12   225 23.53      .4101   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1981 6277 14538 2.316\n",
            "\n",
            "14:54:08 | time:881s total_exs:53544 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6057 14077 30.21  312             16384  19.95    .4395 3.082 1e-05 100.8 234.2 21.8      .4012   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2005 6158 14311 2.324\n",
            "\n",
            "14:54:09 | Overflow: setting loss scale to 16384.0\n",
            "14:54:19 | time:892s total_exs:53840 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9583  6129 14412    29  296             16384  20.88    .4363 2.997 1e-05 65.33 153.6 20.02      .4254   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2029 6194 14565 2.352\n",
            "\n",
            "14:54:23 | Overflow: setting loss scale to 16384.0\n",
            "14:54:29 | Overflow: setting loss scale to 16384.0\n",
            "14:54:29 | time:902s total_exs:54132 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "   .9167  6066 14102 28.28  292             16384  19.54    .4510 2.939 1e-05 65.83   153 18.9      .4228   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2053 6132 14255 2.325\n",
            "\n",
            "14:54:39 | time:912s total_exs:54444 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6077 13954 31.14  312             16384  20.68    .4537  3.07 1e-05 83.61   192 21.55      .4051   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2076 6161 14146 2.296\n",
            "\n",
            "14:54:49 | time:922s total_exs:54812 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5668 13165 35.61  368             16384  18.86    .4317 3.044 1e-05 92.88 215.7 20.98      .4015   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2100 5761 13381 2.323\n",
            "\n",
            "14:55:00 | time:932s total_exs:55128 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6038 13710  31.2  316             16384  17.53    .4591 3.068 1e-05   122 276.9 21.51      .3914   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2123 6160 13987 2.271\n",
            "\n",
            "14:55:06 | Overflow: setting loss scale to 16384.0\n",
            "14:55:10 | time:943s total_exs:55480 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9583  5753 13465 34.33  352             16384  18.16    .4556 3.075 1e-05 107.8 252.3 21.64      .3989   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2147 5861 13718 2.341\n",
            "\n",
            "14:55:20 | time:953s total_exs:55780 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6005 13820 30.01  300             16384  20.11    .4327  2.92 1e-05 81.13 186.7 18.55      .4277   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2170 6086 14006 2.301\n",
            "\n",
            "14:55:30 | time:963s total_exs:56076 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6160 14147 29.56  296             16384  19.76    .4395 3.101 1e-05  92.3   212 22.22      .4098   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2193 6252 14359 2.297\n",
            "\n",
            "14:55:40 | time:973s total_exs:56364 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6133 14213 27.81  288             16384  20.02    .4645 2.953 1e-05  85.5 198.1 19.16      .4240   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2217 6219 14411 2.317\n",
            "\n",
            "14:55:51 | time:983s total_exs:56652 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6102 14173 27.87  288             16384  21.37    .4395 2.991 1e-05 77.38 179.7 19.91      .3963   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2241 6179 14353 2.323\n",
            "\n",
            "14:56:01 | time:994s total_exs:56964 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6017 13714 30.92  312             16384  19.07    .4477 2.997 1e-05 110.8 252.5 20.02      .4082   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2264 6128 13967 2.279\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}