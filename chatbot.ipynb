{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM4QjFQ6rx4FPcQmwIdAdPL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinnik-dmitry07/Chatbot/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY3EisQ766aP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daf67007-460d-4903-9e2f-e94e12399667"
      },
      "source": [
        "!nvidia-smi\r\n",
        "!pip install --quiet parlai\r\n",
        "\r\n",
        "from datetime import timedelta\r\n",
        "from pathlib import Path\r\n",
        "\r\n",
        "GDRIVE_ROOT = Path('/content/drive/MyDrive/')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Feb 13 18:56:32 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   72C    P0    31W /  70W |  11982MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrkWqZx8g2Bm"
      },
      "source": [
        "# Change values only in this cell!\r\n",
        "SAVE_DIR = GDRIVE_ROOT / 'chatbot_model'  # change 'chatbot_model' to 'whatever/you/want'\r\n",
        "EPISODE_DT = timedelta(minutes=3)  # change to split messages in separate dialogues if time delta is greater than EPISODE_DT\r\n",
        "TRAIN_PART, TEST_PART, VALID_PART = 0.996, 0.002, 0.002\r\n",
        "\r\n",
        "assert TRAIN_PART + TEST_PART + VALID_PART == 1"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fr8wrt68ATZl",
        "outputId": "a628ce4a-0677-4fff-c5d8-a51c40bafb08"
      },
      "source": [
        "from google.colab import drive\r\n",
        "\r\n",
        "drive.mount(str(GDRIVE_ROOT.parent))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCM-AVZhHKFK"
      },
      "source": [
        "import json\r\n",
        "\r\n",
        "with open(GDRIVE_ROOT / 'result.json', 'r', encoding='utf8') as f:\r\n",
        "    raw_messages = json.load(f)['messages']"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3CvK47rIujd"
      },
      "source": [
        "from datetime import datetime\r\n",
        "\r\n",
        "filtered_messages = []\r\n",
        "for msg in raw_messages:\r\n",
        "    if (\r\n",
        "            'from' in msg and\r\n",
        "            'from_id' in msg and\r\n",
        "            'mime_type' not in msg and\r\n",
        "            msg['text'] and\r\n",
        "            isinstance(msg['text'], str)\r\n",
        "    ):\r\n",
        "        msg['date'] = datetime.strptime(msg['date'], '%Y-%m-%dT%H:%M:%S')\r\n",
        "        filtered_messages.append(msg)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz06qsAJJLda"
      },
      "source": [
        "joined_messages = [filtered_messages[0]]\r\n",
        "for i in range(1, len(filtered_messages)):\r\n",
        "    if (\r\n",
        "            filtered_messages[i - 1]['from_id'] == filtered_messages[i]['from_id'] and\r\n",
        "            filtered_messages[i - 1]['date'] - filtered_messages[i]['date'] <= EPISODE_DT\r\n",
        "    ):\r\n",
        "        joined_messages[-1]['text'] += ' ' + filtered_messages[i]['text']\r\n",
        "    else:\r\n",
        "        joined_messages.append(filtered_messages[i])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUphzoX3K_FF"
      },
      "source": [
        "def partition(alist, indices):\r\n",
        "    return [alist[a:b] for a, b in zip([0] + indices, indices + [None])]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXMAHZTxL-KN"
      },
      "source": [
        "def save_jsonl(messages, suffix, human_readable=False):\r\n",
        "    time_diffs = [messages[i + 1]['date'] - messages[i]['date'] for i in range(len(messages) - 1)]\r\n",
        "    split_positions = [i + 1 for i in range(len(time_diffs)) if time_diffs[i] > EPISODE_DT]\r\n",
        "    episodes = partition(messages, split_positions)\r\n",
        "    print(f'{suffix} episodes: {len(episodes)}, messages: {len(messages)}')\r\n",
        "\r\n",
        "    with open(f'data_{suffix}.jsonl', 'w', **({'encoding': 'utf8'} if human_readable else {})) as outfile:\r\n",
        "        for episode in episodes:\r\n",
        "            dialog = [\r\n",
        "                {\r\n",
        "                    'id': i % 2,\r\n",
        "                    'text': msg['text'].replace('\\n', ' '),\r\n",
        "                } for i, msg in enumerate(episode)\r\n",
        "            ]\r\n",
        "\r\n",
        "            episode = {'dialog': [dialog]}\r\n",
        "            json.dump(episode, outfile, **({'ensure_ascii': False} if human_readable else {}))\r\n",
        "            outfile.write('\\n')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApxWsw2omwax",
        "outputId": "ff5b5482-8ea1-4bbd-9002-2d1510cf1052"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "train, test, valid = np.split(joined_messages, [\r\n",
        "    int(TRAIN_PART * len(joined_messages)),\r\n",
        "    int((TRAIN_PART + TEST_PART) * len(joined_messages)),\r\n",
        "])\r\n",
        "\r\n",
        "save_jsonl(train, suffix='train')\r\n",
        "save_jsonl(test, suffix='test')\r\n",
        "save_jsonl(valid, suffix='valid')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train episodes: 346, messages: 861230\n",
            "test episodes: 1, messages: 1729\n",
            "valid episodes: 1, messages: 1730\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HWEdG7hMboN",
        "outputId": "92ec3342-3613-4122-f3e7-b3ee2ee94521"
      },
      "source": [
        "import os\r\n",
        "\r\n",
        "os.environ['SAVE_DIR'] = str(SAVE_DIR)\r\n",
        "!rm --recursive --force $SAVE_DIR\r\n",
        "!mkdir --parents $SAVE_DIR\r\n",
        "\r\n",
        "\r\n",
        "from parlai.scripts.train_model import TrainModel\r\n",
        "\r\n",
        "TrainModel.main(\r\n",
        "    task='jsonfile',\r\n",
        "    jsonfile_datapath='data',\r\n",
        "    jsonfile_datatype_extension=True,\r\n",
        "\r\n",
        "    model='transformer/generator',\r\n",
        "    model_file=str(SAVE_DIR / 'model'),\r\n",
        "    \r\n",
        "    init_model='zoo:tutorial_transformer_generator/model',\r\n",
        "\r\n",
        "    n_heads=16, n_layers=8, n_positions=512, text_truncate=512,\r\n",
        "    label_truncate=128, ffn_size=2048, embedding_size=512,\r\n",
        "    activation='gelu', variant='xlm',\r\n",
        "    dict_lower=True, dict_tokenizer='bpe',\r\n",
        "    dict_file='zoo:tutorial_transformer_generator/model.dict',\r\n",
        "    learn_positional_embeddings=True,\r\n",
        "    \r\n",
        "    lr=1e-5, optimizer='adam',\r\n",
        "    warmup_updates=5000,\r\n",
        "    validation_metric='ppl',\r\n",
        "    validation_every_n_secs=60 * 60,\r\n",
        "    save_every_n_secs=10 * 60,\r\n",
        "\r\n",
        "    batchsize=12, fp16=True, fp16_impl='mem_efficient',\r\n",
        "    \r\n",
        "    skip_generation=True,\r\n",
        "    \r\n",
        "    dynamic_batching='full',\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18:56:54 | building dictionary first...\n",
            "18:56:54 | No model with opt yet at: /content/drive/MyDrive/chatbot_model/model(.opt)\n",
            "18:56:54 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: full,datapath: /usr/local/lib/python3.6/dist-packages/data,tensorboard_logdir: None,jsonfile_datapath: data,jsonfile_datatype_extension: True,label_turns: secondspeaker,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,beam_block_full_context: True,beam_length_penalty: 0.65,topk: 10,topp: 0.9,beam_delay: 30,beam_block_list_filename: None,temperature: 1.0,compute_tokenized_bleu: False,interactive_mode: False,fp16_impl: mem_efficient,force_fp16_tokens: False,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,hf_skip_special_tokens: True,max_lr_steps: -1,invsqrt_lr_decay_gamma: -1,parlai_home: /usr/local/lib/python3.6/dist-packages\u001b[0m\n",
            "18:56:54 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--show-advanced-args False --task internal:new_reddit:presorted --datatype train:stream --numthreads 1 --batchsize 48 --num-epochs 5.0 --validation-every-n-secs 1800.0 --save-every-n-secs -1 --save-after-valid True --validation-max-exs 9920 --short-final-eval True --validation-patience 0 --validation-metric-mode min --dict-build-first True --numworkers 4 --pytorch-preprocess False --pytorch-teacher-batch-sort False --batch-sort-cache-type pop --batch-length-range 5 --shuffle False --batch-sort-field text --pytorch-context-length -1 --pytorch-include-labels True --log-every-n-secs 30.0 --distributed-world-size 64 --verbose False --port 61337 --dropout 0.1 --beam-size 8 --beam-min-n-best 3 --beam-min-length 10 --skip-generation False --inference beam --optimizer fused_adam --learningrate 0.0005 --gradient-clip 10.0 --adam-eps 1e-06 --betas 0.9,0.98 --weight-decay 0.01 --lr-scheduler invsqrt --warmup-updates 20000 --gpu 0 --beam-block-ngram 3 --beam-context-block-ngram 3\u001b[0m\n",
            "18:56:54 | Using CUDA\n",
            "18:56:54 | loading dictionary from /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "18:56:54 | num words = 54944\n",
            "18:56:55 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "18:56:55 | Loading existing model params from /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "18:56:56 | \u001b[33mNot loading optim state since optim class changed.\u001b[0m\n",
            "18:56:57 | Opt:\n",
            "18:56:57 |     activation: gelu\n",
            "18:56:57 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "18:56:57 |     adam_eps: 1e-08\n",
            "18:56:57 |     add_p1_after_newln: False\n",
            "18:56:57 |     aggregate_micro: False\n",
            "18:56:57 |     allow_missing_init_opts: False\n",
            "18:56:57 |     attention_dropout: 0.0\n",
            "18:56:57 |     batchsize: 12\n",
            "18:56:57 |     beam_block_full_context: True\n",
            "18:56:57 |     beam_block_list_filename: None\n",
            "18:56:57 |     beam_block_ngram: -1\n",
            "18:56:57 |     beam_context_block_ngram: -1\n",
            "18:56:57 |     beam_delay: 30\n",
            "18:56:57 |     beam_length_penalty: 0.65\n",
            "18:56:57 |     beam_min_length: 1\n",
            "18:56:57 |     beam_size: 1\n",
            "18:56:57 |     betas: '(0.9, 0.999)'\n",
            "18:56:57 |     bpe_add_prefix_space: None\n",
            "18:56:57 |     bpe_debug: False\n",
            "18:56:57 |     bpe_merge: None\n",
            "18:56:57 |     bpe_vocab: None\n",
            "18:56:57 |     compute_tokenized_bleu: False\n",
            "18:56:57 |     datapath: /usr/local/lib/python3.6/dist-packages/data\n",
            "18:56:57 |     datatype: train\n",
            "18:56:57 |     delimiter: '\\n'\n",
            "18:56:57 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "18:56:57 |     dict_endtoken: __end__\n",
            "18:56:57 |     dict_file: /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "18:56:57 |     dict_include_test: False\n",
            "18:56:57 |     dict_include_valid: False\n",
            "18:56:57 |     dict_initpath: None\n",
            "18:56:57 |     dict_language: english\n",
            "18:56:57 |     dict_loaded: True\n",
            "18:56:57 |     dict_lower: True\n",
            "18:56:57 |     dict_max_ngram_size: -1\n",
            "18:56:57 |     dict_maxexs: -1\n",
            "18:56:57 |     dict_maxtokens: -1\n",
            "18:56:57 |     dict_minfreq: 0\n",
            "18:56:57 |     dict_nulltoken: __null__\n",
            "18:56:57 |     dict_starttoken: __start__\n",
            "18:56:57 |     dict_textfields: text,labels\n",
            "18:56:57 |     dict_tokenizer: bpe\n",
            "18:56:57 |     dict_unktoken: __unk__\n",
            "18:56:57 |     display_examples: False\n",
            "18:56:57 |     download_path: None\n",
            "18:56:57 |     dropout: 0.0\n",
            "18:56:57 |     dynamic_batching: full\n",
            "18:56:57 |     embedding_projection: random\n",
            "18:56:57 |     embedding_size: 512\n",
            "18:56:57 |     embedding_type: random\n",
            "18:56:57 |     embeddings_scale: True\n",
            "18:56:57 |     eval_batchsize: None\n",
            "18:56:57 |     evaltask: None\n",
            "18:56:57 |     ffn_size: 2048\n",
            "18:56:57 |     force_fp16_tokens: False\n",
            "18:56:57 |     fp16: True\n",
            "18:56:57 |     fp16_impl: mem_efficient\n",
            "18:56:57 |     gpu: -1\n",
            "18:56:57 |     gradient_clip: 0.1\n",
            "18:56:57 |     hf_skip_special_tokens: True\n",
            "18:56:57 |     hide_labels: False\n",
            "18:56:57 |     history_add_global_end_token: None\n",
            "18:56:57 |     history_reversed: False\n",
            "18:56:57 |     history_size: -1\n",
            "18:56:57 |     image_cropsize: 224\n",
            "18:56:57 |     image_mode: raw\n",
            "18:56:57 |     image_size: 256\n",
            "18:56:57 |     inference: greedy\n",
            "18:56:57 |     init_model: /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "18:56:57 |     init_opt: None\n",
            "18:56:57 |     interactive_mode: False\n",
            "18:56:57 |     invsqrt_lr_decay_gamma: -1\n",
            "18:56:57 |     jsonfile_datapath: data\n",
            "18:56:57 |     jsonfile_datatype_extension: True\n",
            "18:56:57 |     label_truncate: 128\n",
            "18:56:57 |     label_turns: secondspeaker\n",
            "18:56:57 |     learn_positional_embeddings: True\n",
            "18:56:57 |     learningrate: 1e-05\n",
            "18:56:57 |     load_from_checkpoint: True\n",
            "18:56:57 |     log_every_n_secs: 10\n",
            "18:56:57 |     loglevel: info\n",
            "18:56:57 |     lr_scheduler: reduceonplateau\n",
            "18:56:57 |     lr_scheduler_decay: 0.5\n",
            "18:56:57 |     lr_scheduler_patience: 3\n",
            "18:56:57 |     max_lr_steps: -1\n",
            "18:56:57 |     max_train_time: -1\n",
            "18:56:57 |     metrics: default\n",
            "18:56:57 |     model: transformer/generator\n",
            "18:56:57 |     model_file: /content/drive/MyDrive/chatbot_model/model\n",
            "18:56:57 |     model_parallel: False\n",
            "18:56:57 |     momentum: 0\n",
            "18:56:57 |     multitask_weights: [1]\n",
            "18:56:57 |     n_decoder_layers: -1\n",
            "18:56:57 |     n_encoder_layers: -1\n",
            "18:56:57 |     n_heads: 16\n",
            "18:56:57 |     n_layers: 8\n",
            "18:56:57 |     n_positions: 512\n",
            "18:56:57 |     n_segments: 0\n",
            "18:56:57 |     nesterov: True\n",
            "18:56:57 |     no_cuda: False\n",
            "18:56:57 |     num_epochs: -1\n",
            "18:56:57 |     nus: (0.7,)\n",
            "18:56:57 |     optimizer: mem_eff_adam\n",
            "18:56:57 |     output_scaling: 1.0\n",
            "18:56:57 |     override: \"{'task': 'jsonfile', 'jsonfile_datapath': 'data', 'jsonfile_datatype_extension': True, 'model': 'transformer/generator', 'model_file': '/content/drive/MyDrive/chatbot_model/model', 'init_model': 'zoo:tutorial_transformer_generator/model', 'n_heads': 16, 'n_layers': 8, 'n_positions': 512, 'text_truncate': 512, 'label_truncate': 128, 'ffn_size': 2048, 'embedding_size': 512, 'activation': 'gelu', 'variant': 'xlm', 'dict_lower': True, 'dict_tokenizer': 'bpe', 'dict_file': '/usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model.dict', 'learn_positional_embeddings': True, 'learningrate': 1e-05, 'optimizer': 'adam', 'warmup_updates': 5000, 'validation_metric': 'ppl', 'validation_every_n_secs': 3600.0, 'save_every_n_secs': 600.0, 'batchsize': 12, 'fp16': True, 'fp16_impl': 'mem_efficient', 'skip_generation': True, 'dynamic_batching': 'full'}\"\n",
            "18:56:57 |     parlai_home: /usr/local/lib/python3.6/dist-packages\n",
            "18:56:57 |     person_tokens: False\n",
            "18:56:57 |     rank_candidates: False\n",
            "18:56:57 |     relu_dropout: 0.0\n",
            "18:56:57 |     save_after_valid: False\n",
            "18:56:57 |     save_every_n_secs: 600.0\n",
            "18:56:57 |     share_word_embeddings: True\n",
            "18:56:57 |     short_final_eval: False\n",
            "18:56:57 |     skip_generation: True\n",
            "18:56:57 |     special_tok_lst: None\n",
            "18:56:57 |     split_lines: False\n",
            "18:56:57 |     starttime: Feb13_18-56\n",
            "18:56:57 |     task: jsonfile\n",
            "18:56:57 |     temperature: 1.0\n",
            "18:56:57 |     tensorboard_log: False\n",
            "18:56:57 |     tensorboard_logdir: None\n",
            "18:56:57 |     text_truncate: 512\n",
            "18:56:57 |     topk: 10\n",
            "18:56:57 |     topp: 0.9\n",
            "18:56:57 |     truncate: -1\n",
            "18:56:57 |     update_freq: 1\n",
            "18:56:57 |     use_reply: label\n",
            "18:56:57 |     validation_cutoff: 1.0\n",
            "18:56:57 |     validation_every_n_epochs: -1\n",
            "18:56:57 |     validation_every_n_secs: 3600.0\n",
            "18:56:57 |     validation_max_exs: -1\n",
            "18:56:57 |     validation_metric: ppl\n",
            "18:56:57 |     validation_metric_mode: None\n",
            "18:56:57 |     validation_patience: 10\n",
            "18:56:57 |     validation_share_agent: False\n",
            "18:56:57 |     variant: xlm\n",
            "18:56:57 |     warmup_rate: 0.0001\n",
            "18:56:57 |     warmup_updates: 5000\n",
            "18:56:57 |     weight_decay: None\n",
            "18:56:57 | creating task(s): jsonfile\n",
            "18:56:57 | [loading data from json file into task:data_train.jsonl]\n",
            "18:57:00 | \u001b[31mMetadata does not exist. Please double check your datapath.\u001b[0m\n",
            "18:57:04 | training...\n",
            "18:57:04 | Overflow: setting loss scale to 65536.0\n",
            "18:57:05 | Overflow: setting loss scale to 32768.0\n",
            "18:57:05 | Overflow: setting loss scale to 16384.0\n",
            "18:57:14 | time:10s total_exs:4840 epochs:0.01\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .8966  3915 11251 479.6 4840             18644     13    .8419 4.083 5.899e-08   906  2604 59.32      .3049   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                     29 4821 13854 2.874\n",
            "\n",
            "18:57:24 | time:20s total_exs:7956 epochs:0.02\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5014 17780 306.9 3116             16384  13.73    .8645 3.962 1.31e-07 509.3  1806 52.55      .3238   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                     65 5524 19586 3.546\n",
            "\n",
            "18:57:25 | Overflow: setting loss scale to 16384.0\n",
            "18:57:34 | time:31s total_exs:10308 epochs:0.02\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9730  5815 21170 231.4 2352             16384  14.89    .8393 3.952 2.05e-07 378.9  1380 52.03      .3081   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    102 6194 22550 3.641\n",
            "\n",
            "18:57:44 | time:41s total_exs:12516 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5515 19301 220.8 2208             16384  14.94    .8393 3.891 2.75e-07 368.9  1291 48.94      .3256   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    137 5884 20592  3.5\n",
            "\n",
            "18:57:49 | Overflow: setting loss scale to 16384.0\n",
            "18:57:54 | time:51s total_exs:14260 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9714  6182 21454 172.9 1744             16384  15.38    .7144 3.963 3.45e-07 314.2  1090 52.59      .3165   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    172 6496 22544 3.471\n",
            "\n",
            "18:58:05 | time:61s total_exs:15812 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6218 20768 152.5 1552             16384  17.02    .8820 3.999 4.13e-07 301.1  1006 54.52      .3151   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    206 6519 21774 3.34\n",
            "\n",
            "18:58:15 | time:71s total_exs:17336 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6040 20881 150.5 1524             16384  17.35    .6793 3.987 4.83e-07 251.7 870.2 53.92      .3138   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    241 6291 21751 3.458\n",
            "\n",
            "18:58:25 | time:81s total_exs:18724 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6468 22136 135.7 1388             16384  16.23    .6971 3.954 5.529e-07 256.3 877.2 52.16      .3157   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    276 6724 23013 3.423\n",
            "\n",
            "18:58:26 | \u001b[31mRan out of memory, skipping batch. if this happens frequently, decrease batchsize or truncate the inputs to the model.\u001b[0m\n",
            "18:58:35 | time:92s total_exs:20032 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  skipped_batches  \\\n",
            "       1  6091 20355 128.6 1308             16384   17.6    .8921 3.998 6.189e-07 214.2 715.8 54.51                1   \n",
            "    token_acc  total_train_updates  tpb   tps   ups  \n",
            "        .3168                  309 6305 21071 3.244\n",
            "\n",
            "18:58:36 | Overflow: setting loss scale to 16384.0\n",
            "18:58:45 | time:102s total_exs:21256 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9706  6470 21414 119.1 1224             16384  17.72    .8560 3.953 6.869e-07 220.3   729 52.07      .3248   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    343 6691 22143 3.31\n",
            "\n",
            "18:58:56 | time:112s total_exs:22356 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6647 22232 108.2 1100             16384  18.13    .6466 4.072 7.549e-07 198.9 665.3 58.67      .3092   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    377 6845 22898 3.345\n",
            "\n",
            "18:59:04 | \u001b[31mRan out of memory, skipping batch. if this happens frequently, decrease batchsize or truncate the inputs to the model.\u001b[0m\n",
            "18:59:06 | time:122s total_exs:23496 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  skipped_batches  \\\n",
            "       1  6126 20208   114 1140             16384  17.34    .8343 3.967 8.189e-07   206 679.7 52.82                1   \n",
            "    token_acc  total_train_updates  tpb   tps   ups  \n",
            "        .3175                  409 6332 20888 3.199\n",
            "\n",
            "18:59:08 | Overflow: setting loss scale to 16384.0\n",
            "18:59:16 | \u001b[31mRan out of memory, skipping batch. if this happens frequently, decrease batchsize or truncate the inputs to the model.\u001b[0m\n",
            "18:59:16 | time:132s total_exs:24548 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  skipped_batches  \\\n",
            "   .9697  6244 20673 102.4 1052             16384  17.07    .8371 3.931 8.849e-07 197.5 653.8 50.96                1   \n",
            "    token_acc  total_train_updates  tpb   tps   ups  \n",
            "        .3165                  442 6441 21327 3.214\n",
            "\n",
            "18:59:26 | time:143s total_exs:25524 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6491 21010 95.72  976             16384  16.99    .7466 3.897 9.509e-07 216.6 701.1 49.27      .3282   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    475 6708 21711 3.237\n",
            "\n",
            "18:59:36 | time:153s total_exs:26400 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6515 20779 87.31  876             16384   18.6    .7701 3.972 1.015e-06 187.5 598.1 53.1      .3196   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    507 6702 21377 3.19\n",
            "\n",
            "18:59:46 | time:163s total_exs:27280 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6626 21066 87.43  880             16384  18.64    .8858 3.879 1.079e-06 168.3 535.2 48.37      .3260   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    539 6794 21602 3.18\n",
            "\n",
            "18:59:56 | time:173s total_exs:28076 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6361 20359 79.61  796             16384  19.21    .7264 3.953 1.143e-06 156.9 502.2 52.08      .3059   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    571 6518 20861 3.201\n",
            "\n",
            "19:00:07 | time:183s total_exs:28928 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6478 20790 82.86  852             16384  19.36    .7103 3.911 1.209e-06 137.9 442.5 49.93      .3273   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    604 6616 21232 3.21\n",
            "\n",
            "19:00:17 | time:193s total_exs:29660 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6503 20710 72.85  732             16384  18.64    .7653 3.804 1.273e-06 165.5   527 44.88      .3306   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    636 6668 21237 3.185\n",
            "\n",
            "19:00:27 | time:203s total_exs:30380 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6675 20650 71.86  720             16384  19.34    .7128 3.785 1.335e-06 146.2 452.3 44.04      .3202   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    667 6821 21103 3.094\n",
            "\n",
            "19:00:36 | Overflow: setting loss scale to 16384.0\n",
            "19:00:36 | Overflow: setting loss scale to 16384.0\n",
            "19:00:37 | time:213s total_exs:31108 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9355  6304 19538 72.78  728             16384  18.26    .7103 3.932 1.397e-06 140.3 434.8 50.98      .3083   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    698 6445 19973 3.099\n",
            "\n",
            "19:00:47 | time:224s total_exs:31972 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6306 19588 83.86  864             16384  19.12    .7420 3.943 1.461e-06   141   438 51.58      .3096   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    730 6447 20026 3.106\n",
            "\n",
            "19:00:57 | time:234s total_exs:32744 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6284 19578 75.16  772             16384  18.14    .7467 3.831 1.525e-06 138.4 431.1 46.13      .3266   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    762 6422 20009 3.116\n",
            "\n",
            "19:00:58 | Overflow: setting loss scale to 16384.0\n",
            "19:01:01 | Overflow: setting loss scale to 16384.0\n",
            "19:01:08 | time:244s total_exs:33496 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9375  6346 20021 74.13  752             16384  17.57    .7103 3.888 1.589e-06 125.8   397 48.79      .3228   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    794 6472 20418 3.155\n",
            "\n",
            "19:01:16 | Overflow: setting loss scale to 16384.0\n",
            "19:01:18 | time:254s total_exs:34228 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9677  6440 19837 72.73  732             16384  18.08    .7103 3.847 1.651e-06 145.2 447.3 46.87      .3174   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    825 6586 20284 3.08\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}