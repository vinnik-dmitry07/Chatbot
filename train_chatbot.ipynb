{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMsuTTWuOm6uQ2tqh7QZfs/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinnik-dmitry07/Chatbot/blob/main/train_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAbMlxO9DI4h",
        "outputId": "44f49e3c-3f44-486f-bfe1-c132727871ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvidia-smi\r\n",
        "!pip install --quiet parlai"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Feb 15 20:16:40 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   62C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 18.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 58.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 552kB 58.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 10.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.2MB 56.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 50.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 59.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 65.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 57.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 317kB 54.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 13.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 32.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 40kB 7.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.0MB 47.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.8MB 51.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 59.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 12.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 143kB 65.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 13.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 12.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 9.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 11.6MB/s \n",
            "\u001b[?25h  Building wheel for docformatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for websocket-server (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for untokenize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: torchtext 0.8.1 has requirement torch==1.7.1, but you'll have torch 1.7.0+cu101 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: requests 2.23.0 has requirement urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: myst-parser 0.13.5 has requirement sphinx<4,>=2, but you'll have sphinx 1.8.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: sphinx-autodoc-typehints 1.11.1 has requirement Sphinx>=3.0, but you'll have sphinx 1.8.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fvcore 0.1.3.post20210213 has requirement pyyaml>=5.1, but you'll have pyyaml 3.13 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY3EisQ766aP"
      },
      "source": [
        "from pathlib import Path\r\n",
        "\r\n",
        "GDRIVE_ROOT = Path('/content/drive/MyDrive/')\r\n",
        "SAVE_DIR = GDRIVE_ROOT / 'chatbot_model'\r\n",
        "DATA_DIR = GDRIVE_ROOT / 'chatbot_data'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9y2IWmWDFjU"
      },
      "source": [
        "from datetime import timedelta\r\n",
        "\r\n",
        "EPISODE_DT = timedelta(minutes=3)  # change to split messages in separate dialogues if time delta is greater than EPISODE_DT\r\n",
        "TRAIN_PART, TEST_PART, VALID_PART = 0.996, 0.002, 0.002\r\n",
        "\r\n",
        "assert TRAIN_PART + TEST_PART + VALID_PART == 1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr8wrt68ATZl",
        "outputId": "74182999-998b-44e2-e3be-5db9621cae62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\r\n",
        "\r\n",
        "drive.mount(str(GDRIVE_ROOT.parent))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCM-AVZhHKFK"
      },
      "source": [
        "import json\r\n",
        "\r\n",
        "with open(DATA_DIR / 'result.json', 'r', encoding='utf8') as f:\r\n",
        "    raw_messages = json.load(f)['messages']"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3CvK47rIujd",
        "outputId": "6d356854-c8c4-4adc-dbc2-be6fb4237499",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        }
      },
      "source": [
        "from datetime import datetime\r\n",
        "\r\n",
        "filtered_messages = []\r\n",
        "for msg in raw_messages:\r\n",
        "    if (\r\n",
        "            'from' in msg and\r\n",
        "            'from_id' in msg and\r\n",
        "            'mime_type' not in msg and\r\n",
        "            msg['text'] and\r\n",
        "            isinstance(msg['text'], str) and\r\n",
        "            len(msg['text']) < 50\r\n",
        "    ):\r\n",
        "        msg['date'] = datetime.strptime(msg['date'], '%Y-%m-%dT%H:%M:%S')\r\n",
        "        filtered_messages.append(msg)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-0638f8bf7ef9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     ):\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mmsg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%Y-%m-%dT%H:%M:%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mfiltered_messages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: strptime() argument 1 must be str, not datetime.datetime"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz06qsAJJLda"
      },
      "source": [
        "import re\r\n",
        "\r\n",
        "joined_messages = [filtered_messages[0]]\r\n",
        "for i in range(1, len(filtered_messages)):\r\n",
        "    if (\r\n",
        "            filtered_messages[i - 1]['from_id'] == filtered_messages[i]['from_id'] and\r\n",
        "            filtered_messages[i - 1]['date'] - filtered_messages[i]['date'] <= EPISODE_DT\r\n",
        "    ):\r\n",
        "        joined_messages[-1]['text'] += ' ' + re.sub(r'\\W+', '', filtered_messages[i]['text']),\r\n",
        "    else:\r\n",
        "        joined_messages.append(filtered_messages[i])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUphzoX3K_FF"
      },
      "source": [
        "def partition(alist, indices):\r\n",
        "    return [alist[a:b] for a, b in zip([0] + indices, indices + [None])]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXMAHZTxL-KN"
      },
      "source": [
        "def save_jsonl(messages, suffix, human_readable=False):\r\n",
        "    time_diffs = [messages[i + 1]['date'] - messages[i]['date'] for i in range(len(messages) - 1)]\r\n",
        "    split_positions = [i + 1 for i in range(len(time_diffs)) if time_diffs[i] > EPISODE_DT]\r\n",
        "    episodes = partition(messages, split_positions)\r\n",
        "    print(f'{suffix} episodes: {len(episodes)}, messages: {len(messages)}')\r\n",
        "\r\n",
        "    with open(DATA_DIR / f'data_{suffix}.jsonl', 'w', **({'encoding': 'utf8'} if human_readable else {})) as outfile:\r\n",
        "        for episode in episodes:\r\n",
        "            dialog = [{'id': i % 2, 'text': msg['text']} for i, msg in enumerate(episode)]\r\n",
        "            episode = {'dialog': [dialog]}\r\n",
        "            json.dump(episode, outfile, **({'ensure_ascii': False} if human_readable else {}))\r\n",
        "            outfile.write('\\n')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApxWsw2omwax",
        "outputId": "31afe74a-f473-4982-8aa1-022ef91fdf4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "train, test, valid = np.split(joined_messages, [\r\n",
        "    int(TRAIN_PART * len(joined_messages)),\r\n",
        "    int((TRAIN_PART + TEST_PART) * len(joined_messages)),\r\n",
        "])\r\n",
        "\r\n",
        "save_jsonl(train, suffix='train')\r\n",
        "save_jsonl(test, suffix='test')\r\n",
        "save_jsonl(valid, suffix='valid')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train episodes: 366, messages: 822582\n",
            "test episodes: 1, messages: 1652\n",
            "valid episodes: 1, messages: 1652\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HWEdG7hMboN",
        "outputId": "3ee6450d-cadf-40c9-e194-ea4d02b379d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import os\r\n",
        "\r\n",
        "os.environ['SAVE_DIR'] = str(SAVE_DIR)\r\n",
        "!rm --recursive --force $SAVE_DIR\r\n",
        "!mkdir --parents $SAVE_DIR\r\n",
        "\r\n",
        "\r\n",
        "from parlai.scripts.train_model import TrainModel\r\n",
        "\r\n",
        "TrainModel.main(\r\n",
        "    task='jsonfile',\r\n",
        "    jsonfile_datapath=str(DATA_DIR / 'data'),\r\n",
        "    jsonfile_datatype_extension=True,\r\n",
        "\r\n",
        "    model='transformer/generator',\r\n",
        "    model_file=str(SAVE_DIR / 'model'),\r\n",
        "    \r\n",
        "    init_model='zoo:tutorial_transformer_generator/model',\r\n",
        "\r\n",
        "    n_heads=16, n_layers=8, n_positions=512, text_truncate=512,\r\n",
        "    label_truncate=128, ffn_size=2048, embedding_size=512,\r\n",
        "    activation='gelu', variant='xlm',\r\n",
        "    dict_lower=True, dict_tokenizer='bpe',\r\n",
        "    dict_file='zoo:tutorial_transformer_generator/model.dict',\r\n",
        "    learn_positional_embeddings=True,\r\n",
        "    \r\n",
        "    lr=1e-5, optimizer='adam',\r\n",
        "    warmup_updates=5000,\r\n",
        "    validation_metric='ppl',\r\n",
        "    validation_every_n_secs=60 * 60,  # running eval: valid\r\n",
        "    save_every_n_secs=10 * 60,  # saving model checkpoint\r\n",
        "\r\n",
        "    batchsize=12, fp16=True, fp16_impl='mem_efficient',\r\n",
        "    \r\n",
        "    skip_generation=True,\r\n",
        "    \r\n",
        "    dynamic_batching='full',\r\n",
        "\r\n",
        "    label_turns='both',  # https://parl.ai/docs/core/teachers.html#parlai.core.teachers.ConversationTeacher\r\n",
        ")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22:21:36 | building dictionary first...\n",
            "22:21:36 | No model with opt yet at: /content/drive/MyDrive/chatbot_model/model(.opt)\n",
            "22:21:36 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: full,datapath: /usr/local/lib/python3.6/dist-packages/data,tensorboard_logdir: None,jsonfile_datapath: /content/drive/MyDrive/chatbot_data/data,jsonfile_datatype_extension: True,label_turns: both,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,beam_block_full_context: True,beam_length_penalty: 0.65,topk: 10,topp: 0.9,beam_delay: 30,beam_block_list_filename: None,temperature: 1.0,compute_tokenized_bleu: False,interactive_mode: False,fp16_impl: mem_efficient,force_fp16_tokens: False,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,hf_skip_special_tokens: True,max_lr_steps: -1,invsqrt_lr_decay_gamma: -1,parlai_home: /usr/local/lib/python3.6/dist-packages\u001b[0m\n",
            "22:21:36 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--show-advanced-args False --task internal:new_reddit:presorted --datatype train:stream --numthreads 1 --batchsize 48 --num-epochs 5.0 --validation-every-n-secs 1800.0 --save-every-n-secs -1 --save-after-valid True --validation-max-exs 9920 --short-final-eval True --validation-patience 0 --validation-metric-mode min --dict-build-first True --numworkers 4 --pytorch-preprocess False --pytorch-teacher-batch-sort False --batch-sort-cache-type pop --batch-length-range 5 --shuffle False --batch-sort-field text --pytorch-context-length -1 --pytorch-include-labels True --log-every-n-secs 30.0 --distributed-world-size 64 --verbose False --port 61337 --dropout 0.1 --beam-size 8 --beam-min-n-best 3 --beam-min-length 10 --skip-generation False --inference beam --optimizer fused_adam --learningrate 0.0005 --gradient-clip 10.0 --adam-eps 1e-06 --betas 0.9,0.98 --weight-decay 0.01 --lr-scheduler invsqrt --warmup-updates 20000 --gpu 0 --beam-block-ngram 3 --beam-context-block-ngram 3\u001b[0m\n",
            "22:21:36 | Using CUDA\n",
            "22:21:36 | loading dictionary from /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "22:21:36 | num words = 54944\n",
            "22:21:37 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "22:21:37 | Loading existing model params from /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "22:22:11 | \u001b[33mNot loading optim state since optim class changed.\u001b[0m\n",
            "22:22:11 | Opt:\n",
            "22:22:11 |     activation: gelu\n",
            "22:22:11 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "22:22:11 |     adam_eps: 1e-08\n",
            "22:22:11 |     add_p1_after_newln: False\n",
            "22:22:11 |     aggregate_micro: False\n",
            "22:22:11 |     allow_missing_init_opts: False\n",
            "22:22:11 |     attention_dropout: 0.0\n",
            "22:22:12 |     batchsize: 12\n",
            "22:22:12 |     beam_block_full_context: True\n",
            "22:22:12 |     beam_block_list_filename: None\n",
            "22:22:12 |     beam_block_ngram: -1\n",
            "22:22:12 |     beam_context_block_ngram: -1\n",
            "22:22:12 |     beam_delay: 30\n",
            "22:22:12 |     beam_length_penalty: 0.65\n",
            "22:22:12 |     beam_min_length: 1\n",
            "22:22:12 |     beam_size: 1\n",
            "22:22:12 |     betas: '(0.9, 0.999)'\n",
            "22:22:12 |     bpe_add_prefix_space: None\n",
            "22:22:12 |     bpe_debug: False\n",
            "22:22:12 |     bpe_merge: None\n",
            "22:22:12 |     bpe_vocab: None\n",
            "22:22:12 |     compute_tokenized_bleu: False\n",
            "22:22:12 |     datapath: /usr/local/lib/python3.6/dist-packages/data\n",
            "22:22:12 |     datatype: train\n",
            "22:22:12 |     delimiter: '\\n'\n",
            "22:22:12 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "22:22:12 |     dict_endtoken: __end__\n",
            "22:22:12 |     dict_file: /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "22:22:12 |     dict_include_test: False\n",
            "22:22:12 |     dict_include_valid: False\n",
            "22:22:12 |     dict_initpath: None\n",
            "22:22:12 |     dict_language: english\n",
            "22:22:12 |     dict_loaded: True\n",
            "22:22:12 |     dict_lower: True\n",
            "22:22:12 |     dict_max_ngram_size: -1\n",
            "22:22:12 |     dict_maxexs: -1\n",
            "22:22:12 |     dict_maxtokens: -1\n",
            "22:22:12 |     dict_minfreq: 0\n",
            "22:22:12 |     dict_nulltoken: __null__\n",
            "22:22:12 |     dict_starttoken: __start__\n",
            "22:22:12 |     dict_textfields: text,labels\n",
            "22:22:12 |     dict_tokenizer: bpe\n",
            "22:22:12 |     dict_unktoken: __unk__\n",
            "22:22:12 |     display_examples: False\n",
            "22:22:12 |     download_path: None\n",
            "22:22:12 |     dropout: 0.0\n",
            "22:22:12 |     dynamic_batching: full\n",
            "22:22:12 |     embedding_projection: random\n",
            "22:22:12 |     embedding_size: 512\n",
            "22:22:12 |     embedding_type: random\n",
            "22:22:12 |     embeddings_scale: True\n",
            "22:22:12 |     eval_batchsize: None\n",
            "22:22:12 |     evaltask: None\n",
            "22:22:12 |     ffn_size: 2048\n",
            "22:22:12 |     force_fp16_tokens: False\n",
            "22:22:12 |     fp16: True\n",
            "22:22:12 |     fp16_impl: mem_efficient\n",
            "22:22:12 |     gpu: -1\n",
            "22:22:12 |     gradient_clip: 0.1\n",
            "22:22:12 |     hf_skip_special_tokens: True\n",
            "22:22:12 |     hide_labels: False\n",
            "22:22:12 |     history_add_global_end_token: None\n",
            "22:22:12 |     history_reversed: False\n",
            "22:22:12 |     history_size: -1\n",
            "22:22:12 |     image_cropsize: 224\n",
            "22:22:12 |     image_mode: raw\n",
            "22:22:12 |     image_size: 256\n",
            "22:22:12 |     inference: greedy\n",
            "22:22:12 |     init_model: /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "22:22:12 |     init_opt: None\n",
            "22:22:12 |     interactive_mode: False\n",
            "22:22:12 |     invsqrt_lr_decay_gamma: -1\n",
            "22:22:12 |     jsonfile_datapath: /content/drive/MyDrive/chatbot_data/data\n",
            "22:22:12 |     jsonfile_datatype_extension: True\n",
            "22:22:12 |     label_truncate: 128\n",
            "22:22:12 |     label_turns: both\n",
            "22:22:12 |     learn_positional_embeddings: True\n",
            "22:22:12 |     learningrate: 1e-05\n",
            "22:22:12 |     load_from_checkpoint: True\n",
            "22:22:12 |     log_every_n_secs: 10\n",
            "22:22:12 |     loglevel: info\n",
            "22:22:12 |     lr_scheduler: reduceonplateau\n",
            "22:22:12 |     lr_scheduler_decay: 0.5\n",
            "22:22:12 |     lr_scheduler_patience: 3\n",
            "22:22:12 |     max_lr_steps: -1\n",
            "22:22:12 |     max_train_time: -1\n",
            "22:22:12 |     metrics: default\n",
            "22:22:12 |     model: transformer/generator\n",
            "22:22:12 |     model_file: /content/drive/MyDrive/chatbot_model/model\n",
            "22:22:12 |     model_parallel: False\n",
            "22:22:12 |     momentum: 0\n",
            "22:22:12 |     multitask_weights: [1]\n",
            "22:22:12 |     n_decoder_layers: -1\n",
            "22:22:12 |     n_encoder_layers: -1\n",
            "22:22:12 |     n_heads: 16\n",
            "22:22:12 |     n_layers: 8\n",
            "22:22:12 |     n_positions: 512\n",
            "22:22:12 |     n_segments: 0\n",
            "22:22:12 |     nesterov: True\n",
            "22:22:12 |     no_cuda: False\n",
            "22:22:12 |     num_epochs: -1\n",
            "22:22:12 |     nus: (0.7,)\n",
            "22:22:12 |     optimizer: mem_eff_adam\n",
            "22:22:12 |     output_scaling: 1.0\n",
            "22:22:12 |     override: \"{'task': 'jsonfile', 'jsonfile_datapath': '/content/drive/MyDrive/chatbot_data/data', 'jsonfile_datatype_extension': True, 'model': 'transformer/generator', 'model_file': '/content/drive/MyDrive/chatbot_model/model', 'init_model': 'zoo:tutorial_transformer_generator/model', 'n_heads': 16, 'n_layers': 8, 'n_positions': 512, 'text_truncate': 512, 'label_truncate': 128, 'ffn_size': 2048, 'embedding_size': 512, 'activation': 'gelu', 'variant': 'xlm', 'dict_lower': True, 'dict_tokenizer': 'bpe', 'dict_file': '/usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model.dict', 'learn_positional_embeddings': True, 'learningrate': 1e-05, 'optimizer': 'adam', 'warmup_updates': 5000, 'validation_metric': 'ppl', 'validation_every_n_secs': 3600.0, 'save_every_n_secs': 600.0, 'batchsize': 12, 'fp16': True, 'fp16_impl': 'mem_efficient', 'skip_generation': True, 'dynamic_batching': 'full', 'label_turns': 'both'}\"\n",
            "22:22:12 |     parlai_home: /usr/local/lib/python3.6/dist-packages\n",
            "22:22:12 |     person_tokens: False\n",
            "22:22:12 |     rank_candidates: False\n",
            "22:22:12 |     relu_dropout: 0.0\n",
            "22:22:12 |     save_after_valid: False\n",
            "22:22:12 |     save_every_n_secs: 600.0\n",
            "22:22:12 |     share_word_embeddings: True\n",
            "22:22:12 |     short_final_eval: False\n",
            "22:22:12 |     skip_generation: True\n",
            "22:22:12 |     special_tok_lst: None\n",
            "22:22:12 |     split_lines: False\n",
            "22:22:12 |     starttime: Feb15_22-21\n",
            "22:22:12 |     task: jsonfile\n",
            "22:22:12 |     temperature: 1.0\n",
            "22:22:12 |     tensorboard_log: False\n",
            "22:22:12 |     tensorboard_logdir: None\n",
            "22:22:12 |     text_truncate: 512\n",
            "22:22:12 |     topk: 10\n",
            "22:22:12 |     topp: 0.9\n",
            "22:22:12 |     truncate: -1\n",
            "22:22:12 |     update_freq: 1\n",
            "22:22:12 |     use_reply: label\n",
            "22:22:12 |     validation_cutoff: 1.0\n",
            "22:22:12 |     validation_every_n_epochs: -1\n",
            "22:22:12 |     validation_every_n_secs: 3600.0\n",
            "22:22:12 |     validation_max_exs: -1\n",
            "22:22:12 |     validation_metric: ppl\n",
            "22:22:12 |     validation_metric_mode: None\n",
            "22:22:12 |     validation_patience: 10\n",
            "22:22:12 |     validation_share_agent: False\n",
            "22:22:12 |     variant: xlm\n",
            "22:22:12 |     warmup_rate: 0.0001\n",
            "22:22:12 |     warmup_updates: 5000\n",
            "22:22:12 |     weight_decay: None\n",
            "22:22:12 | creating task(s): jsonfile\n",
            "22:22:12 | [loading data from json file into task:/content/drive/MyDrive/chatbot_data/data_train.jsonl]\n",
            "22:22:16 | \u001b[31mMetadata does not exist. Please double check your datapath.\u001b[0m\n",
            "22:22:20 | training...\n",
            "22:22:20 | Overflow: setting loss scale to 65536.0\n",
            "22:22:21 | Overflow: setting loss scale to 32768.0\n",
            "22:22:24 | Overflow: setting loss scale to 16384.0\n",
            "22:22:30 | time:10s total_exs:5396 epochs:0.01\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9000  4159 12504 540.8 5396             22938  10.47    .8196 4.105 6.099e-08 907.4  2729 60.65      .3061   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                     30 5066 15233 3.007\n",
            "\n",
            "22:22:40 | time:20s total_exs:8064 epochs:0.01\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  5602 15381 261.6 2668             16384   12.2    .8196 3.946 1.17e-07 521.6  1432 51.7      .3142   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                     58 6123 16813 2.746\n",
            "\n",
            "22:22:50 | time:31s total_exs:11012 epochs:0.01\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5429 18026 287.9 2948             16384  12.53    .7857 3.972 1.85e-07 448.3  1489 53.08      .3192   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                     92 5877 19515 3.321\n",
            "\n",
            "22:23:01 | time:41s total_exs:13200 epochs:0.02\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6089 20611 217.8 2188             16384  13.44    .6669 4.043 2.53e-07 346.8  1174 56.99      .3177   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    126 6436 21785 3.385\n",
            "\n",
            "22:23:11 | time:51s total_exs:15100 epochs:0.02\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6489 21982 189.3 1900             16384  14.32    .6639 3.886 3.21e-07 288.1 975.9 48.72      .3324   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    160 6777 22958 3.388\n",
            "\n",
            "22:23:21 | time:61s total_exs:16808 epochs:0.02\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6242 20943 168.5 1708             16384  14.76    .6669  3.99 3.89e-07 279.9   939 54.03      .3155   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    194 6522 21882 3.355\n",
            "\n",
            "22:23:31 | time:71s total_exs:18400 epochs:0.02\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6336 21351 157.8 1592             16384  14.89    .6713 3.976 4.57e-07 249.4 840.6 53.32      .3169   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    228 6585 22192 3.37\n",
            "\n",
            "22:23:41 | time:81s total_exs:19968 epochs:0.02\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6122 20805 156.7 1568             16384  14.64    .6940 3.979 5.249e-07 247.9 842.5 53.45      .3192   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    262 6370 21648 3.398\n",
            "\n",
            "22:23:51 | time:91s total_exs:21428 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6474 21735 144.2 1460             16384  14.71    .6583 3.936 5.929e-07 238.8 801.8 51.23      .3240   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    296 6713 22537 3.357\n",
            "\n",
            "22:24:01 | time:101s total_exs:22768 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6425 21037 132.9 1340             16384  15.48    .6810 3.969 6.589e-07 197.2 645.7 52.95      .3264   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    329 6622 21682 3.275\n",
            "\n",
            "22:24:11 | time:111s total_exs:23956 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6506 21287 117.8 1188             16384  15.33    .6670 3.924 7.249e-07   193 631.4 50.59      .3227   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    362 6699 21918 3.272\n",
            "\n",
            "22:24:21 | time:122s total_exs:25156 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6418 21136 119.7 1200             16384  15.71    .6701  3.98 7.909e-07 217.4   716 53.5      .3188   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    395 6635 21852 3.294\n",
            "\n",
            "22:24:32 | time:132s total_exs:26344 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6273 20373 116.9 1188             16384  15.92    .6990 4.016 8.569e-07 187.4 608.5 55.48      .3115   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    428 6461 20981 3.248\n",
            "\n",
            "22:24:42 | time:142s total_exs:27236 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6624 19092 88.65  892             16384  17.04    .6836 3.954 9.149e-07 153.3 441.9 52.15      .3194   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    457 6777 19534 2.883\n",
            "\n",
            "22:24:52 | time:152s total_exs:28200 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6384 19148 96.38  964             16384  16.76    .6833 3.986 9.749e-07 156.7 469.9 53.83      .3123   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    487 6541 19618    3\n",
            "\n",
            "22:25:02 | time:162s total_exs:29116 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6526 20761 91.06  916             16384  17.28    .6976 3.903 1.039e-06 169.8   540 49.56      .3203   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    519 6696 21301 3.181\n",
            "\n",
            "22:25:12 | time:172s total_exs:30132 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6313 20512   100 1016             16384     16    .7032 3.917 1.105e-06 167.3 543.6 50.25      .3251   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    552 6480 21055 3.249\n",
            "\n",
            "22:25:22 | time:182s total_exs:30996 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6708 21418 86.21  864             16384  16.87    .7126  3.93 1.169e-06 158.2 505.3 50.91      .3248   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    584 6866 21923 3.193\n",
            "\n",
            "22:25:32 | time:192s total_exs:31820 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6735 21014 80.34  824             16384  17.68    .6758 3.916 1.233e-06 127.4 397.4 50.21      .3258   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    616 6863 21411 3.12\n",
            "\n",
            "22:25:42 | time:203s total_exs:32736 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6120 19837 89.97  916             16384  15.95    .6739 3.964 1.299e-06 154.5 500.7 52.66      .3306   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    649 6274 20337 3.242\n",
            "\n",
            "22:25:51 | Overflow: setting loss scale to 16384.0\n",
            "22:25:53 | time:213s total_exs:33520 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9677  6600 20351 77.98  784             16384  17.31    .6837 3.854 1.361e-06 128.9 397.4 47.19      .3472   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    680 6729 20749 3.084\n",
            "\n",
            "22:26:03 | time:223s total_exs:34264 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6328 19434  73.7  744             16384  18.69    .7126 3.962 1.423e-06 133.6 410.3 52.58      .3329   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    711 6462 19844 3.071\n",
            "\n",
            "22:26:12 | Overflow: setting loss scale to 16384.0\n",
            "22:26:13 | time:233s total_exs:35004 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9677  6312 19221 72.69  740             16384  17.36    .7033 3.842 1.485e-06 138.1 420.6 46.64      .3148   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    742 6450 19641 3.045\n",
            "\n",
            "22:26:23 | time:243s total_exs:35784 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6450 20290 76.68  780             16384  18.03    .7135 3.841 1.549e-06 119.5 375.9 46.56      .3350   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    774 6569 20666 3.146\n",
            "\n",
            "22:26:33 | time:253s total_exs:36528 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6606 20050 72.84  744             16384  16.61    .7217 3.817 1.611e-06 130.3 395.4 45.48      .3268   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    805 6736 20446 3.035\n",
            "\n",
            "22:26:44 | time:264s total_exs:37260 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6431 19379 71.15  732             16384  18.26    .7257 4.009 1.673e-06 145.2 437.5 55.08      .3117   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    836 6577 19817 3.013\n",
            "\n",
            "22:26:54 | time:274s total_exs:37972 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6638 20257 70.09  712             16384  16.96    .7327 3.869 1.735e-06 125.2 382.1 47.89      .3162   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    867 6763 20639 3.052\n",
            "\n",
            "22:27:01 | Overflow: setting loss scale to 16384.0\n",
            "22:27:04 | time:284s total_exs:38628 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9677  6409 19346 63.88  656             16384  17.78    .7439 3.723 1.797e-06   116 350.1 41.38      .3335   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    898 6525 19696 3.019\n",
            "\n",
            "22:27:11 | Overflow: setting loss scale to 16384.0\n",
            "22:27:14 | time:294s total_exs:39244 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9677  6623 20217 60.65  616             16384     18    .7414 3.783 1.859e-06 110.6 337.7 43.96      .3391   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    929 6734 20555 3.053\n",
            "\n",
            "22:27:24 | time:305s total_exs:39888 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6546 19393  63.6  644             16384  18.56    .7569 3.671 1.919e-06   108 319.9 39.28      .3467   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    959 6654 19713 2.963\n",
            "\n",
            "22:27:35 | time:315s total_exs:40532 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6145 16968 63.51  644             16384  17.46    .7033 3.842 1.975e-06 131.9 364.3 46.61      .3227   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    987 6277 17333 2.762\n",
            "\n",
            "22:27:45 | time:325s total_exs:41156 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6487 19152  61.4  624             16384  18.02    .7474 3.908 2.035e-06 114.2 337.1 49.79      .3135   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1017 6601 19489 2.952\n",
            "\n",
            "22:27:55 | time:335s total_exs:41816 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6401 19809 65.88  660             16384   18.8    .7033  3.85 2.097e-06 116.8 361.6   47      .3291   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1048 6518 20171 3.095\n",
            "\n",
            "22:28:01 | Overflow: setting loss scale to 16384.0\n",
            "22:28:05 | time:345s total_exs:42444 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9677  6278 19218 62.01  628             16384  17.44    .7077 3.762 2.159e-06 106.7 326.5 43.05      .3432   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1079 6385 19545 3.061\n",
            "\n",
            "22:28:15 | time:355s total_exs:43004 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6517 19569 56.05  560             16384  18.46    .7217 3.817 2.219e-06 107.8 323.8 45.48      .3283   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1109 6625 19893 3.003\n",
            "\n",
            "22:28:25 | time:365s total_exs:43636 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6295 18771 62.82  632             16384  18.48    .7344 3.777 2.279e-06 112.8 336.4 43.7      .3322   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1139 6408 19108 2.982\n",
            "\n",
            "22:28:30 | Overflow: setting loss scale to 16384.0\n",
            "22:28:35 | time:375s total_exs:44296 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9667  6277 18797 65.88  660             16384  17.77    .7569 3.922 2.339e-06 121.6 364.3 50.48      .3086   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1169 6398 19162 2.995\n",
            "\n",
            "22:28:45 | time:386s total_exs:44888 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6371 18637 57.72  592             16384  18.21    .7440 3.834 2.399e-06 113.3 331.3 46.27      .3175   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1199 6484 18968 2.925\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-1f273592bf0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mdynamic_batching\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'full'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mlabel_turns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'both'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# https://parl.ai/docs/core/teachers.html#parlai.core.teachers.ConversationTeacher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36m_run_kwargs\u001b[0;34m(cls, kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_from_parser_and_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36m_run_from_parser_and_opt\u001b[0;34m(cls, opt, parser)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mscript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/scripts/train_model.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/scripts/train_model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    656\u001b[0m                 \u001b[0;31m# do one example / batch of examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m                     \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparley\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mStopTrainException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m                     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Stopping from {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/core/worlds.py\u001b[0m in \u001b[0;36mparley\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0;31m# great, this batch is good to go! let's run it!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m         \u001b[0macts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_acts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m         \u001b[0;31m# broadcast the results back to all the models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/core/torch_agent.py\u001b[0m in \u001b[0;36mbatch_act\u001b[0;34m(self, observations)\u001b[0m\n\u001b[1;32m   1967\u001b[0m             \u001b[0;31m# register the start of updates for later counting when they occur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1968\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ups'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGlobalTimerMetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1970\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/core/torch_generator_agent.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/core/torch_generator_agent.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, batch, return_output)\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_vec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot compute loss without a label.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m         \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0mscore_view\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/core/torch_generator_agent.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ys, prev_enc, maxlen, bsz, *xs)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;31m# use teacher forcing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_forced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/core/torch_generator_agent.py\u001b[0m in \u001b[0;36mdecode_forced\u001b[0;34m(self, encoder_states, ys)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mseqlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseqlen\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTART_IDX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m             raise AssertionError(\n\u001b[1;32m    191\u001b[0m                 \u001b[0;34m\"The Beginning of Sentence token is automatically added to the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}