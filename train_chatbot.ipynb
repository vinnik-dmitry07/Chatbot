{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN9WpQrW6ZkL6Xdj6mHiocf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinnik-dmitry07/Chatbot/blob/main/train_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAbMlxO9DI4h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cdeda94-d765-4c99-881d-ed2394563cb0"
      },
      "source": [
        "!nvidia-smi\r\n",
        "!pip install --quiet parlai"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Feb 18 17:11:22 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    31W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 5.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 9.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.0MB 37.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 57.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.8MB 44.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 61.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 317kB 59.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 40kB 7.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 61.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 63.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 52.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 11.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 46.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 552kB 28.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.2MB 46.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 58.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 40kB 5.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 11.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 12.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 12.3MB/s \n",
            "\u001b[?25h  Building wheel for websocket-server (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docformatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for untokenize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: torchtext 0.8.1 has requirement torch==1.7.1, but you'll have torch 1.7.0+cu101 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: botocore 1.20.10 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: sphinx-autodoc-typehints 1.11.1 has requirement Sphinx>=3.0, but you'll have sphinx 1.8.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: myst-parser 0.13.5 has requirement sphinx<4,>=2, but you'll have sphinx 1.8.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fvcore 0.1.3.post20210218 has requirement pyyaml>=5.1, but you'll have pyyaml 3.13 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY3EisQ766aP"
      },
      "source": [
        "from pathlib import Path\r\n",
        "\r\n",
        "GDRIVE_ROOT = Path('/content/drive/MyDrive/')\r\n",
        "SAVE_DIR = GDRIVE_ROOT / 'chatbot_model'\r\n",
        "DATA_DIR = GDRIVE_ROOT / 'chatbot_data'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9y2IWmWDFjU"
      },
      "source": [
        "from datetime import timedelta\r\n",
        "\r\n",
        "EPISODE_DT = timedelta(minutes=3)  # change to split messages in separate dialogues if time delta is greater than EPISODE_DT\r\n",
        "TRAIN_PART, TEST_PART, VALID_PART = 0.996, 0.002, 0.002\r\n",
        "\r\n",
        "assert TRAIN_PART + TEST_PART + VALID_PART == 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr8wrt68ATZl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81409a7f-97e3-491b-de98-0ffb5f172367"
      },
      "source": [
        "from google.colab import drive\r\n",
        "\r\n",
        "drive.mount(str(GDRIVE_ROOT.parent))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCM-AVZhHKFK"
      },
      "source": [
        "import json\r\n",
        "\r\n",
        "with open(DATA_DIR / 'result.json', 'r', encoding='utf8') as f:\r\n",
        "    raw_messages = json.load(f)['messages']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3CvK47rIujd"
      },
      "source": [
        "from datetime import datetime\r\n",
        "\r\n",
        "filtered_messages = []\r\n",
        "for msg in raw_messages:\r\n",
        "    if (\r\n",
        "            'from' in msg and\r\n",
        "            'from_id' in msg and\r\n",
        "            'mime_type' not in msg and\r\n",
        "            msg['text'] and\r\n",
        "            isinstance(msg['text'], str) and\r\n",
        "            len(msg['text']) < 50\r\n",
        "    ):\r\n",
        "        msg1 = msg.copy()\r\n",
        "        msg1['date'] = datetime.strptime(msg1['date'], '%Y-%m-%dT%H:%M:%S')\r\n",
        "        filtered_messages.append(msg1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz06qsAJJLda"
      },
      "source": [
        "import re\r\n",
        "\r\n",
        "joined_messages = []\r\n",
        "for i in range(len(filtered_messages)):\r\n",
        "    alphanum_text = re.sub(r'[^A-Za-z0-9 ]+', '', filtered_messages[i]['text']).strip()\r\n",
        "    if alphanum_text:\r\n",
        "        if (    \r\n",
        "                joined_messages and    \r\n",
        "                filtered_messages[i - 1]['from_id'] == filtered_messages[i]['from_id'] and\r\n",
        "                filtered_messages[i - 1]['date'] - filtered_messages[i]['date'] <= EPISODE_DT\r\n",
        "        ):\r\n",
        "            joined_messages[-1]['text'] += ' ' + alphanum_text\r\n",
        "        else:\r\n",
        "            new_message = filtered_messages[i].copy()\r\n",
        "            new_message['text'] = alphanum_text\r\n",
        "            joined_messages.append(new_message)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUphzoX3K_FF"
      },
      "source": [
        "def partition(alist, indices):\r\n",
        "    return [alist[a:b] for a, b in zip([0] + indices, indices + [None])]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXMAHZTxL-KN"
      },
      "source": [
        "def save_jsonl(messages, suffix, human_readable=False):\r\n",
        "    time_diffs = [messages[i + 1]['date'] - messages[i]['date'] for i in range(len(messages) - 1)]\r\n",
        "    split_positions = [i + 1 for i in range(len(time_diffs)) if time_diffs[i] > EPISODE_DT]\r\n",
        "    episodes = partition(messages, split_positions)\r\n",
        "    print(f'{suffix} episodes: {len(episodes)}, messages: {len(messages)}')\r\n",
        "\r\n",
        "    with open(DATA_DIR / f'data_{suffix}.jsonl', 'w', **({'encoding': 'utf8'} if human_readable else {})) as outfile:\r\n",
        "        for episode in episodes:\r\n",
        "            dialog = [{'id': i % 2, 'text': msg['text']} for i, msg in enumerate(episode)]\r\n",
        "            episode = {'dialog': [dialog]}\r\n",
        "            json.dump(episode, outfile, **({'ensure_ascii': False} if human_readable else {}))\r\n",
        "            outfile.write('\\n')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApxWsw2omwax",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "d59be9a6-efbd-4dd8-ea94-851e3c422e34"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "train, test, valid = np.split(joined_messages, [\r\n",
        "    int(TRAIN_PART * len(joined_messages)),\r\n",
        "    int((TRAIN_PART + TEST_PART) * len(joined_messages)),\r\n",
        "])\r\n",
        "\r\n",
        "save_jsonl(train, suffix='train')\r\n",
        "save_jsonl(test, suffix='test')\r\n",
        "save_jsonl(valid, suffix='valid')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-131b13736c4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m train, test, valid = np.split(joined_messages, [\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_PART\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoined_messages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_PART\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mTEST_PART\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoined_messages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'joined_messages' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5JX4EM1Jb56",
        "outputId": "128669b8-eb70-4282-a4f3-8220e436b970"
      },
      "source": [
        "import shutil\r\n",
        "import subprocess\r\n",
        "import time\r\n",
        "import threading\r\n",
        "from pathlib import Path\r\n",
        "\r\n",
        "\r\n",
        "def dir_size_bytes(path):\r\n",
        "    return int(subprocess.check_output(['du','--bytes', '--summarize', path]).split()[0].decode())\r\n",
        "\r\n",
        "def check_chache(max_cache_size_gb=18, check_period_minutes=5):\r\n",
        "    this_id = str(threading.get_ident())\r\n",
        "    thread_path = Path('/threads')\r\n",
        "    thread_path.mkdir(exist_ok=True)\r\n",
        "\r\n",
        "    def threads_ids():\r\n",
        "        return [str(p.name) for p in thread_path.iterdir() if p.is_file()]\r\n",
        "    \r\n",
        "    if not(threads_ids()):\r\n",
        "        (thread_path / this_id).open(mode='w').close()\r\n",
        "        while True:\r\n",
        "            ids = threads_ids()\r\n",
        "            if not (len(ids) == 1 and ids[0] == this_id):\r\n",
        "                break\r\n",
        "\r\n",
        "            print(f'Thread {this_id} is checking chache.')\r\n",
        "\r\n",
        "            for cache_path in Path('/root/.config/Google/DriveFS').glob('**/content_cache'):\r\n",
        "                chache_path_str = str(cache_path)\r\n",
        "                chache_size_gb = dir_size_bytes(chache_path_str) / 10 ** 9\r\n",
        "                if chache_size_gb > max_cache_size_gb:\r\n",
        "                    print(f'Deleting {chache_path_str} with size {chache_size_gb} GB.')\r\n",
        "                    shutil.rmtree(chache_path_str)\r\n",
        "            time.sleep(check_period_minutes * 60)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HWEdG7hMboN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c52f28c6-db59-4c66-d4e8-1a055792d3ce"
      },
      "source": [
        "threading.Thread(target=check_chache).start()\r\n",
        "\r\n",
        "import os\r\n",
        "\r\n",
        "os.environ['SAVE_DIR'] = str(SAVE_DIR)\r\n",
        "!rm --recursive --force $SAVE_DIR\r\n",
        "!mkdir --parents $SAVE_DIR\r\n",
        "\r\n",
        "\r\n",
        "from parlai.scripts.train_model import TrainModel\r\n",
        "\r\n",
        "TrainModel.main(\r\n",
        "    task='jsonfile',\r\n",
        "    jsonfile_datapath=str(DATA_DIR / 'data'),\r\n",
        "    jsonfile_datatype_extension=True,\r\n",
        "\r\n",
        "    model='transformer/generator',\r\n",
        "    model_file=str(SAVE_DIR / 'model'),\r\n",
        "    \r\n",
        "    init_model='zoo:tutorial_transformer_generator/model',\r\n",
        "\r\n",
        "    n_heads=16, n_layers=8, n_positions=512, text_truncate=512,\r\n",
        "    label_truncate=128, ffn_size=2048, embedding_size=512,\r\n",
        "    activation='gelu', variant='xlm',\r\n",
        "    dict_lower=True, dict_tokenizer='bpe',\r\n",
        "    dict_file='zoo:tutorial_transformer_generator/model.dict',\r\n",
        "    learn_positional_embeddings=True,\r\n",
        "    \r\n",
        "    lr=1e-5, optimizer='adam',\r\n",
        "    warmup_updates=5000,\r\n",
        "    validation_metric='ppl',\r\n",
        "    validation_every_n_secs=60 * 60,  # running eval: valid\r\n",
        "    save_every_n_secs=60,  # saving model checkpoint\r\n",
        "\r\n",
        "    batchsize=12, fp16=True, fp16_impl='mem_efficient',\r\n",
        "    \r\n",
        "    skip_generation=True,\r\n",
        "    \r\n",
        "    dynamic_batching='full',\r\n",
        "\r\n",
        "    label_turns='both',  # https://parl.ai/docs/core/teachers.html#parlai.core.teachers.ConversationTeacher\r\n",
        ")"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14:33:21 | building dictionary first...\n",
            "14:33:21 | No model with opt yet at: /content/drive/MyDrive/chatbot_model/model(.opt)\n",
            "14:33:21 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: full,datapath: /usr/local/lib/python3.6/dist-packages/data,tensorboard_logdir: None,jsonfile_datapath: /content/drive/MyDrive/chatbot_data/data,jsonfile_datatype_extension: True,label_turns: both,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,beam_block_full_context: True,beam_length_penalty: 0.65,topk: 10,topp: 0.9,beam_delay: 30,beam_block_list_filename: None,temperature: 1.0,compute_tokenized_bleu: False,interactive_mode: False,fp16_impl: mem_efficient,force_fp16_tokens: False,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,hf_skip_special_tokens: True,max_lr_steps: -1,invsqrt_lr_decay_gamma: -1,parlai_home: /usr/local/lib/python3.6/dist-packages\u001b[0m\n",
            "14:33:21 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--show-advanced-args False --task internal:new_reddit:presorted --datatype train:stream --numthreads 1 --batchsize 48 --num-epochs 5.0 --validation-every-n-secs 1800.0 --save-every-n-secs -1 --save-after-valid True --validation-max-exs 9920 --short-final-eval True --validation-patience 0 --validation-metric-mode min --dict-build-first True --numworkers 4 --pytorch-preprocess False --pytorch-teacher-batch-sort False --batch-sort-cache-type pop --batch-length-range 5 --shuffle False --batch-sort-field text --pytorch-context-length -1 --pytorch-include-labels True --log-every-n-secs 30.0 --distributed-world-size 64 --verbose False --port 61337 --dropout 0.1 --beam-size 8 --beam-min-n-best 3 --beam-min-length 10 --skip-generation False --inference beam --optimizer fused_adam --learningrate 0.0005 --gradient-clip 10.0 --adam-eps 1e-06 --betas 0.9,0.98 --weight-decay 0.01 --lr-scheduler invsqrt --warmup-updates 20000 --gpu 0 --beam-block-ngram 3 --beam-context-block-ngram 3\u001b[0m\n",
            "14:33:21 | Using CUDA\n",
            "14:33:21 | loading dictionary from /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "14:33:21 | num words = 54944\n",
            "14:33:23 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "14:33:23 | Loading existing model params from /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "14:33:57 | \u001b[33mNot loading optim state since optim class changed.\u001b[0m\n",
            "14:33:58 | Opt:\n",
            "14:33:58 |     activation: gelu\n",
            "14:33:58 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "14:33:58 |     adam_eps: 1e-08\n",
            "14:33:58 |     add_p1_after_newln: False\n",
            "14:33:58 |     aggregate_micro: False\n",
            "14:33:58 |     allow_missing_init_opts: False\n",
            "14:33:58 |     attention_dropout: 0.0\n",
            "14:33:58 |     batchsize: 12\n",
            "14:33:58 |     beam_block_full_context: True\n",
            "14:33:58 |     beam_block_list_filename: None\n",
            "14:33:58 |     beam_block_ngram: -1\n",
            "14:33:58 |     beam_context_block_ngram: -1\n",
            "14:33:58 |     beam_delay: 30\n",
            "14:33:58 |     beam_length_penalty: 0.65\n",
            "14:33:58 |     beam_min_length: 1\n",
            "14:33:58 |     beam_size: 1\n",
            "14:33:58 |     betas: '(0.9, 0.999)'\n",
            "14:33:58 |     bpe_add_prefix_space: None\n",
            "14:33:58 |     bpe_debug: False\n",
            "14:33:58 |     bpe_merge: None\n",
            "14:33:58 |     bpe_vocab: None\n",
            "14:33:58 |     compute_tokenized_bleu: False\n",
            "14:33:58 |     datapath: /usr/local/lib/python3.6/dist-packages/data\n",
            "14:33:58 |     datatype: train\n",
            "14:33:58 |     delimiter: '\\n'\n",
            "14:33:58 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "14:33:58 |     dict_endtoken: __end__\n",
            "14:33:58 |     dict_file: /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "14:33:58 |     dict_include_test: False\n",
            "14:33:58 |     dict_include_valid: False\n",
            "14:33:58 |     dict_initpath: None\n",
            "14:33:58 |     dict_language: english\n",
            "14:33:58 |     dict_loaded: True\n",
            "14:33:58 |     dict_lower: True\n",
            "14:33:58 |     dict_max_ngram_size: -1\n",
            "14:33:58 |     dict_maxexs: -1\n",
            "14:33:58 |     dict_maxtokens: -1\n",
            "14:33:58 |     dict_minfreq: 0\n",
            "14:33:58 |     dict_nulltoken: __null__\n",
            "14:33:58 |     dict_starttoken: __start__\n",
            "14:33:58 |     dict_textfields: text,labels\n",
            "14:33:58 |     dict_tokenizer: bpe\n",
            "14:33:58 |     dict_unktoken: __unk__\n",
            "14:33:58 |     display_examples: False\n",
            "14:33:58 |     download_path: None\n",
            "14:33:58 |     dropout: 0.0\n",
            "14:33:58 |     dynamic_batching: full\n",
            "14:33:58 |     embedding_projection: random\n",
            "14:33:58 |     embedding_size: 512\n",
            "14:33:58 |     embedding_type: random\n",
            "14:33:58 |     embeddings_scale: True\n",
            "14:33:58 |     eval_batchsize: None\n",
            "14:33:58 |     evaltask: None\n",
            "14:33:58 |     ffn_size: 2048\n",
            "14:33:58 |     force_fp16_tokens: False\n",
            "14:33:58 |     fp16: True\n",
            "14:33:58 |     fp16_impl: mem_efficient\n",
            "14:33:58 |     gpu: -1\n",
            "14:33:58 |     gradient_clip: 0.1\n",
            "14:33:58 |     hf_skip_special_tokens: True\n",
            "14:33:58 |     hide_labels: False\n",
            "14:33:58 |     history_add_global_end_token: None\n",
            "14:33:58 |     history_reversed: False\n",
            "14:33:58 |     history_size: -1\n",
            "14:33:58 |     image_cropsize: 224\n",
            "14:33:58 |     image_mode: raw\n",
            "14:33:58 |     image_size: 256\n",
            "14:33:58 |     inference: greedy\n",
            "14:33:58 |     init_model: /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "14:33:58 |     init_opt: None\n",
            "14:33:58 |     interactive_mode: False\n",
            "14:33:58 |     invsqrt_lr_decay_gamma: -1\n",
            "14:33:58 |     jsonfile_datapath: /content/drive/MyDrive/chatbot_data/data\n",
            "14:33:58 |     jsonfile_datatype_extension: True\n",
            "14:33:58 |     label_truncate: 128\n",
            "14:33:58 |     label_turns: both\n",
            "14:33:58 |     learn_positional_embeddings: True\n",
            "14:33:58 |     learningrate: 1e-05\n",
            "14:33:58 |     load_from_checkpoint: True\n",
            "14:33:58 |     log_every_n_secs: 10\n",
            "14:33:58 |     loglevel: info\n",
            "14:33:58 |     lr_scheduler: reduceonplateau\n",
            "14:33:58 |     lr_scheduler_decay: 0.5\n",
            "14:33:58 |     lr_scheduler_patience: 3\n",
            "14:33:58 |     max_lr_steps: -1\n",
            "14:33:58 |     max_train_time: -1\n",
            "14:33:58 |     metrics: default\n",
            "14:33:58 |     model: transformer/generator\n",
            "14:33:58 |     model_file: /content/drive/MyDrive/chatbot_model/model\n",
            "14:33:58 |     model_parallel: False\n",
            "14:33:58 |     momentum: 0\n",
            "14:33:58 |     multitask_weights: [1]\n",
            "14:33:58 |     n_decoder_layers: -1\n",
            "14:33:58 |     n_encoder_layers: -1\n",
            "14:33:58 |     n_heads: 16\n",
            "14:33:58 |     n_layers: 8\n",
            "14:33:58 |     n_positions: 512\n",
            "14:33:58 |     n_segments: 0\n",
            "14:33:58 |     nesterov: True\n",
            "14:33:58 |     no_cuda: False\n",
            "14:33:58 |     num_epochs: -1\n",
            "14:33:58 |     nus: (0.7,)\n",
            "14:33:58 |     optimizer: mem_eff_adam\n",
            "14:33:58 |     output_scaling: 1.0\n",
            "14:33:58 |     override: \"{'task': 'jsonfile', 'jsonfile_datapath': '/content/drive/MyDrive/chatbot_data/data', 'jsonfile_datatype_extension': True, 'model': 'transformer/generator', 'model_file': '/content/drive/MyDrive/chatbot_model/model', 'init_model': 'zoo:tutorial_transformer_generator/model', 'n_heads': 16, 'n_layers': 8, 'n_positions': 512, 'text_truncate': 512, 'label_truncate': 128, 'ffn_size': 2048, 'embedding_size': 512, 'activation': 'gelu', 'variant': 'xlm', 'dict_lower': True, 'dict_tokenizer': 'bpe', 'dict_file': '/usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model.dict', 'learn_positional_embeddings': True, 'learningrate': 1e-05, 'optimizer': 'adam', 'warmup_updates': 5000, 'validation_metric': 'ppl', 'validation_every_n_secs': 3600.0, 'save_every_n_secs': 600.0, 'batchsize': 12, 'fp16': True, 'fp16_impl': 'mem_efficient', 'skip_generation': True, 'dynamic_batching': 'full', 'label_turns': 'both'}\"\n",
            "14:33:58 |     parlai_home: /usr/local/lib/python3.6/dist-packages\n",
            "14:33:58 |     person_tokens: False\n",
            "14:33:58 |     rank_candidates: False\n",
            "14:33:58 |     relu_dropout: 0.0\n",
            "14:33:58 |     save_after_valid: False\n",
            "14:33:58 |     save_every_n_secs: 600.0\n",
            "14:33:58 |     share_word_embeddings: True\n",
            "14:33:58 |     short_final_eval: False\n",
            "14:33:58 |     skip_generation: True\n",
            "14:33:58 |     special_tok_lst: None\n",
            "14:33:58 |     split_lines: False\n",
            "14:33:58 |     starttime: Feb18_14-33\n",
            "14:33:58 |     task: jsonfile\n",
            "14:33:58 |     temperature: 1.0\n",
            "14:33:58 |     tensorboard_log: False\n",
            "14:33:58 |     tensorboard_logdir: None\n",
            "14:33:58 |     text_truncate: 512\n",
            "14:33:58 |     topk: 10\n",
            "14:33:58 |     topp: 0.9\n",
            "14:33:58 |     truncate: -1\n",
            "14:33:58 |     update_freq: 1\n",
            "14:33:58 |     use_reply: label\n",
            "14:33:58 |     validation_cutoff: 1.0\n",
            "14:33:58 |     validation_every_n_epochs: -1\n",
            "14:33:58 |     validation_every_n_secs: 3600.0\n",
            "14:33:58 |     validation_max_exs: -1\n",
            "14:33:58 |     validation_metric: ppl\n",
            "14:33:58 |     validation_metric_mode: None\n",
            "14:33:58 |     validation_patience: 10\n",
            "14:33:58 |     validation_share_agent: False\n",
            "14:33:58 |     variant: xlm\n",
            "14:33:58 |     warmup_rate: 0.0001\n",
            "14:33:58 |     warmup_updates: 5000\n",
            "14:33:58 |     weight_decay: None\n",
            "14:33:58 | creating task(s): jsonfile\n",
            "14:33:58 | [loading data from json file into task:/content/drive/MyDrive/chatbot_data/data_train.jsonl]\n",
            "14:34:01 | \u001b[31mMetadata does not exist. Please double check your datapath.\u001b[0m\n",
            "14:34:04 | training...\n",
            "14:34:05 | Overflow: setting loss scale to 65536.0\n",
            "14:34:06 | Overflow: setting loss scale to 32768.0\n",
            "14:34:06 | Overflow: setting loss scale to 16384.0\n",
            "14:34:15 | time:10s total_exs:4160 epochs:0.01\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss      lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .8500  3104  6141 411.4 4160             19661  11.43    .5302 4.493 4.1e-08 975.3  1929 89.37      .3004   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                     20 4079 8070 1.978\n",
            "\n",
            "14:34:17 | Overflow: setting loss scale to 16384.0\n",
            "14:34:25 | time:21s total_exs:6996 epochs:0.01\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9565  5026 11213 275.1 2836             16384  12.55    .5294 4.316 8.699e-08   616  1374 74.89      .3107   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                     43 5642 12588 2.231\n",
            "\n",
            "14:34:34 | Overflow: setting loss scale to 16384.0\n",
            "14:34:35 | time:31s total_exs:9108 epochs:0.01\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9565  5276 12126   211 2112             16384   13.7    .4375 4.291 1.33e-07 446.1  1025 73.07      .3161   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                     66 5723 13152 2.298\n",
            "\n",
            "14:34:45 | time:41s total_exs:11000 epochs:0.01\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5836 13791 186.3 1892             16384  13.86    .4380 4.386 1.81e-07 399.6 944.4 80.34      .3149   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                     90 6235 14736 2.364\n",
            "\n",
            "14:34:55 | time:51s total_exs:12624 epochs:0.02\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5594 12852 162.2 1624             16384   14.1    .4686 4.289 2.27e-07   383 879.8 72.88      .3216   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    113 5977 13732 2.298\n",
            "\n",
            "14:35:02 | Overflow: setting loss scale to 16384.0\n",
            "14:35:05 | time:61s total_exs:14208 epochs:0.02\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9600  6250 15261 154.7 1584             16384  14.09    .3709  4.37 2.77e-07 324.4   792 79.05      .3199   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    138 6575 16054 2.442\n",
            "\n",
            "14:35:16 | time:71s total_exs:15520 epochs:0.02\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6140 14454 128.7 1312             16384  14.23    .4448 4.364 3.25e-07   294 692.1 78.54      .3232   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    162 6434 15146 2.354\n",
            "\n",
            "14:35:26 | time:82s total_exs:16760 epochs:0.02\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6428 15875 122.5 1240             16384  15.97    .3812 4.355 3.75e-07 245.5 606.2 77.9      .3251   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    187 6674 16481 2.47\n",
            "\n",
            "14:35:36 | time:92s total_exs:18036 epochs:0.02\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6351 15498 124.5 1276             16384  15.79    .3793 4.404 4.25e-07 252.5 616.2 81.77      .3108   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    212 6604 16115 2.44\n",
            "\n",
            "14:35:46 | time:102s total_exs:19172 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6351 15618 111.7 1136             16384  16.43    .3936 4.226 4.75e-07 214.4 527.1 68.41      .3357   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    237 6566 16145 2.459\n",
            "\n",
            "14:35:57 | time:112s total_exs:20244 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6217 15237 105.1 1072             16384  15.85    .3715 4.288 5.249e-07 215.2 527.5 72.84      .3206   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    262 6432 15764 2.451\n",
            "\n",
            "14:36:07 | time:123s total_exs:21244 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6401 15419 96.35 1000             16384  17.29    .4098 4.298 5.749e-07 228.8   551 73.58      .3228   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    287 6630 15970 2.409\n",
            "\n",
            "14:36:17 | time:133s total_exs:22320 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5995 14257 106.6 1076             16384  16.22    .4444 4.391 6.229e-07 241.4   574 80.68      .3055   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    311 6236 14831 2.378\n",
            "\n",
            "14:36:27 | time:143s total_exs:23292 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6621 16054 94.27  972             16384  16.54    .3973 4.394 6.729e-07 182.9 443.5 80.93      .3193   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    336 6804 16498 2.425\n",
            "\n",
            "14:36:38 | time:153s total_exs:24264 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6269 15230 94.45  972             16384  18.26    .4053 4.295 7.229e-07 201.4 489.2 73.31      .3234   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    361 6471 15719 2.43\n",
            "\n",
            "14:36:48 | time:163s total_exs:25160 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6455 15470 89.47  896             16384  16.71    .4449 4.324 7.709e-07 179.1 429.3 75.51      .3305   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    385 6634 15899 2.397\n",
            "\n",
            "14:36:58 | time:173s total_exs:26076 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6275 15070 91.66  916             16384  18.09    .4449 4.405 8.189e-07 187.7 450.7 81.87      .3093   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    409 6462 15520 2.402\n",
            "\n",
            "14:37:08 | time:184s total_exs:27036 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6301 14921 94.72  960             16384  16.61    .3953 4.385 8.669e-07 209.2 495.5 80.21      .3150   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    433 6510 15417 2.368\n",
            "\n",
            "14:37:12 | Overflow: setting loss scale to 16384.0\n",
            "14:37:18 | time:194s total_exs:27940 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9600  6442 15525 87.14  904             16384  15.91    .3799 4.283 9.169e-07   175 421.8 72.49      .3304   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    458 6617 15947 2.41\n",
            "\n",
            "14:37:29 | time:204s total_exs:28728 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6547 15872 76.41  788             16384  17.82    .3893 4.398 9.669e-07 165.2 400.5 81.25      .3094   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    483 6712 16272 2.424\n",
            "\n",
            "14:37:39 | time:215s total_exs:29516 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6524 15422 77.61  788             16384  17.49    .3856 4.298 1.015e-06 174.5 412.5 73.54      .3133   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    507 6698 15835 2.364\n",
            "\n",
            "14:37:49 | time:225s total_exs:30208 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6621 16233 67.86  692             16384  18.06    .3842 4.265 1.065e-06 142.5 349.3 71.13      .3212   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    532 6763 16582 2.452\n",
            "\n",
            "14:37:59 | time:235s total_exs:31012 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6261 15051 77.31  804             16384  18.07    .3826 4.281 1.115e-06 146.7 352.7 72.3      .3274   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    557 6407 15404 2.404\n",
            "\n",
            "14:38:10 | time:245s total_exs:31696 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6409 15289 67.99  684             16384  18.19    .3936 4.308 1.163e-06 160.9 383.8 74.32      .3326   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    581 6570 15673 2.386\n",
            "\n",
            "14:38:20 | time:255s total_exs:32412 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6601 15583 70.42  716             16384  17.36    .4271 4.365 1.211e-06 147.3 347.8 78.61      .3133   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    605 6749 15931 2.361\n",
            "\n",
            "14:38:30 | time:266s total_exs:33148 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6404 15449 71.02  736             16384  18.12    .3795  4.22 1.261e-06 145.2 350.4 68.01      .3451   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    630 6549 15800 2.413\n",
            "\n",
            "14:38:40 | time:276s total_exs:33768 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6402 15326 61.85  620             16384  18.94    .4358 4.244 1.309e-06 127.1 304.3 69.69      .3348   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    654 6529 15631 2.394\n",
            "\n",
            "14:38:50 | time:286s total_exs:34484 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6537 15516 70.81  716             16384  17.27    .4116 4.153 1.357e-06 137.5 326.3 63.61      .3362   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    678 6674 15842 2.374\n",
            "\n",
            "14:39:00 | time:296s total_exs:35124 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6531 15597 63.68  640             16384  18.17    .4019  4.28 1.405e-06 148.4 354.4 72.23      .3141   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    702 6679 15951 2.388\n",
            "\n",
            "14:39:06 | Overflow: setting loss scale to 16384.0\n",
            "14:39:10 | time:306s total_exs:35720 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9583  6601 15832 59.56  596             16384  18.23    .4159 4.221 1.453e-06 127.4 305.5 68.09      .3219   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    726 6728 16137 2.399\n",
            "\n",
            "14:39:11 | Overflow: setting loss scale to 16384.0\n",
            "14:39:21 | time:316s total_exs:36372 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9583  6158 14653 64.64  652             16384  17.24    .4051 4.288 1.501e-06   153 364.1 72.81      .3243   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    750 6311 15017 2.38\n",
            "\n",
            "14:39:31 | time:326s total_exs:36904 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6803 16314 53.16  532             16384   21.4    .4159 4.105 1.549e-06    96 230.2 60.66      .3407   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    774 6899 16544 2.398\n",
            "\n",
            "14:39:41 | time:336s total_exs:37560 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6048 14227 64.29  656             16384  18.43    .4555 4.351 1.597e-06 137.4 323.2 77.52      .3129   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    798 6186 14550 2.352\n",
            "\n",
            "14:39:51 | time:347s total_exs:38124 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6457 15052 54.78  564             16384  20.67    .4188 4.381 1.645e-06 117.7 274.3 79.96      .3215   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    822 6575 15326 2.331\n",
            "\n",
            "14:40:01 | time:357s total_exs:38648 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6777 15917 51.28  524             16384  18.66    .4291 4.145 1.693e-06 113.9 267.5 63.12      .3427   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    846 6891 16185 2.349\n",
            "\n",
            "14:40:12 | time:367s total_exs:39220 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6620 15437 55.57  572             16384  17.98    .4187 4.249 1.741e-06 130.7 304.8 70.03      .3191   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    870 6751 15742 2.332\n",
            "\n",
            "14:40:22 | time:378s total_exs:39800 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6589 15569  57.1  580             16384  18.92    .4245 4.303 1.789e-06 124.1 293.2 73.95      .3227   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    894 6713 15862 2.363\n",
            "\n",
            "14:40:23 | Overflow: setting loss scale to 16384.0\n",
            "14:40:32 | time:388s total_exs:40412 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9583  6377 14967 59.85  612             16384  17.43    .4127 4.129 1.837e-06   122 286.4 62.14      .3322   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    918 6499 15254 2.347\n",
            "\n",
            "14:40:34 | Overflow: setting loss scale to 16384.0\n",
            "14:40:42 | time:398s total_exs:40968 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9583  6362 15040 54.77  556             16384  19.02    .4486 4.164 1.885e-06 124.5 294.3 64.35      .3394   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    942 6486 15335 2.364\n",
            "\n",
            "14:40:53 | time:408s total_exs:41588 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5993 14444 59.77  620             16384  18.43    .4127   4.2 1.935e-06 116.4 280.5 66.65      .3361   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    967 6110 14725 2.41\n",
            "\n",
            "14:41:03 | time:419s total_exs:42140 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6494 15309 54.22  552             16384  19.32    .4127 4.174 1.983e-06 115.1 271.3 64.99      .3345   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    991 6609 15581 2.358\n",
            "\n",
            "14:41:03 | Overflow: setting loss scale to 16384.0\n",
            "14:41:13 | time:429s total_exs:42696 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9565  6469 14762 55.16  556             16384  18.19    .4655 4.194 2.029e-06 122.3   279 66.26      .3176   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1014 6591 15041 2.282\n",
            "\n",
            "14:41:14 | Overflow: setting loss scale to 16384.0\n",
            "14:41:15 | Overflow: setting loss scale to 16384.0\n",
            "14:41:23 | time:439s total_exs:43156 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9167  6603 15654 45.44  460             16384  18.14    .4275 3.981 2.077e-06 94.83 224.8 53.58      .3484   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1038 6698 15879 2.371\n",
            "\n",
            "14:41:27 | Overflow: setting loss scale to 16384.0\n",
            "14:41:33 | time:449s total_exs:43636 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9583  6424 15053 46.87  480             16384  18.61    .4655 4.141 2.125e-06 107.7 252.3 62.84      .3398   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1062 6531 15305 2.344\n",
            "\n",
            "14:41:43 | time:459s total_exs:44180 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6010 13711 53.96  544             16384  19.78    .4609 4.154 2.171e-06 133.4 304.3 63.69      .3243   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1085 6143 14015 2.282\n",
            "\n",
            "14:41:54 | time:469s total_exs:44704 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6487 15264 51.37  524             16384  18.58    .4655 4.126 2.219e-06 110.8 260.7 61.91      .3415   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1109 6598 15524 2.353\n",
            "\n",
            "14:42:04 | time:480s total_exs:45224 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6476 15049 50.35  520             16384  19.43    .4448 4.029 2.267e-06 105.3 244.8 56.2      .3501   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1133 6581 15294 2.324\n",
            "\n",
            "14:42:14 | time:490s total_exs:45724 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6576 15267 48.36  500             16384  19.67    .4570 4.158 2.315e-06 111.8 259.4 63.93      .3289   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1157 6688 15527 2.322\n",
            "\n",
            "14:42:25 | time:500s total_exs:46204 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6379 15118 47.39  480             16384  19.39    .4191 3.977 2.363e-06  95.5 226.3 53.35      .3551   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   1181 6475 15344 2.37\n",
            "\n",
            "14:42:35 | time:510s total_exs:46684 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6249 14801 47.37  480             16384  20.15    .4487 4.117 2.411e-06 103.9 246.1 61.4      .3356   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1205 6353 15047 2.369\n",
            "\n",
            "14:42:45 | time:521s total_exs:47176 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6243 14687 48.22  492             16384  20.42    .4407 4.029 2.459e-06 104.4 245.6 56.21      .3452   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1229 6348 14933 2.353\n",
            "\n",
            "14:42:55 | time:531s total_exs:47644 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6536 15027 44.83  468             16384  20.26    .4655 4.011 2.507e-06 100.7 231.5 55.23      .3361   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1253 6636 15258 2.299\n",
            "\n",
            "14:43:06 | time:541s total_exs:48216 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6216 14417 55.27  572             16384  19.66    .4654 4.183 2.555e-06   114 264.5 65.59      .3486   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1277 6330 14682 2.319\n",
            "\n",
            "14:43:16 | time:552s total_exs:48652 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6668 15366 41.86  436             16384  19.59    .4449 4.092 2.603e-06 96.58 222.6 59.85      .3253   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1301 6765 15589 2.305\n",
            "\n",
            "14:43:25 | Overflow: setting loss scale to 16384.0\n",
            "14:43:26 | time:562s total_exs:49112 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9565  6619 15117 45.68  460             16384  18.11    .4538 3.996 2.649e-06 97.57 222.8 54.38      .3583   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1324 6717 15340 2.284\n",
            "\n",
            "14:43:34 | Overflow: setting loss scale to 16384.0\n",
            "14:43:36 | time:572s total_exs:49580 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9565  6569 15023 46.53  468             16384  18.44    .4629 4.072 2.695e-06 101.4   232 58.67      .3433   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1347 6671 15255 2.287\n",
            "\n",
            "14:43:47 | time:582s total_exs:50016 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6454 15006 42.24  436             16384  20.47    .4628 4.007 2.743e-06 93.42 217.2 54.96      .3466   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1371 6548 15223 2.325\n",
            "\n",
            "14:43:57 | time:593s total_exs:50476 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6260 14481 44.33  460             16384   20.8    .4629 4.096 2.791e-06 108.5 251.1 60.07      .3286   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1395 6369 14732 2.313\n",
            "\n",
            "14:44:05 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n",
            "14:44:05 | Saving dictionary to /content/drive/MyDrive/chatbot_model/model.checkpoint.dict\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "14:44:12 | time:607s total_exs:50840 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6404  8032 25.36  364             16384  19.06    .4628 4.165 2.827e-06 101.9 127.8 64.37      .3293   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                   1413 6506 8160 1.254\n",
            "\n",
            "14:44:22 | time:618s total_exs:51296 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6227 14423 44.01  456             16384  20.02    .4628 4.045 2.875e-06 98.92 229.1 57.08      .3340   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1437 6326 14652 2.316\n",
            "\n",
            "14:44:32 | time:628s total_exs:51660 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6680 14901  35.3  364             16384  21.62    .4538 4.183 2.921e-06  78.7 175.5 65.59      .3227   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1460 6758 15076 2.231\n",
            "\n",
            "14:44:42 | time:638s total_exs:52076 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6063 13804 41.18  416             16384  19.44    .4487 4.024 2.967e-06 98.39   224 55.91      .3491   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1483 6162 14028 2.277\n",
            "\n",
            "14:44:53 | time:648s total_exs:52548 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5929 13968 46.33  472             16384  19.06    .4629 4.102 3.015e-06 100.7 237.1 60.48      .3444   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1507 6030 14205 2.356\n",
            "\n",
            "14:45:03 | time:659s total_exs:52992 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6556 15260 43.06  444             16384   20.1    .4628 3.975 3.063e-06 84.88 197.6 53.23      .3574   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1531 6641 15458 2.328\n",
            "\n",
            "14:45:13 | time:669s total_exs:53480 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6033 13743 48.33  488             16384  18.31    .4538 3.998 3.109e-06 112.2 255.5 54.47      .3426   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1554 6145 13998 2.278\n",
            "\n",
            "14:45:23 | time:679s total_exs:53856 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6582 15277 36.36  376             16384  21.18    .4673 3.863 3.157e-06 72.92 169.2 47.59      .3606   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1578 6655 15446 2.321\n",
            "\n",
            "14:45:34 | time:689s total_exs:54344 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5827 13555  47.3  488             16384  18.45    .4570 4.119 3.205e-06 111.5 259.3 61.53      .3286   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1602 5939 13814 2.326\n",
            "\n",
            "14:45:44 | time:700s total_exs:54736 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6254 14719 38.44  392             16384  20.58    .4629 3.993 3.253e-06 80.38 189.2 54.24      .3546   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1626 6334 14908 2.354\n",
            "\n",
            "14:45:54 | time:710s total_exs:55100 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6506 15189  35.4  364             16384  21.77    .4538 3.837 3.301e-06 76.08 177.6 46.39      .3669   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1650 6583 15367 2.335\n",
            "\n",
            "14:46:01 | Overflow: setting loss scale to 16384.0\n",
            "14:46:05 | time:720s total_exs:55472 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9583  6510 15220 36.24  372             16384  19.74    .4538 3.754 3.349e-06 72.42 169.3 42.69      .3751   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1674 6582 15389 2.338\n",
            "\n",
            "14:46:15 | time:731s total_exs:55912 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5955 13832 42.58  440             16384  18.96    .4629 3.974 3.397e-06 91.92 213.5 53.18      .3613   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1698 6047 14046 2.323\n",
            "\n",
            "14:46:25 | time:741s total_exs:56296 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6370 14995 37.66  384             16384  20.83    .4629 4.012 3.445e-06 81.46 191.8 55.24      .3371   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1722 6451 15187 2.354\n",
            "\n",
            "14:46:35 | time:751s total_exs:56748 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5895 13962 44.61  452             16384  19.12    .4449 3.913 3.493e-06 100.9   239 50.04      .3563   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1746 5996 14201 2.369\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-1f273592bf0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mdynamic_batching\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'full'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mlabel_turns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'both'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# https://parl.ai/docs/core/teachers.html#parlai.core.teachers.ConversationTeacher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36m_run_kwargs\u001b[0;34m(cls, kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_from_parser_and_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36m_run_from_parser_and_opt\u001b[0;34m(cls, opt, parser)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mscript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/scripts/train_model.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/scripts/train_model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    656\u001b[0m                 \u001b[0;31m# do one example / batch of examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m                     \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparley\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mStopTrainException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m                     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Stopping from {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/core/worlds.py\u001b[0m in \u001b[0;36mparley\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0;31m# great, this batch is good to go! let's run it!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m         \u001b[0macts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_acts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m         \u001b[0;31m# broadcast the results back to all the models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/core/torch_agent.py\u001b[0m in \u001b[0;36mbatch_act\u001b[0;34m(self, observations)\u001b[0m\n\u001b[1;32m   1967\u001b[0m             \u001b[0;31m# register the start of updates for later counting when they occur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1968\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ups'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGlobalTimerMetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1970\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/core/torch_generator_agent.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m             \u001b[0moom_sync\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/core/torch_agent.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m   2053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2054\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2055\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_master_grads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2056\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2057\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/utils/fp16.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, update_master_grads)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \"\"\"\n\u001b[1;32m    363\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grads_are_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}