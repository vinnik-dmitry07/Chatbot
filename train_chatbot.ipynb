{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMei3303Is6EMM76qemDpYJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinnik-dmitry07/Chatbot/blob/main/train_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAbMlxO9DI4h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cdeda94-d765-4c99-881d-ed2394563cb0"
      },
      "source": [
        "!nvidia-smi\r\n",
        "!pip install --quiet parlai"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Feb 18 17:11:22 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    31W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 5.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 9.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.0MB 37.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 57.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.8MB 44.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 61.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 317kB 59.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 40kB 7.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 61.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 63.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 52.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 11.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 46.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 552kB 28.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.2MB 46.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 58.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 40kB 5.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 11.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 12.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 12.3MB/s \n",
            "\u001b[?25h  Building wheel for websocket-server (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docformatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for untokenize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: torchtext 0.8.1 has requirement torch==1.7.1, but you'll have torch 1.7.0+cu101 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: botocore 1.20.10 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: sphinx-autodoc-typehints 1.11.1 has requirement Sphinx>=3.0, but you'll have sphinx 1.8.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: myst-parser 0.13.5 has requirement sphinx<4,>=2, but you'll have sphinx 1.8.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fvcore 0.1.3.post20210218 has requirement pyyaml>=5.1, but you'll have pyyaml 3.13 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY3EisQ766aP"
      },
      "source": [
        "from pathlib import Path\r\n",
        "\r\n",
        "GDRIVE_ROOT = Path('/content/drive/MyDrive/')\r\n",
        "SAVE_DIR = GDRIVE_ROOT / 'chatbot_model'\r\n",
        "DATA_DIR = GDRIVE_ROOT / 'chatbot_data'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9y2IWmWDFjU"
      },
      "source": [
        "from datetime import timedelta\r\n",
        "\r\n",
        "EPISODE_DT = timedelta(minutes=3)  # change to split messages in separate dialogues if time delta is greater than EPISODE_DT\r\n",
        "TRAIN_PART, TEST_PART, VALID_PART = 0.996, 0.002, 0.002\r\n",
        "\r\n",
        "assert TRAIN_PART + TEST_PART + VALID_PART == 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr8wrt68ATZl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81409a7f-97e3-491b-de98-0ffb5f172367"
      },
      "source": [
        "from google.colab import drive\r\n",
        "\r\n",
        "drive.mount(str(GDRIVE_ROOT.parent))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCM-AVZhHKFK"
      },
      "source": [
        "import json\r\n",
        "\r\n",
        "with open(DATA_DIR / 'result.json', 'r', encoding='utf8') as f:\r\n",
        "    raw_messages = json.load(f)['messages']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3CvK47rIujd"
      },
      "source": [
        "from datetime import datetime\r\n",
        "\r\n",
        "filtered_messages = []\r\n",
        "for msg in raw_messages:\r\n",
        "    if (\r\n",
        "            'from' in msg and\r\n",
        "            'from_id' in msg and\r\n",
        "            'mime_type' not in msg and\r\n",
        "            msg['text'] and\r\n",
        "            isinstance(msg['text'], str) and\r\n",
        "            len(msg['text']) < 50\r\n",
        "    ):\r\n",
        "        msg1 = msg.copy()\r\n",
        "        msg1['date'] = datetime.strptime(msg1['date'], '%Y-%m-%dT%H:%M:%S')\r\n",
        "        filtered_messages.append(msg1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz06qsAJJLda"
      },
      "source": [
        "import re\r\n",
        "\r\n",
        "joined_messages = []\r\n",
        "for i in range(len(filtered_messages)):\r\n",
        "    alphanum_text = re.sub(r'[^A-Za-z0-9 ]+', '', filtered_messages[i]['text']).strip()\r\n",
        "    if alphanum_text:\r\n",
        "        if (    \r\n",
        "                joined_messages and    \r\n",
        "                filtered_messages[i - 1]['from_id'] == filtered_messages[i]['from_id'] and\r\n",
        "                filtered_messages[i - 1]['date'] - filtered_messages[i]['date'] <= EPISODE_DT\r\n",
        "        ):\r\n",
        "            joined_messages[-1]['text'] += ' ' + alphanum_text\r\n",
        "        else:\r\n",
        "            new_message = filtered_messages[i].copy()\r\n",
        "            new_message['text'] = alphanum_text\r\n",
        "            joined_messages.append(new_message)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUphzoX3K_FF"
      },
      "source": [
        "def partition(alist, indices):\r\n",
        "    return [alist[a:b] for a, b in zip([0] + indices, indices + [None])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXMAHZTxL-KN"
      },
      "source": [
        "def save_jsonl(messages, suffix, human_readable=False):\r\n",
        "    time_diffs = [messages[i + 1]['date'] - messages[i]['date'] for i in range(len(messages) - 1)]\r\n",
        "    split_positions = [i + 1 for i in range(len(time_diffs)) if time_diffs[i] > EPISODE_DT]\r\n",
        "    episodes = partition(messages, split_positions)\r\n",
        "    print(f'{suffix} episodes: {len(episodes)}, messages: {len(messages)}')\r\n",
        "\r\n",
        "    with open(DATA_DIR / f'data_{suffix}.jsonl', 'w', **({'encoding': 'utf8'} if human_readable else {})) as outfile:\r\n",
        "        for episode in episodes:\r\n",
        "            dialog = [{'id': i % 2, 'text': msg['text']} for i, msg in enumerate(episode)]\r\n",
        "            episode = {'dialog': [dialog]}\r\n",
        "            json.dump(episode, outfile, **({'ensure_ascii': False} if human_readable else {}))\r\n",
        "            outfile.write('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApxWsw2omwax",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "d59be9a6-efbd-4dd8-ea94-851e3c422e34"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "train, test, valid = np.split(joined_messages, [\r\n",
        "    int(TRAIN_PART * len(joined_messages)),\r\n",
        "    int((TRAIN_PART + TEST_PART) * len(joined_messages)),\r\n",
        "])\r\n",
        "\r\n",
        "save_jsonl(train, suffix='train')\r\n",
        "save_jsonl(test, suffix='test')\r\n",
        "save_jsonl(valid, suffix='valid')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-131b13736c4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m train, test, valid = np.split(joined_messages, [\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_PART\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoined_messages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_PART\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mTEST_PART\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoined_messages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'joined_messages' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5JX4EM1Jb56",
        "outputId": "128669b8-eb70-4282-a4f3-8220e436b970"
      },
      "source": [
        "import shutil\r\n",
        "import subprocess\r\n",
        "import time\r\n",
        "import threading\r\n",
        "from pathlib import Path\r\n",
        "\r\n",
        "\r\n",
        "def dir_size_bytes(path):\r\n",
        "    return int(subprocess.check_output(['du','--bytes', '--summarize', path]).split()[0].decode())\r\n",
        "\r\n",
        "def check_chache(max_cache_size_gb=18, check_period_minutes=5):\r\n",
        "    this_id = str(threading.get_ident())\r\n",
        "    thread_path = Path('/threads')\r\n",
        "    thread_path.mkdir(exist_ok=True)\r\n",
        "\r\n",
        "    def threads_ids():\r\n",
        "        return [str(p.name) for p in thread_path.iterdir() if p.is_file()]\r\n",
        "    \r\n",
        "    if not(threads_ids()):\r\n",
        "        (thread_path / this_id).open(mode='w').close()\r\n",
        "        while True:\r\n",
        "            ids = threads_ids()\r\n",
        "            if not (len(ids) == 1 and ids[0] == this_id):\r\n",
        "                break\r\n",
        "\r\n",
        "            print(f'Thread {this_id} is checking chache.')\r\n",
        "\r\n",
        "            for cache_path in Path('/root/.config/Google/DriveFS').glob('**/content_cache'):\r\n",
        "                chache_path_str = str(cache_path)\r\n",
        "                chache_size_gb = dir_size_bytes(chache_path_str) / 10 ** 9\r\n",
        "                if chache_size_gb > max_cache_size_gb:\r\n",
        "                    print(f'Deleting {chache_path_str} with size {chache_size_gb} GB.')\r\n",
        "                    shutil.rmtree(chache_path_str)\r\n",
        "            time.sleep(check_period_minutes * 60)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HWEdG7hMboN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "359e8b78-edc9-491d-bf5f-c1c7c3463f4e"
      },
      "source": [
        "# threading.Thread(target=check_chache).start()\r\n",
        "\r\n",
        "import os\r\n",
        "\r\n",
        "os.environ['SAVE_DIR'] = str(SAVE_DIR)\r\n",
        "!rm --recursive --force $SAVE_DIR\r\n",
        "!mkdir --parents $SAVE_DIR\r\n",
        "\r\n",
        "\r\n",
        "from parlai.scripts.train_model import TrainModel\r\n",
        "\r\n",
        "TrainModel.main(\r\n",
        "    task='jsonfile',\r\n",
        "    jsonfile_datapath=str(DATA_DIR / 'data'),\r\n",
        "    jsonfile_datatype_extension=True,\r\n",
        "\r\n",
        "    model='transformer/generator',\r\n",
        "    model_file=str(SAVE_DIR / 'model'),\r\n",
        "    \r\n",
        "    init_model='zoo:tutorial_transformer_generator/model',\r\n",
        "\r\n",
        "    n_heads=16, n_layers=8, n_positions=512, text_truncate=512,\r\n",
        "    label_truncate=128, ffn_size=2048, embedding_size=512,\r\n",
        "    activation='gelu', variant='xlm',\r\n",
        "    dict_lower=True, dict_tokenizer='bpe',\r\n",
        "    dict_file='zoo:tutorial_transformer_generator/model.dict',\r\n",
        "    learn_positional_embeddings=True,\r\n",
        "    \r\n",
        "    lr=1e-5, optimizer='adam',\r\n",
        "    warmup_updates=5000,\r\n",
        "    validation_metric='ppl',\r\n",
        "    validation_every_n_secs=60 * 60,  # running eval: valid\r\n",
        "    save_every_n_secs=60,  # saving model checkpoint\r\n",
        "\r\n",
        "    batchsize=12, fp16=True, fp16_impl='mem_efficient',\r\n",
        "    \r\n",
        "    skip_generation=True,\r\n",
        "    \r\n",
        "    dynamic_batching='full',\r\n",
        "\r\n",
        "    label_turns='both',  # https://parl.ai/docs/core/teachers.html#parlai.core.teachers.ConversationTeacher\r\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thread 140129149110016 is checking chache.\n",
            "10:50:57 | building data: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz\n",
            "10:50:57 | Downloading http://parl.ai/downloads/_models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz to /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading tutorial_transformer_generator_v1.tar.gz: 100%|██████████| 1.12G/1.12G [00:39<00:00, 28.7MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10:51:55 | building dictionary first...\n",
            "10:51:55 | No model with opt yet at: /content/drive/MyDrive/chatbot_model/model(.opt)\n",
            "10:51:55 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: full,verbose: False,datapath: /usr/local/lib/python3.7/dist-packages/data,eval_dynamic_batching: None,load_from_checkpoint: True,tensorboard_logdir: None,jsonfile_datapath: /content/drive/MyDrive/chatbot_data/data,jsonfile_datatype_extension: True,label_turns: both,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,beam_block_full_context: True,beam_length_penalty: 0.65,topk: 10,topp: 0.9,beam_delay: 30,beam_block_list_filename: None,temperature: 1.0,compute_tokenized_bleu: False,interactive_mode: False,fp16_impl: mem_efficient,force_fp16_tokens: False,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,max_lr_steps: -1,invsqrt_lr_decay_gamma: -1,parlai_home: /usr/local/lib/python3.7/dist-packages\u001b[0m\n",
            "10:51:55 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--show-advanced-args False --task internal:new_reddit:presorted --datatype train:stream --numthreads 1 --batchsize 48 --num-epochs 5.0 --validation-every-n-secs 1800.0 --save-every-n-secs -1 --save-after-valid True --validation-max-exs 9920 --short-final-eval True --validation-patience 0 --validation-metric-mode min --dict-build-first True --numworkers 4 --pytorch-preprocess False --pytorch-teacher-batch-sort False --batch-sort-cache-type pop --batch-length-range 5 --shuffle False --batch-sort-field text --pytorch-context-length -1 --pytorch-include-labels True --log-every-n-secs 30.0 --distributed-world-size 64 --port 61337 --dropout 0.1 --beam-size 8 --beam-min-n-best 3 --beam-min-length 10 --skip-generation False --inference beam --optimizer fused_adam --learningrate 0.0005 --gradient-clip 10.0 --adam-eps 1e-06 --betas 0.9,0.98 --weight-decay 0.01 --lr-scheduler invsqrt --warmup-updates 20000 --gpu 0 --beam-block-ngram 3 --beam-context-block-ngram 3\u001b[0m\n",
            "10:51:55 | Using CUDA\n",
            "10:51:55 | loading dictionary from /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "10:51:55 | num words = 54944\n",
            "10:51:56 | \u001b[33mDEPRECATED: XLM should only be used for backwards compatibility, as it involves a less-stable layernorm operation.\u001b[0m\n",
            "10:52:07 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "10:52:07 | Loading existing model params from /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "10:52:08 | \u001b[33mNot loading optim state since optim class changed.\u001b[0m\n",
            "10:52:08 | Opt:\n",
            "10:52:08 |     activation: gelu\n",
            "10:52:08 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "10:52:08 |     adam_eps: 1e-08\n",
            "10:52:08 |     add_p1_after_newln: False\n",
            "10:52:08 |     aggregate_micro: False\n",
            "10:52:08 |     allow_missing_init_opts: False\n",
            "10:52:08 |     attention_dropout: 0.0\n",
            "10:52:08 |     batchsize: 12\n",
            "10:52:08 |     beam_block_full_context: True\n",
            "10:52:08 |     beam_block_list_filename: None\n",
            "10:52:08 |     beam_block_ngram: -1\n",
            "10:52:08 |     beam_context_block_ngram: -1\n",
            "10:52:08 |     beam_delay: 30\n",
            "10:52:08 |     beam_length_penalty: 0.65\n",
            "10:52:08 |     beam_min_length: 1\n",
            "10:52:08 |     beam_size: 1\n",
            "10:52:08 |     betas: '(0.9, 0.999)'\n",
            "10:52:08 |     bpe_add_prefix_space: None\n",
            "10:52:08 |     bpe_debug: False\n",
            "10:52:08 |     bpe_dropout: None\n",
            "10:52:08 |     bpe_merge: None\n",
            "10:52:08 |     bpe_vocab: None\n",
            "10:52:08 |     compute_tokenized_bleu: False\n",
            "10:52:08 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "10:52:08 |     datatype: train\n",
            "10:52:08 |     delimiter: '\\n'\n",
            "10:52:08 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "10:52:08 |     dict_endtoken: __end__\n",
            "10:52:08 |     dict_file: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "10:52:08 |     dict_include_test: False\n",
            "10:52:08 |     dict_include_valid: False\n",
            "10:52:08 |     dict_initpath: None\n",
            "10:52:08 |     dict_language: english\n",
            "10:52:08 |     dict_loaded: True\n",
            "10:52:08 |     dict_lower: True\n",
            "10:52:08 |     dict_max_ngram_size: -1\n",
            "10:52:08 |     dict_maxexs: -1\n",
            "10:52:08 |     dict_maxtokens: -1\n",
            "10:52:08 |     dict_minfreq: 0\n",
            "10:52:08 |     dict_nulltoken: __null__\n",
            "10:52:08 |     dict_starttoken: __start__\n",
            "10:52:08 |     dict_textfields: text,labels\n",
            "10:52:08 |     dict_tokenizer: bpe\n",
            "10:52:08 |     dict_unktoken: __unk__\n",
            "10:52:08 |     display_examples: False\n",
            "10:52:08 |     download_path: None\n",
            "10:52:08 |     dropout: 0.0\n",
            "10:52:08 |     dynamic_batching: full\n",
            "10:52:08 |     embedding_projection: random\n",
            "10:52:08 |     embedding_size: 512\n",
            "10:52:08 |     embedding_type: random\n",
            "10:52:08 |     embeddings_scale: True\n",
            "10:52:08 |     eval_batchsize: None\n",
            "10:52:08 |     eval_dynamic_batching: None\n",
            "10:52:08 |     evaltask: None\n",
            "10:52:08 |     ffn_size: 2048\n",
            "10:52:08 |     force_fp16_tokens: False\n",
            "10:52:08 |     fp16: True\n",
            "10:52:08 |     fp16_impl: mem_efficient\n",
            "10:52:08 |     gpu: -1\n",
            "10:52:08 |     gradient_clip: 0.1\n",
            "10:52:08 |     hide_labels: False\n",
            "10:52:08 |     history_add_global_end_token: None\n",
            "10:52:08 |     history_reversed: False\n",
            "10:52:08 |     history_size: -1\n",
            "10:52:08 |     image_cropsize: 224\n",
            "10:52:08 |     image_mode: raw\n",
            "10:52:08 |     image_size: 256\n",
            "10:52:08 |     inference: greedy\n",
            "10:52:08 |     init_model: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "10:52:08 |     init_opt: None\n",
            "10:52:08 |     interactive_mode: False\n",
            "10:52:08 |     invsqrt_lr_decay_gamma: -1\n",
            "10:52:08 |     jsonfile_datapath: /content/drive/MyDrive/chatbot_data/data\n",
            "10:52:08 |     jsonfile_datatype_extension: True\n",
            "10:52:08 |     label_truncate: 128\n",
            "10:52:08 |     label_turns: both\n",
            "10:52:08 |     learn_positional_embeddings: True\n",
            "10:52:08 |     learningrate: 1e-05\n",
            "10:52:08 |     load_from_checkpoint: True\n",
            "10:52:08 |     log_every_n_secs: 10\n",
            "10:52:08 |     loglevel: info\n",
            "10:52:08 |     lr_scheduler: reduceonplateau\n",
            "10:52:08 |     lr_scheduler_decay: 0.5\n",
            "10:52:08 |     lr_scheduler_patience: 3\n",
            "10:52:08 |     max_lr_steps: -1\n",
            "10:52:08 |     max_train_time: -1\n",
            "10:52:08 |     metrics: default\n",
            "10:52:08 |     model: transformer/generator\n",
            "10:52:08 |     model_file: /content/drive/MyDrive/chatbot_model/model\n",
            "10:52:08 |     model_parallel: False\n",
            "10:52:08 |     momentum: 0\n",
            "10:52:08 |     multitask_weights: [1]\n",
            "10:52:08 |     n_decoder_layers: -1\n",
            "10:52:08 |     n_encoder_layers: -1\n",
            "10:52:08 |     n_heads: 16\n",
            "10:52:08 |     n_layers: 8\n",
            "10:52:08 |     n_positions: 512\n",
            "10:52:08 |     n_segments: 0\n",
            "10:52:08 |     nesterov: True\n",
            "10:52:08 |     no_cuda: False\n",
            "10:52:08 |     num_epochs: -1\n",
            "10:52:08 |     nus: (0.7,)\n",
            "10:52:08 |     optimizer: mem_eff_adam\n",
            "10:52:08 |     output_scaling: 1.0\n",
            "10:52:08 |     override: \"{'task': 'jsonfile', 'jsonfile_datapath': '/content/drive/MyDrive/chatbot_data/data', 'jsonfile_datatype_extension': True, 'model': 'transformer/generator', 'model_file': '/content/drive/MyDrive/chatbot_model/model', 'init_model': 'zoo:tutorial_transformer_generator/model', 'n_heads': 16, 'n_layers': 8, 'n_positions': 512, 'text_truncate': 512, 'label_truncate': 128, 'ffn_size': 2048, 'embedding_size': 512, 'activation': 'gelu', 'variant': 'xlm', 'dict_lower': True, 'dict_tokenizer': 'bpe', 'dict_file': '/usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model.dict', 'learn_positional_embeddings': True, 'learningrate': 1e-05, 'optimizer': 'adam', 'warmup_updates': 5000, 'validation_metric': 'ppl', 'validation_every_n_secs': 3600.0, 'save_every_n_secs': 60.0, 'batchsize': 12, 'fp16': True, 'fp16_impl': 'mem_efficient', 'skip_generation': True, 'dynamic_batching': 'full', 'label_turns': 'both'}\"\n",
            "10:52:08 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "10:52:08 |     person_tokens: False\n",
            "10:52:08 |     rank_candidates: False\n",
            "10:52:08 |     relu_dropout: 0.0\n",
            "10:52:08 |     save_after_valid: False\n",
            "10:52:08 |     save_every_n_secs: 60.0\n",
            "10:52:08 |     share_word_embeddings: True\n",
            "10:52:08 |     short_final_eval: False\n",
            "10:52:08 |     skip_generation: True\n",
            "10:52:08 |     special_tok_lst: None\n",
            "10:52:08 |     split_lines: False\n",
            "10:52:08 |     starttime: Feb25_10-51\n",
            "10:52:08 |     task: jsonfile\n",
            "10:52:08 |     temperature: 1.0\n",
            "10:52:08 |     tensorboard_log: False\n",
            "10:52:08 |     tensorboard_logdir: None\n",
            "10:52:08 |     text_truncate: 512\n",
            "10:52:08 |     topk: 10\n",
            "10:52:08 |     topp: 0.9\n",
            "10:52:08 |     truncate: -1\n",
            "10:52:08 |     update_freq: 1\n",
            "10:52:08 |     use_reply: label\n",
            "10:52:08 |     validation_cutoff: 1.0\n",
            "10:52:08 |     validation_every_n_epochs: -1\n",
            "10:52:08 |     validation_every_n_secs: 3600.0\n",
            "10:52:08 |     validation_max_exs: -1\n",
            "10:52:08 |     validation_metric: ppl\n",
            "10:52:08 |     validation_metric_mode: None\n",
            "10:52:08 |     validation_patience: 10\n",
            "10:52:08 |     validation_share_agent: False\n",
            "10:52:08 |     variant: xlm\n",
            "10:52:08 |     verbose: False\n",
            "10:52:08 |     warmup_rate: 0.0001\n",
            "10:52:08 |     warmup_updates: 5000\n",
            "10:52:08 |     weight_decay: None\n",
            "10:52:08 | creating task(s): jsonfile\n",
            "10:52:08 | [loading data from json file into task:/content/drive/MyDrive/chatbot_data/data_train.jsonl]\n",
            "10:52:12 | \u001b[31mMetadata does not exist. Please double check your datapath.\u001b[0m\n",
            "10:52:16 | training...\n",
            "10:52:17 | Overflow: setting loss scale to 65536.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/parlai/utils/fp16.py:487: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10:52:17 | Overflow: setting loss scale to 32768.0\n",
            "10:52:18 | Overflow: setting loss scale to 16384.0\n",
            "10:52:26 | time:10s total_exs:6252 epochs:0.01\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9000  4029 12081 623.5 6252             20207  11.59    .4716  4.65 6.099e-08 899.6  2697 104.6      .2877   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                     30 4929 14778 2.998\n",
            "\n",
            "10:52:36 | time:20s total_exs:10096 epochs:0.01\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5574 19834 379.9 3844             16384  13.14    .3837 4.453 1.33e-07 480.2  1709 85.92      .3137   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                     66 6054 21543 3.559\n",
            "\n",
            "10:52:46 | time:31s total_exs:12976 epochs:0.02\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6335 22926 281.7 2880             16384  13.93    .3094 4.478 2.07e-07 361.8  1309 88.08      .2997   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    103 6697 24236 3.619\n",
            "\n",
            "10:52:57 | time:41s total_exs:15296 epochs:0.02\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6157 21329 229.6 2320             16384  14.63    .3488 4.441 2.77e-07 304.9  1056 84.83      .3189   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    138 6461 22385 3.465\n",
            "\n",
            "10:53:07 | time:51s total_exs:17292 epochs:0.02\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6557 22885   199 1996             16384   15.8    .3218  4.47 3.47e-07 261.9 914.1 87.38      .3160   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    173 6819 23799 3.49\n",
            "\n",
            "10:53:16 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n",
            "10:53:16 | Saving dictionary to /content/drive/MyDrive/chatbot_model/model.checkpoint.dict\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10:53:22 | time:66s total_exs:18984 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6364 13638 109.9 1692             16384  15.19    .3064 4.422 4.13e-07 246.8 528.9 83.3      .3133   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    206 6611 14166 2.143\n",
            "\n",
            "10:53:32 | time:77s total_exs:20568 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6422 20036 154.4 1584             16384   15.5    .3068 4.466 4.77e-07 244.2 762.1 86.99      .3076   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    238 6666 20798 3.12\n",
            "\n",
            "10:53:43 | time:87s total_exs:22072 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6126 18638 147.6 1504             16384  15.66    .3352  4.44 5.389e-07 227.1 690.8 84.78      .3039   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    269 6353 19329 3.043\n",
            "\n",
            "10:53:53 | time:97s total_exs:23752 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6116 21149   166 1680             16384  15.34    .3169  4.38 6.089e-07 216.4 748.4 79.81      .3200   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    304 6333 21897 3.458\n",
            "\n",
            "10:54:03 | time:107s total_exs:25204 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6637 22782 142.4 1452             16384  15.66    .3169 4.324 6.789e-07 184.4 632.9 75.46      .3243   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    339 6821 23415 3.433\n",
            "\n",
            "10:54:13 | time:117s total_exs:26476 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6824 23213 123.6 1272             16384  16.74    .3169  4.42 7.489e-07 168.9 574.6 83.06      .3127   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    374 6993 23788 3.402\n",
            "\n",
            "10:54:22 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10:54:28 | time:133s total_exs:27576 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6410 12695 72.61 1100             16384  17.01    .3108 4.483 8.089e-07 172.5 341.5 88.47      .3098   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    404 6583 13036 1.98\n",
            "\n",
            "10:54:35 | Overflow: setting loss scale to 16384.0\n",
            "10:54:39 | time:143s total_exs:28764 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9688  6206 19779 118.3 1188             16384   16.6    .3373 4.498 8.729e-07   173 551.4 89.79      .3067   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    436 6379 20331 3.188\n",
            "\n",
            "10:54:49 | time:153s total_exs:29680 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6548 16920 91.03  916             16384  17.51    .3122 4.401 9.249e-07 160.3 414.2 81.53      .3169   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    462 6708 17334 2.584\n",
            "\n",
            "10:54:59 | time:163s total_exs:30812 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6340 20027 111.7 1132             16384   16.4    .3203 4.518 9.889e-07 170.3 537.9 91.61      .3056   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    494 6511 20565 3.159\n",
            "\n",
            "10:55:09 | time:173s total_exs:31756 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6685 20428 93.05  944             16384  18.41    .3393 4.436 1.051e-06 137.7 420.7 84.4      .3156   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    525 6822 20849 3.056\n",
            "\n",
            "10:55:11 | Overflow: setting loss scale to 16384.0\n",
            "10:55:19 | time:184s total_exs:32796 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9667  6292 18326   101 1040             16384  16.03    .3393 4.332 1.111e-06 152.6 444.5 76.12      .3304   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    555 6445 18770 2.913\n",
            "\n",
            "10:55:28 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10:55:35 | time:199s total_exs:33672 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6741 12448 57.77  876             16384  17.36    .3034 4.319 1.167e-06 139.7 257.9 75.13      .3283   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    583 6880 12705 1.847\n",
            "\n",
            "10:55:45 | time:209s total_exs:34552 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6655 20544 87.62  880             16384  17.87    .3170 4.403 1.229e-06 128.1 395.5 81.66      .3218   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    614 6783 20940 3.087\n",
            "\n",
            "Thread 140129149110016 is checking chache.\n",
            "10:55:55 | time:219s total_exs:35108 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6857 14047 54.24  556             16384  17.61    .3406 4.366 1.271e-06 134.4 275.4 78.7      .3160   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    635 6991 14322 2.049\n",
            "\n",
            "10:56:05 | time:229s total_exs:35860 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6782 18290  75.1  752             16384  17.03    .3251 4.484 1.325e-06 138.9 374.6 88.62      .3058   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    662 6921 18665 2.697\n",
            "\n",
            "10:56:15 | time:239s total_exs:36732 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6379 19345  85.3  872             16384  18.19    .3681 4.265 1.387e-06 132.5   402 71.19      .3225   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    693 6512 19747 3.033\n",
            "\n",
            "10:56:26 | time:250s total_exs:37604 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6594 20506 84.73  872             16384   17.8    .3463 4.305 1.451e-06 113.7 353.6 74.09      .3350   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    725 6708 20860 3.11\n",
            "\n",
            "10:56:34 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10:56:41 | time:265s total_exs:38364 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6499 12495 50.39  760             16384  18.19    .3681 4.366 1.509e-06 125.2 240.7 78.69      .3184   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    754 6624 12736 1.923\n",
            "\n",
            "10:56:51 | time:275s total_exs:39264 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6393 19802 89.92  900             16384  17.48    .3318 4.138 1.571e-06 124.7 386.4 62.7      .3504   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    785 6518 20188 3.098\n",
            "\n",
            "10:57:01 | time:285s total_exs:39784 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6568 13296 50.13  520             16384  18.89    .3681  4.29 1.613e-06   108 218.7   73      .3332   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    806 6676 13515 2.025\n",
            "\n",
            "10:57:11 | time:296s total_exs:40452 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6375 14264 64.98  668             16384  17.67    .3276 4.322 1.659e-06 124.7 279.1 75.36      .3273   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    829 6499 14543 2.238\n",
            "\n",
            "10:57:22 | time:306s total_exs:41280 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6416 18951 81.52  828             16384  18.73    .3374 4.294 1.719e-06   121 357.3 73.29      .3395   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    859 6537 19308 2.954\n",
            "\n",
            "10:57:32 | time:316s total_exs:42028 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6610 21070 74.51  748             16384  18.46    .3302  4.21 1.783e-06   106 337.8 67.34      .3409   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    891 6716 21408 3.188\n",
            "\n",
            "10:57:40 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10:57:47 | time:331s total_exs:42664 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6567 11949 42.86  636             16384  18.46    .3768  4.38 1.837e-06 115.2 209.7 79.82      .3247   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    918 6682 12159 1.82\n",
            "\n",
            "10:57:57 | time:341s total_exs:43372 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6667 19036  69.7  708             16384  17.75    .3526 4.194 1.895e-06 112.1 320.2 66.3      .3373   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    947 6779 19356 2.856\n",
            "\n",
            "10:58:07 | time:351s total_exs:44032 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6503 17938 65.02  660             16384  17.88    .3973 4.355 1.951e-06 112.2 309.4 77.9      .3136   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    975 6615 18248 2.759\n",
            "\n",
            "10:58:12 | Overflow: setting loss scale to 16384.0\n",
            "10:58:17 | time:361s total_exs:44732 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9655  6558 18571 68.35  700             16384  16.35    .3857 4.259 2.009e-06   119   337 70.74      .3222   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1004 6677 18908 2.832\n",
            "\n",
            "10:58:27 | time:371s total_exs:45408 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6096 17083 67.65  676             16384  18.89    .3903 4.225 2.065e-06 108.9 305.2 68.37      .3233   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1032 6205 17388 2.802\n",
            "\n",
            "10:58:38 | time:382s total_exs:45980 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6937 18851 55.51  572             16384  19.02    .4033 4.236 2.121e-06 102.1 277.6 69.15      .3227   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1060 7039 19129 2.718\n",
            "\n",
            "10:58:46 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10:58:53 | time:397s total_exs:46580 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6275 10541 38.77  600             16384   17.5    .3903 4.258 2.173e-06 114.1 191.7 70.69      .3306   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   1086 6389 10733 1.68\n",
            "\n",
            "10:58:54 | Overflow: setting loss scale to 16384.0\n",
            "10:58:56 | Overflow: setting loss scale to 16384.0\n",
            "10:59:04 | time:408s total_exs:47308 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "   .9355  6223 18577  70.1  728             16384  17.51    .4095 4.227 2.235e-06 108.2 322.9 68.5      .3251   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1117 6331 18899 2.986\n",
            "\n",
            "10:59:14 | time:418s total_exs:47676 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6446 11309 35.86  368             16384  18.96    .4125 4.183 2.271e-06 92.78 162.8 65.53      .3419   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1135 6539 11472 1.755\n",
            "\n",
            "10:59:17 | Overflow: setting loss scale to 16384.0\n",
            "10:59:20 | Overflow: setting loss scale to 16384.0\n",
            "10:59:24 | time:428s total_exs:48192 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9231  6531 16887 51.31  516             16384  17.27    .3945 4.086 2.323e-06 93.73 242.3 59.47      .3291   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1161 6625 17130 2.586\n",
            "\n",
            "10:59:25 | Overflow: setting loss scale to 16384.0\n",
            "10:59:26 | Overflow: setting loss scale to 16384.0\n",
            "10:59:28 | Overflow: setting loss scale to 16384.0\n",
            "10:59:34 | time:438s total_exs:48764 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .8966  6739 19035 55.71  572             16384  17.56    .3903 4.314 2.381e-06 99.93 282.3 74.76      .3102   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1190 6839 19317 2.825\n",
            "\n",
            "10:59:44 | time:449s total_exs:49416 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6622 19993  63.5  652             16384  18.77    .4096 4.167 2.443e-06 97.42 294.1 64.5      .3384   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1221 6719 20287 3.019\n",
            "\n",
            "10:59:53 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10:59:59 | time:464s total_exs:49976 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6358 11146 37.76  560             16384  18.64    .3904  4.15 2.495e-06 97.19 170.4 63.42      .3411   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1247 6455 11316 1.753\n",
            "\n",
            "11:00:10 | time:474s total_exs:50540 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6711 19084 55.31  564             16384   19.9    .4144 4.083 2.553e-06 91.69 260.8 59.34      .3524   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1276 6802 19345 2.844\n",
            "\n",
            "11:00:19 | Overflow: setting loss scale to 16384.0\n",
            "11:00:20 | time:484s total_exs:51024 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9600  6528 16013 47.49  484             16384   18.2    .3857 4.189 2.603e-06 89.32 219.1 65.98      .3359   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1301 6617 16232 2.453\n",
            "\n",
            "11:00:30 | time:494s total_exs:51608 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6535 19582 58.33  584             16384  19.97    .4096 4.008 2.663e-06  87.4 261.9 55.01      .3581   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1331 6622 19844 2.997\n",
            "\n",
            "11:00:40 | time:504s total_exs:52196 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6327 18695 57.91  588             16384  20.04    .4125 4.343 2.723e-06 104.3 308.2 76.91      .3014   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1361 6431 19003 2.955\n",
            "\n",
            "11:00:47 | Overflow: setting loss scale to 16384.0\n",
            "11:00:50 | time:514s total_exs:52848 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9677  6196 19111 64.87  652             16384  18.75    .3683 3.999 2.785e-06 90.65 279.6 54.52      .3541   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1392 6286 19391 3.085\n",
            "\n",
            "Thread 140129149110016 is checking chache.\n",
            "11:00:59 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "11:01:05 | time:530s total_exs:53360 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6423 11944    34  512             16384  18.79    .3811 4.072 2.841e-06 78.79 146.5 58.65      .3708   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   1420 6502 12091 1.86\n",
            "\n",
            "11:01:15 | Overflow: setting loss scale to 16384.0\n",
            "11:01:15 | time:540s total_exs:53908 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9667  6396 19051  54.4  548             16384  18.91    .4126 4.218 2.901e-06 85.27   254 67.91      .3307   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1450 6482 19305 2.979\n",
            "\n",
            "11:01:26 | time:550s total_exs:54444 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6264 16744 53.06  536             16384  19.11    .4057 4.167 2.955e-06 95.11 254.2 64.49      .3427   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1477 6359 16998 2.673\n",
            "\n",
            "11:01:30 | Overflow: setting loss scale to 16384.0\n",
            "11:01:36 | time:560s total_exs:54948 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9667  6385 18746 49.32  504             16384  20.17    .4093 4.197 3.015e-06 78.97 231.8 66.49      .3385   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1507 6464 18977 2.936\n",
            "\n",
            "11:01:46 | time:570s total_exs:55468 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6451 16633 51.56  520             16384  18.99    .3996 4.051 3.067e-06 92.35 238.1 57.45      .3424   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1533 6544 16871 2.579\n",
            "\n",
            "11:01:56 | time:580s total_exs:55956 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6548 16549 47.43  488             16384  18.79    .3901 4.217 3.119e-06 88.54 223.7 67.86      .3367   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1559 6637 16773 2.527\n",
            "\n",
            "11:02:03 | Overflow: setting loss scale to 16384.0\n",
            "11:02:05 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "11:02:57 | time:641s total_exs:56516 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9600  5987  2485 9.296  560             16384  16.77    .3808 4.169 3.169e-06 114.6 47.58 64.64      .3172   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                   1584 6101 2532 .4150\n",
            "\n",
            "11:03:07 | time:651s total_exs:57000 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6558 18998 48.35  484             16384  21.15    .4093 4.071 3.227e-06 73.31 212.4 58.63      .3401   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1613 6631 19211 2.897\n",
            "\n",
            "11:03:11 | Overflow: setting loss scale to 16384.0\n",
            "11:03:17 | time:661s total_exs:57472 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9655  6479 18770 47.15  472             16384  20.05    .4094 4.172 3.285e-06 74.07 214.6 64.83      .3371   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1642 6553 18985 2.897\n",
            "\n",
            "11:03:27 | time:671s total_exs:57976 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6461 18832 48.96  504             16384  20.43    .4123 4.135 3.345e-06  78.1 227.6 62.49      .3453   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1672 6540 19059 2.915\n",
            "\n",
            "11:03:31 | Overflow: setting loss scale to 16384.0\n",
            "11:03:37 | time:681s total_exs:58460 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "   .9655  6672 18727 46.84  484             16384  18.74    .4123 3.982 3.403e-06 80.93 227.2 53.6      .3464   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1701 6753 18954 2.807\n",
            "\n",
            "11:03:47 | time:692s total_exs:59004 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6213 17877 53.98  544             16384  20.27    .4094  4.42 3.461e-06 100.7 289.7 83.07      .2952   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1730 6313 18167 2.878\n",
            "\n",
            "11:03:56 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "11:04:02 | time:707s total_exs:59464 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6453 11615 30.67  460             16384  20.18    .3996 4.144 3.515e-06 89.78 161.6 63.05      .3296   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   1757 6542 11777  1.8\n",
            "\n",
            "11:04:12 | Overflow: setting loss scale to 16384.0\n",
            "11:04:13 | time:717s total_exs:59984 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9677  6186 18604 50.44  520             16384  19.99    .3996 4.218 3.577e-06 90.65 272.6 67.86      .3142   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1788 6277 18876 3.008\n",
            "\n",
            "11:04:21 | Overflow: setting loss scale to 16384.0\n",
            "11:04:23 | time:727s total_exs:60596 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9667  6125 17923 59.69  612             16384  19.07    .4094 3.986 3.637e-06  89.1 260.7 53.82      .3614   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1818 6214 18184 2.926\n",
            "\n",
            "11:04:33 | time:737s total_exs:61144 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6365 18801 53.96  548             16384   19.4    .3996 4.017 3.697e-06  81.9 241.9 55.53      .3496   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1848 6447 19043 2.954\n",
            "\n",
            "11:04:43 | time:747s total_exs:61612 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6401 19031 46.38  468             16384  20.71    .3996 3.895 3.757e-06 70.97   211 49.16      .3546   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1878 6472 19242 2.973\n",
            "\n",
            "11:04:54 | time:758s total_exs:62184 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5963 17495 55.94  572             16384   19.5    .4094 4.131 3.817e-06 90.47 265.4 62.25      .3394   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1908 6053 17761 2.934\n",
            "\n",
            "11:05:02 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "11:05:15 | time:779s total_exs:62700 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6242  7949 24.34  516             16384  18.66    .4094 4.096 3.871e-06 97.15 123.7 60.07      .3317   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                   1935 6339 8073 1.274\n",
            "\n",
            "11:05:25 | time:789s total_exs:63144 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6176 17428  43.2  444             16384  20.51    .4094 4.135 3.929e-06 77.59 218.9 62.5      .3369   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1964 6254 17647 2.822\n",
            "\n",
            "11:05:35 | time:799s total_exs:63568 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6535 18283 42.36  424             16384  20.03    .4094 3.988 3.985e-06 71.46 199.9 53.94      .3478   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1992 6606 18483 2.798\n",
            "\n",
            "11:05:45 | time:809s total_exs:63968 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6376 17230 40.03  400             16384  21.16    .4094  4.07 4.039e-06 71.93 194.4 58.55      .3563   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2019 6448 17424 2.703\n",
            "\n",
            "Thread 140129149110016 is checking chache.\n",
            "11:05:55 | time:820s total_exs:64396 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6415 16938 41.85  428             16384  20.33    .4094 3.954 4.093e-06 72.78 192.2 52.16      .3517   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2046 6487 17130 2.641\n",
            "\n",
            "11:06:05 | time:830s total_exs:64820 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6429 17352 42.38  424             16384  19.59    .4142 3.945 4.147e-06 84.41 227.8 51.67      .3493   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2073 6513 17580 2.699\n",
            "\n",
            "11:06:15 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "11:07:22 | time:906s total_exs:65200 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6052  2064 4.984  380             16384   21.3    .3902 3.989 4.199e-06 68.73 23.44 53.97      .3665   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                   2099 6121 2087 .3410\n",
            "\n",
            "11:07:32 | time:916s total_exs:65720 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5837 17452 51.82  520             16384  20.36    .3856 3.995 4.259e-06  78.4 234.4 54.31      .3588   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   2129 5916 17686 2.99\n",
            "\n",
            "11:07:35 | Overflow: setting loss scale to 16384.0\n",
            "11:07:42 | time:926s total_exs:66180 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9677  6135 18615 45.02  460             16384  20.16    .3997 3.883 4.321e-06    70 212.4 48.57      .3525   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2160 6205 18827 3.034\n",
            "\n",
            "11:07:46 | Overflow: setting loss scale to 16384.0\n",
            "11:07:52 | time:936s total_exs:66692 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9667  6158 18125 50.23  512             16384  18.72    .4094 3.961 4.381e-06 82.77 243.6 52.49      .3592   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2190 6240 18368 2.944\n",
            "\n",
            "11:08:00 | Overflow: setting loss scale to 16384.0\n",
            "11:08:02 | time:946s total_exs:67104 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9655  6084 17513 40.89  412             16384  20.21    .4124 4.165 4.439e-06 68.55 197.3 64.38      .3270   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2219 6152 17710 2.879\n",
            "\n",
            "11:08:12 | time:957s total_exs:67568 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6081 17498 46.04  464             16384  20.13    .4123 4.197 4.497e-06 82.86 238.4 66.48      .3279   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2248 6163 17736 2.878\n",
            "\n",
            "11:08:19 | Overflow: setting loss scale to 16384.0\n",
            "11:08:21 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "11:08:27 | Overflow: setting loss scale to 16384.0\n",
            "11:08:27 | time:972s total_exs:68004 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9286  6166 11464 28.95  436             16384   19.2    .4031 3.817 4.553e-06 65.04 120.9 45.45      .3740   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2276 6231 11585 1.859\n",
            "\n",
            "11:08:38 | time:982s total_exs:68504 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6133 18445  48.5  500             16384  19.67    .4094 4.051 4.615e-06 83.55 251.3 57.45      .3405   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2307 6217 18697 3.008\n",
            "\n",
            "11:08:48 | time:992s total_exs:68908 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6159 18246 39.89  404             16384  21.24    .3902 3.997 4.675e-06  69.7 206.5 54.41      .3333   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2337 6229 18453 2.963\n",
            "\n",
            "11:08:50 | Overflow: setting loss scale to 16384.0\n",
            "11:08:58 | time:1002s total_exs:69372 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "   .9667  6063 17701 45.15  464             16384  19.52    .3997 3.943 4.735e-06 70.57   206 51.6      .3510   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   2367 6134 17907 2.92\n",
            "\n",
            "11:09:08 | time:1013s total_exs:69868 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6222 18368 48.81  496             16384  19.55    .4094  4.05 4.795e-06 82.03 242.2 57.4      .3413   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2397 6304 18611 2.953\n",
            "\n",
            "11:09:16 | Overflow: setting loss scale to 16384.0\n",
            "11:09:19 | time:1023s total_exs:70316 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9677  5881 17944 44.09  448             16384  20.54    .3682 3.922 4.857e-06    69 210.5 50.52      .3450   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2428 5950 18154 3.051\n",
            "\n",
            "11:09:27 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "11:10:20 | time:1084s total_exs:70664 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6209  2633 5.676  348             16384  21.44    .4094 4.201 4.909e-06    69 29.26 66.75      .3339   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                   2454 6278 2662 .4241\n",
            "\n",
            "11:10:30 | time:1094s total_exs:71044 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6235 17461 38.01  380             16384  21.18    .4124 4.103 4.965e-06 66.68 186.7 60.55      .3321   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2482 6301 17647 2.801\n",
            "\n",
            "11:10:32 | Overflow: setting loss scale to 16384.0\n",
            "11:10:40 | time:1104s total_exs:71380 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9630  6170 16512  33.3  336             16384  21.81    .4124  3.92 5.018e-06 58.63 156.9 50.43      .3474   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2509 6228 16669 2.677\n",
            "\n",
            "11:10:42 | Overflow: setting loss scale to 16384.0\n",
            "11:10:50 | time:1114s total_exs:71784 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9643  6012 16825 40.38  404             16384  19.78    .3902 3.948 5.074e-06  65.5 183.3 51.81      .3550   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2537 6078 17009 2.799\n",
            "\n",
            "Thread 140129149110016 is checking chache.\n",
            "11:11:00 | time:1125s total_exs:72132 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6183 16144 33.65  348             16384   20.7    .3997 4.086 5.128e-06 68.59 179.1 59.51      .3359   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2564 6251 16323 2.611\n",
            "\n",
            "11:11:11 | time:1135s total_exs:72496 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6183 15967 36.15  364             16384  20.53    .4124 3.978 5.18e-06 66.92 172.8 53.39      .3483   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2590 6249 16139 2.583\n",
            "\n",
            "11:11:20 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "11:11:41 | time:1165s total_exs:72860 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6237  5323 11.95  364             16384  21.13    .4094 3.996 5.232e-06 64.38 54.95 54.39      .3501   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                   2616 6301 5378 .8535\n",
            "\n",
            "11:11:51 | time:1176s total_exs:73272 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6039 18170 39.98  412             16384  20.87    .4058 3.932 5.294e-06 67.13   202 51.02      .3518   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2647 6106 18372 3.009\n",
            "\n",
            "11:12:01 | time:1186s total_exs:73676 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6021 17992 40.23  404             16384  20.86    .3856 4.091 5.354e-06 69.17 206.6 59.8      .3214   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2677 6090 18199 2.988\n",
            "\n",
            "11:12:12 | time:1196s total_exs:74116 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6262 18488  43.3  440             16384   20.3    .4124 3.855 5.414e-06  68.4 201.9 47.22      .3679   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2707 6331 18689 2.952\n",
            "\n",
            "11:12:22 | time:1206s total_exs:74580 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5833 16950 44.94  464             16384  20.51    .4094 3.897 5.474e-06 71.37 207.4 49.23      .3545   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2737 5904 17157 2.906\n",
            "\n",
            "11:12:32 | time:1216s total_exs:75020 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5716 17151    44  440             16384  21.58    .3682 3.842 5.534e-06 65.93 197.8 46.62      .3726   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2767 5782 17349 3.001\n",
            "\n",
            "11:12:41 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "11:12:47 | time:1231s total_exs:75388 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6026 11447 24.97  368             16384  21.28    .3762 3.838 5.59e-06 58.11 110.4 46.42      .3835   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   2795 6084 11558  1.9\n",
            "\n",
            "11:12:57 | time:1241s total_exs:75836 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6087 18433 43.76  448             16384  20.21    .3844 3.979 5.652e-06 69.77 211.3 53.47      .3430   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2826 6156 18644 3.029\n",
            "\n",
            "11:13:07 | time:1251s total_exs:76300 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5912 17965 45.48  464             16384  20.77    .3682 3.892 5.714e-06 66.03 200.6 49.03      .3561   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2857 5978 18166 3.039\n",
            "\n",
            "11:13:16 | Overflow: setting loss scale to 16384.0\n",
            "11:13:18 | time:1262s total_exs:76712 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9655  6024 16908 39.88  412             16384   20.7    .3682 3.846 5.772e-06 65.69 184.4 46.81      .3622   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2886 6089 17092 2.807\n",
            "\n",
            "11:13:28 | time:1272s total_exs:77152 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5779 16597 43.57  440             16384  21.23    .3682 3.928 5.83e-06 71.79 206.2 50.81      .3583   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2915 5851 16803 2.872\n",
            "\n",
            "11:13:38 | time:1282s total_exs:77580 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6211 17343 42.68  428             16384  20.18    .4059 3.867 5.886e-06    72   201 47.81      .3467   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2943 6283 17544 2.793\n",
            "\n",
            "11:13:46 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "11:14:41 | time:1346s total_exs:77904 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6188  2334 5.092  324             16384  21.65    .3682 3.934 5.934e-06 61.33 23.13 51.13      .3342   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                   2967 6250 2357 .3772\n",
            "\n",
            "11:14:52 | time:1356s total_exs:78360 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5916 16809 44.67  456             16384  20.16    .3682 3.938 5.992e-06 75.79 215.3 51.32      .3453   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2996 5992 17024 2.841\n",
            "\n",
            "11:15:02 | time:1366s total_exs:78788 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6179 18472 42.65  428             16384  21.11    .4032 3.777 6.052e-06  61.7 184.4 43.69      .3603   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   3026 6241 18657 2.99\n",
            "\n",
            "11:15:12 | time:1376s total_exs:79164 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6186 18195 36.86  376             16384  20.54    .3856  4.06 6.112e-06 71.07   209 57.95      .3302   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3056 6257 18404 2.942\n",
            "\n",
            "11:15:22 | time:1386s total_exs:79556 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6002 17913 38.99  392             16384  21.69    .3682 3.792 6.172e-06    58 173.1 44.36      .3672   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3086 6060 18086 2.985\n",
            "\n",
            "11:15:32 | time:1396s total_exs:79988 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6148 18076 42.34  432             16384  20.86    .3682 3.946 6.232e-06 70.07   206 51.75      .3411   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   3116 6218 18282 2.94\n",
            "\n",
            "11:15:41 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "11:15:47 | time:1411s total_exs:80372 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5942 11125 25.68  384             16384  20.82    .3682 3.963 6.288e-06 65.75 123.1 52.62      .3542   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3144 6008 11249 1.872\n",
            "\n",
            "Thread 140129149110016 is checking chache.\n",
            "Deleting /root/.config/Google/DriveFS/107380112206456973130/content_cache with size 22.315599233 GB.\n",
            "11:15:57 | time:1421s total_exs:80736 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6162 17850 36.36  364             16384  21.55    .3682 3.813 6.346e-06 57.72 167.2 45.29      .3495   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3173 6220 18017 2.897\n",
            "\n",
            "11:16:07 | time:1432s total_exs:81132 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6124 18113 39.04  396             16384  21.29    .3844  4.01 6.406e-06 72.77 215.2 55.14      .3303   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3203 6197 18328 2.958\n",
            "\n",
            "11:16:18 | time:1442s total_exs:81660 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  5594 17118 52.11  528             16384  20.13    .3682 3.723 6.468e-06 73.45 224.7 41.4      .3720   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   3234 5668 17342 3.06\n",
            "\n",
            "11:16:28 | time:1452s total_exs:82096 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5982 17956 43.62  436             16384  20.47    .3857 3.882 6.528e-06 66.37 199.2 48.54      .3531   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3264 6048 18155 3.002\n",
            "\n",
            "11:16:38 | time:1462s total_exs:82596 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5764 17420 48.75  500             16384  20.66    .3682 3.977 6.59e-06 78.87 238.4 53.37      .3444   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3295 5842 17658 3.022\n",
            "\n",
            "11:16:47 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-494969079ffd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mdynamic_batching\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'full'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mlabel_turns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'both'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# https://parl.ai/docs/core/teachers.html#parlai.core.teachers.ConversationTeacher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36m_run_kwargs\u001b[0;34m(cls, kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_from_parser_and_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36m_run_from_parser_and_opt\u001b[0;34m(cls, opt, parser)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mscript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/scripts/train_model.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/scripts/train_model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    740\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tensorboard_log'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_primary_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.checkpoint'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    743\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/scripts/train_model.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(self, suffix)\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;31m# don't ever let a ctrl-c interrupt saving\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_train_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/core/torch_agent.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1844\u001b[0m             \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1845\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# anything found to save?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1846\u001b[0;31m                 \u001b[0matomic_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1847\u001b[0m                 \u001b[0;31m# save opt file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.opt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/parlai/utils/torch.py\u001b[0m in \u001b[0;36matomic_save\u001b[0;34m(state_dict, path)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUSE_ATOMIC_TORCH_SAVE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".tmp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".tmp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/chatbot_model/model.checkpoint.tmp'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9JwQut7wLgz",
        "outputId": "00790f75-73ec-40e2-96db-71a6319c7ad2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls /content/drive/MyDrive/chatbot_model/"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model.checkpoint\t      model.checkpoint.dict.opt\n",
            "model.checkpoint.dict\t      model.checkpoint.opt\n",
            "model.checkpoint.dict.codecs  model.checkpoint.trainstats\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}