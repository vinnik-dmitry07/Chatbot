{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "train_chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM0jpkKHFwFrt7s0ulKeNf1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinnik-dmitry07/Chatbot/blob/main/train_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAbMlxO9DI4h"
      },
      "source": [
        "!nvidia-smi\r\n",
        "!pip install --quiet parlai"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY3EisQ766aP"
      },
      "source": [
        "from pathlib import Path\r\n",
        "\r\n",
        "GDRIVE_ROOT = Path('/content/drive/MyDrive/')\r\n",
        "SAVE_DIR = GDRIVE_ROOT / 'chatbot_model'\r\n",
        "DATA_DIR = GDRIVE_ROOT / 'chatbot_data'"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9y2IWmWDFjU"
      },
      "source": [
        "from datetime import timedelta\r\n",
        "\r\n",
        "EPISODE_DT = timedelta(minutes=3)  # change to split messages in separate dialogues if time delta is greater than EPISODE_DT\r\n",
        "TRAIN_PART, TEST_PART, VALID_PART = 0.996, 0.002, 0.002\r\n",
        "\r\n",
        "assert TRAIN_PART + TEST_PART + VALID_PART == 1"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fr8wrt68ATZl",
        "outputId": "06997fd9-2b64-4a59-a932-54139b569d4b"
      },
      "source": [
        "from google.colab import drive\r\n",
        "\r\n",
        "drive.mount(str(GDRIVE_ROOT.parent))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCM-AVZhHKFK"
      },
      "source": [
        "import json\r\n",
        "\r\n",
        "with open(DATA_DIR / 'result.json', 'r', encoding='utf8') as f:\r\n",
        "    raw_messages = json.load(f)['messages']"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3CvK47rIujd"
      },
      "source": [
        "from datetime import datetime\r\n",
        "\r\n",
        "filtered_messages = []\r\n",
        "for msg in raw_messages:\r\n",
        "    if (\r\n",
        "            'from' in msg and\r\n",
        "            'from_id' in msg and\r\n",
        "            'mime_type' not in msg and\r\n",
        "            msg['text'] and\r\n",
        "            isinstance(msg['text'], str)\r\n",
        "    ):\r\n",
        "        msg['date'] = datetime.strptime(msg['date'], '%Y-%m-%dT%H:%M:%S')\r\n",
        "        filtered_messages.append(msg)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz06qsAJJLda"
      },
      "source": [
        "joined_messages = [filtered_messages[0]]\r\n",
        "for i in range(1, len(filtered_messages)):\r\n",
        "    if (\r\n",
        "            filtered_messages[i - 1]['from_id'] == filtered_messages[i]['from_id'] and\r\n",
        "            filtered_messages[i - 1]['date'] - filtered_messages[i]['date'] <= EPISODE_DT\r\n",
        "    ):\r\n",
        "        joined_messages[-1]['text'] += ' ' + filtered_messages[i]['text']\r\n",
        "    else:\r\n",
        "        joined_messages.append(filtered_messages[i])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUphzoX3K_FF"
      },
      "source": [
        "def partition(alist, indices):\r\n",
        "    return [alist[a:b] for a, b in zip([0] + indices, indices + [None])]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXMAHZTxL-KN"
      },
      "source": [
        "def save_jsonl(messages, suffix, human_readable=False):\r\n",
        "    time_diffs = [messages[i + 1]['date'] - messages[i]['date'] for i in range(len(messages) - 1)]\r\n",
        "    split_positions = [i + 1 for i in range(len(time_diffs)) if time_diffs[i] > EPISODE_DT]\r\n",
        "    episodes = partition(messages, split_positions)\r\n",
        "    print(f'{suffix} episodes: {len(episodes)}, messages: {len(messages)}')\r\n",
        "\r\n",
        "    with open(DATA_DIR / f'data_{suffix}.jsonl', 'w', **({'encoding': 'utf8'} if human_readable else {})) as outfile:\r\n",
        "        for episode in episodes:\r\n",
        "            dialog = [\r\n",
        "                {\r\n",
        "                    'id': i % 2,\r\n",
        "                    'text': msg['text'].replace('\\n', ' '),\r\n",
        "                } for i, msg in enumerate(episode)\r\n",
        "            ]\r\n",
        "\r\n",
        "            episode = {'dialog': [dialog]}\r\n",
        "            json.dump(episode, outfile, **({'ensure_ascii': False} if human_readable else {}))\r\n",
        "            outfile.write('\\n')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApxWsw2omwax",
        "outputId": "fe6d9f00-2c48-45c0-8032-437d52af6da3"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "train, test, valid = np.split(joined_messages, [\r\n",
        "    int(TRAIN_PART * len(joined_messages)),\r\n",
        "    int((TRAIN_PART + TEST_PART) * len(joined_messages)),\r\n",
        "])\r\n",
        "\r\n",
        "save_jsonl(train, suffix='train')\r\n",
        "save_jsonl(test, suffix='test')\r\n",
        "save_jsonl(valid, suffix='valid')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train episodes: 346, messages: 861230\n",
            "test episodes: 1, messages: 1729\n",
            "valid episodes: 1, messages: 1730\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0HWEdG7hMboN",
        "outputId": "5fc52a5c-8800-456d-c820-df77b54870a6"
      },
      "source": [
        "#import os\r\n",
        "\r\n",
        "#os.environ['SAVE_DIR'] = str(SAVE_DIR)\r\n",
        "#!rm --recursive --force $SAVE_DIR\r\n",
        "#!mkdir --parents $SAVE_DIR\r\n",
        "\r\n",
        "\r\n",
        "from parlai.scripts.train_model import TrainModel\r\n",
        "\r\n",
        "TrainModel.main(\r\n",
        "    task='jsonfile',\r\n",
        "    jsonfile_datapath=str(DATA_DIR / 'data'),\r\n",
        "    jsonfile_datatype_extension=True,\r\n",
        "\r\n",
        "    model='transformer/generator',\r\n",
        "    model_file=str(SAVE_DIR / 'model'),\r\n",
        "    \r\n",
        "    init_model='zoo:tutorial_transformer_generator/model',\r\n",
        "\r\n",
        "    n_heads=16, n_layers=8, n_positions=512, text_truncate=512,\r\n",
        "    label_truncate=128, ffn_size=2048, embedding_size=512,\r\n",
        "    activation='gelu', variant='xlm',\r\n",
        "    dict_lower=True, dict_tokenizer='bpe',\r\n",
        "    dict_file='zoo:tutorial_transformer_generator/model.dict',\r\n",
        "    learn_positional_embeddings=True,\r\n",
        "    \r\n",
        "    lr=1e-5, optimizer='adam',\r\n",
        "    warmup_updates=5000,\r\n",
        "    validation_metric='ppl',\r\n",
        "    validation_every_n_secs=60 * 60,\r\n",
        "    save_every_n_secs=10 * 60,  # saving model checkpoint\r\n",
        "\r\n",
        "    batchsize=12, fp16=True, fp16_impl='mem_efficient',\r\n",
        "    \r\n",
        "    skip_generation=True,\r\n",
        "    \r\n",
        "    dynamic_batching='full',\r\n",
        ")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19:31:38 | building dictionary first...\n",
            "19:31:38 | \u001b[33mOverriding opt[\"jsonfile_datapath\"] to /content/drive/MyDrive/chatbot_data/data (previously: data)\u001b[0m\n",
            "19:31:38 | \u001b[33mOverriding opt[\"init_model\"] to zoo:tutorial_transformer_generator/model (previously: /usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model)\u001b[0m\n",
            "19:31:38 | \u001b[33mOverriding opt[\"optimizer\"] to adam (previously: mem_eff_adam)\u001b[0m\n",
            "19:31:38 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: download_path: None,datapath: /usr/local/lib/python3.6/dist-packages/data,interactive_mode: False\u001b[0m\n",
            "19:31:38 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--jsonfile-datapath data --force-fp16-tokens False --optimizer mem_eff_adam\u001b[0m\n",
            "19:31:38 | Using CUDA\n",
            "19:31:38 | loading dictionary from /content/drive/MyDrive/chatbot_model/model.checkpoint.dict\n",
            "19:31:38 | num words = 54944\n",
            "19:31:40 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "19:31:40 | Loading existing model params from /content/drive/MyDrive/chatbot_model/model.checkpoint\n",
            "19:32:13 | Opt:\n",
            "19:32:13 |     activation: gelu\n",
            "19:32:13 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "19:32:13 |     adam_eps: 1e-08\n",
            "19:32:13 |     add_p1_after_newln: False\n",
            "19:32:13 |     aggregate_micro: False\n",
            "19:32:13 |     allow_missing_init_opts: False\n",
            "19:32:13 |     attention_dropout: 0.0\n",
            "19:32:13 |     batchsize: 12\n",
            "19:32:13 |     beam_block_full_context: True\n",
            "19:32:13 |     beam_block_list_filename: None\n",
            "19:32:13 |     beam_block_ngram: -1\n",
            "19:32:13 |     beam_context_block_ngram: -1\n",
            "19:32:13 |     beam_delay: 30\n",
            "19:32:13 |     beam_length_penalty: 0.65\n",
            "19:32:13 |     beam_min_length: 1\n",
            "19:32:13 |     beam_size: 1\n",
            "19:32:13 |     betas: '[0.9, 0.999]'\n",
            "19:32:13 |     bpe_add_prefix_space: None\n",
            "19:32:13 |     bpe_debug: False\n",
            "19:32:13 |     bpe_merge: None\n",
            "19:32:13 |     bpe_vocab: None\n",
            "19:32:13 |     compute_tokenized_bleu: False\n",
            "19:32:13 |     datapath: /usr/local/lib/python3.6/dist-packages/data\n",
            "19:32:13 |     datatype: train\n",
            "19:32:13 |     delimiter: '\\n'\n",
            "19:32:13 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "19:32:13 |     dict_endtoken: __end__\n",
            "19:32:13 |     dict_file: /content/drive/MyDrive/chatbot_model/model.checkpoint.dict\n",
            "19:32:13 |     dict_include_test: False\n",
            "19:32:13 |     dict_include_valid: False\n",
            "19:32:13 |     dict_initpath: None\n",
            "19:32:13 |     dict_language: english\n",
            "19:32:13 |     dict_loaded: True\n",
            "19:32:13 |     dict_lower: True\n",
            "19:32:13 |     dict_max_ngram_size: -1\n",
            "19:32:13 |     dict_maxexs: -1\n",
            "19:32:13 |     dict_maxtokens: -1\n",
            "19:32:13 |     dict_minfreq: 0\n",
            "19:32:13 |     dict_nulltoken: __null__\n",
            "19:32:13 |     dict_starttoken: __start__\n",
            "19:32:13 |     dict_textfields: text,labels\n",
            "19:32:13 |     dict_tokenizer: bpe\n",
            "19:32:13 |     dict_unktoken: __unk__\n",
            "19:32:13 |     display_examples: False\n",
            "19:32:13 |     download_path: None\n",
            "19:32:13 |     dropout: 0.0\n",
            "19:32:13 |     dynamic_batching: full\n",
            "19:32:13 |     embedding_projection: random\n",
            "19:32:13 |     embedding_size: 512\n",
            "19:32:13 |     embedding_type: random\n",
            "19:32:13 |     embeddings_scale: True\n",
            "19:32:13 |     eval_batchsize: None\n",
            "19:32:13 |     evaltask: None\n",
            "19:32:13 |     ffn_size: 2048\n",
            "19:32:13 |     force_fp16_tokens: True\n",
            "19:32:13 |     fp16: True\n",
            "19:32:13 |     fp16_impl: mem_efficient\n",
            "19:32:13 |     gpu: -1\n",
            "19:32:13 |     gradient_clip: 0.1\n",
            "19:32:13 |     hf_skip_special_tokens: True\n",
            "19:32:13 |     hide_labels: False\n",
            "19:32:13 |     history_add_global_end_token: None\n",
            "19:32:13 |     history_reversed: False\n",
            "19:32:13 |     history_size: -1\n",
            "19:32:13 |     image_cropsize: 224\n",
            "19:32:13 |     image_mode: raw\n",
            "19:32:13 |     image_size: 256\n",
            "19:32:13 |     inference: greedy\n",
            "19:32:13 |     init_model: /content/drive/MyDrive/chatbot_model/model.checkpoint\n",
            "19:32:13 |     init_opt: None\n",
            "19:32:13 |     interactive_mode: False\n",
            "19:32:13 |     invsqrt_lr_decay_gamma: -1\n",
            "19:32:13 |     jsonfile_datapath: /content/drive/MyDrive/chatbot_data/data\n",
            "19:32:13 |     jsonfile_datatype_extension: True\n",
            "19:32:13 |     label_truncate: 128\n",
            "19:32:13 |     label_turns: secondspeaker\n",
            "19:32:13 |     learn_positional_embeddings: True\n",
            "19:32:13 |     learningrate: 1e-05\n",
            "19:32:13 |     load_from_checkpoint: True\n",
            "19:32:13 |     log_every_n_secs: 10\n",
            "19:32:13 |     loglevel: info\n",
            "19:32:13 |     lr_scheduler: reduceonplateau\n",
            "19:32:13 |     lr_scheduler_decay: 0.5\n",
            "19:32:13 |     lr_scheduler_patience: 3\n",
            "19:32:13 |     max_lr_steps: -1\n",
            "19:32:13 |     max_train_time: -1\n",
            "19:32:13 |     metrics: default\n",
            "19:32:13 |     model: transformer/generator\n",
            "19:32:13 |     model_file: /content/drive/MyDrive/chatbot_model/model\n",
            "19:32:13 |     model_parallel: False\n",
            "19:32:13 |     momentum: 0\n",
            "19:32:13 |     multitask_weights: [1]\n",
            "19:32:13 |     n_decoder_layers: -1\n",
            "19:32:13 |     n_encoder_layers: -1\n",
            "19:32:13 |     n_heads: 16\n",
            "19:32:13 |     n_layers: 8\n",
            "19:32:13 |     n_positions: 512\n",
            "19:32:13 |     n_segments: 0\n",
            "19:32:13 |     nesterov: True\n",
            "19:32:13 |     no_cuda: False\n",
            "19:32:13 |     num_epochs: -1\n",
            "19:32:13 |     nus: [0.7]\n",
            "19:32:13 |     optimizer: mem_eff_adam\n",
            "19:32:13 |     output_scaling: 1.0\n",
            "19:32:13 |     override: \"{'task': 'jsonfile', 'jsonfile_datapath': '/content/drive/MyDrive/chatbot_data/data', 'jsonfile_datatype_extension': True, 'model': 'transformer/generator', 'model_file': '/content/drive/MyDrive/chatbot_model/model', 'init_model': 'zoo:tutorial_transformer_generator/model', 'n_heads': 16, 'n_layers': 8, 'n_positions': 512, 'text_truncate': 512, 'label_truncate': 128, 'ffn_size': 2048, 'embedding_size': 512, 'activation': 'gelu', 'variant': 'xlm', 'dict_lower': True, 'dict_tokenizer': 'bpe', 'dict_file': '/usr/local/lib/python3.6/dist-packages/data/models/tutorial_transformer_generator/model.dict', 'learn_positional_embeddings': True, 'learningrate': 1e-05, 'optimizer': 'adam', 'warmup_updates': 5000, 'validation_metric': 'ppl', 'validation_every_n_secs': 3600.0, 'save_every_n_secs': 600.0, 'batchsize': 12, 'fp16': True, 'fp16_impl': 'mem_efficient', 'skip_generation': True, 'dynamic_batching': 'full'}\"\n",
            "19:32:13 |     parlai_home: /usr/local/lib/python3.6/dist-packages\n",
            "19:32:13 |     person_tokens: False\n",
            "19:32:13 |     rank_candidates: False\n",
            "19:32:13 |     relu_dropout: 0.0\n",
            "19:32:13 |     save_after_valid: False\n",
            "19:32:13 |     save_every_n_secs: 600.0\n",
            "19:32:13 |     share_word_embeddings: True\n",
            "19:32:13 |     short_final_eval: False\n",
            "19:32:13 |     skip_generation: True\n",
            "19:32:13 |     special_tok_lst: None\n",
            "19:32:13 |     split_lines: False\n",
            "19:32:13 |     starttime: Feb14_18-20\n",
            "19:32:13 |     task: jsonfile\n",
            "19:32:13 |     temperature: 1.0\n",
            "19:32:13 |     tensorboard_log: False\n",
            "19:32:13 |     tensorboard_logdir: None\n",
            "19:32:13 |     text_truncate: 512\n",
            "19:32:13 |     topk: 10\n",
            "19:32:13 |     topp: 0.9\n",
            "19:32:13 |     truncate: -1\n",
            "19:32:13 |     update_freq: 1\n",
            "19:32:13 |     use_reply: label\n",
            "19:32:13 |     validation_cutoff: 1.0\n",
            "19:32:13 |     validation_every_n_epochs: -1\n",
            "19:32:13 |     validation_every_n_secs: 3600.0\n",
            "19:32:13 |     validation_max_exs: -1\n",
            "19:32:13 |     validation_metric: ppl\n",
            "19:32:13 |     validation_metric_mode: None\n",
            "19:32:13 |     validation_patience: 10\n",
            "19:32:13 |     validation_share_agent: False\n",
            "19:32:13 |     variant: xlm\n",
            "19:32:13 |     warmup_rate: 0.0001\n",
            "19:32:13 |     warmup_updates: 5000\n",
            "19:32:13 |     weight_decay: None\n",
            "19:32:13 | creating task(s): jsonfile\n",
            "19:32:13 | [loading data from json file into task:/content/drive/MyDrive/chatbot_data/data_train.jsonl]\n",
            "19:32:16 | \u001b[31mMetadata does not exist. Please double check your datapath.\u001b[0m\n",
            "19:32:20 | training...\n",
            "19:32:21 | Overflow: setting loss scale to 65536.0\n",
            "19:32:21 | Overflow: setting loss scale to 32768.0\n",
            "19:32:30 | time:3697s total_exs:164192 epochs:0.38\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9355  3838 11937 502.4 5008             33825  9.289    .5908 3.345 1e-05   885  2753 28.36      .3762   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10171 4723 14689 3.111\n",
            "\n",
            "19:32:41 | time:3707s total_exs:167104 epochs:0.39\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5387 18066 287.2 2912             32768   11.8    .6404 3.147 1e-05 509.1  1707 23.27      .3890   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10205 5896 19773 3.354\n",
            "\n",
            "19:32:45 | Overflow: setting loss scale to 32768.0\n",
            "19:32:47 | Overflow: setting loss scale to 32768.0\n",
            "19:32:51 | time:3717s total_exs:169436 epochs:0.39\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9394  5716 18727 231.5 2332             32768  11.41    .6270 3.189 1e-05 403.8  1323 24.27      .3842   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10238 6120 20050 3.277\n",
            "\n",
            "19:32:54 | Overflow: setting loss scale to 32768.0\n",
            "19:32:55 | Overflow: setting loss scale to 32768.0\n",
            "19:32:56 | Overflow: setting loss scale to 32768.0\n",
            "19:32:59 | Overflow: setting loss scale to 32768.0\n",
            "19:33:01 | time:3727s total_exs:171368 epochs:0.40\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .8788  5963 19661   193 1932             32768   11.4    .5183 3.264 1e-05 417.8  1378 26.15      .3742   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10271 6381 21039 3.298\n",
            "\n",
            "19:33:03 | Overflow: setting loss scale to 16384.0\n",
            "19:33:11 | time:3737s total_exs:173072 epochs:0.40\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "   .9697  6175 20300 169.7 1704             20852  13.41    .5622 3.203 1e-05 307.8  1012 24.6      .3919   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10304 6483 21312 3.288\n",
            "\n",
            "19:33:21 | time:3747s total_exs:174860 epochs:0.41\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5944 19849 175.6 1788             16384  13.29    .6409 3.082 1e-05 297.4 993.1 21.79      .4040   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                  10338 6241 20842 3.34\n",
            "\n",
            "19:33:31 | time:3757s total_exs:176268 epochs:0.41\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6133 20495 138.4 1408             16384  14.84    .5672 3.258 1e-05 288.7 964.9   26      .3751   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10372 6422 21459 3.342\n",
            "\n",
            "19:33:41 | time:3768s total_exs:177808 epochs:0.41\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5982 19804 149.9 1540             16384  13.83    .4936 3.087 1e-05 241.6 799.6 21.91      .3944   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10406 6224 20604 3.311\n",
            "\n",
            "19:33:52 | time:3778s total_exs:179036 epochs:0.42\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6623 21811 122.5 1228             16384  15.35    .5871 3.023 1e-05 221.4   729 20.56      .4193   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10439 6845 22540 3.293\n",
            "\n",
            "19:34:02 | time:3788s total_exs:180168 epochs:0.42\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6482 20959 110.9 1132             16384  16.81    .6168 3.195 1e-05 217.4 703.1 24.41      .3854   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10472 6699 21662 3.234\n",
            "\n",
            "19:34:12 | time:3798s total_exs:181324 epochs:0.42\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6227 20018 112.6 1156             16384  15.87    .4302 3.062 1e-05 201.2 646.9 21.38      .4011   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10505 6429 20665 3.215\n",
            "\n",
            "19:34:22 | time:3808s total_exs:182268 epochs:0.42\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6570 21009 94.33  944             16384  16.84    .4640 3.265 1e-05 186.8 597.2 26.18      .3845   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10537 6757 21606 3.198\n",
            "\n",
            "19:34:32 | time:3818s total_exs:183236 epochs:0.43\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6243 19981 96.81  968             16384  16.01    .5072 3.238 1e-05 190.3   609 25.49      .3804   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10569 6433 20590 3.201\n",
            "\n",
            "19:34:42 | time:3829s total_exs:184384 epochs:0.43\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6131 19677 111.7 1148             16384  16.15    .4496 3.179 1e-05 194.7 624.9 24.02      .3891   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                  10602 6325 20302 3.21\n",
            "\n",
            "19:34:53 | time:3839s total_exs:185500 epochs:0.43\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6215 20291 110.4 1116             16384  15.88    .4630 3.189 1e-05 205.6 671.2 24.26      .3841   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10635 6421 20962 3.265\n",
            "\n",
            "19:35:03 | time:3849s total_exs:186316 epochs:0.43\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6665 20883 79.89  816             16384  17.11    .4532 3.117 1e-05 152.6 478.2 22.59      .3958   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10667 6818 21362 3.133\n",
            "\n",
            "19:35:13 | time:3859s total_exs:187204 epochs:0.43\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6419 20097 86.88  888             16384  17.03    .5638 3.038 1e-05 166.6 521.7 20.86      .4000   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10699 6585 20618 3.131\n",
            "\n",
            "19:35:23 | time:3869s total_exs:188124 epochs:0.44\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6340 20081 91.05  920             16384  16.85    .4594 3.113 1e-05 164.2 519.9 22.49      .3885   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10731 6505 20601 3.167\n",
            "\n",
            "19:35:23 | Overflow: setting loss scale to 16384.0\n",
            "19:35:33 | time:3880s total_exs:188932 epochs:0.44\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9688  6443 20151 78.97  808             16384  16.77    .4738 3.198 1e-05 163.1 510.2 24.49      .3914   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10763 6606 20661 3.128\n",
            "\n",
            "19:35:39 | Overflow: setting loss scale to 16384.0\n",
            "19:35:43 | time:3890s total_exs:189744 epochs:0.44\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9677  6273 19363 80.85  812             16384  17.86    .4591 3.165 1e-05 139.4 430.1 23.68      .3850   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10794 6413 19793 3.087\n",
            "\n",
            "19:35:49 | Overflow: setting loss scale to 16384.0\n",
            "19:35:50 | Overflow: setting loss scale to 16384.0\n",
            "19:35:53 | Overflow: setting loss scale to 16384.0\n",
            "19:35:54 | time:3900s total_exs:190444 epochs:0.44\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9062  6278 19508 67.97  700             16384  17.66    .4591 3.051 1e-05 118.7 368.9 21.14      .3962   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10826 6397 19877 3.107\n",
            "\n",
            "19:35:57 | Overflow: setting loss scale to 16384.0\n",
            "19:36:04 | time:3910s total_exs:191168 epochs:0.44\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9677  6623 19990 70.49  724             16384  17.75    .4453 3.252 1e-05 139.1 419.9 25.85      .3793   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10857 6762 20410 3.018\n",
            "\n",
            "19:36:08 | Overflow: setting loss scale to 16384.0\n",
            "19:36:14 | time:3921s total_exs:191900 epochs:0.45\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9677  6436 19624 71.99  732             16384  17.74    .4511 3.141 1e-05 131.7 401.5 23.12      .3934   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10888 6568 20026 3.049\n",
            "\n",
            "19:36:25 | time:3931s total_exs:192640 epochs:0.45\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6429 19550 72.59  740             16384  17.77    .4832  3.03 1e-05 124.3   378 20.7      .4189   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10919 6553 19928 3.041\n",
            "\n",
            "19:36:35 | time:3941s total_exs:193276 epochs:0.45\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6474 19525 61.87  636             16384  18.97    .4812 3.287 1e-05 139.1 419.4 26.77      .3711   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10950 6613 19944 3.016\n",
            "\n",
            "19:36:45 | time:3951s total_exs:194024 epochs:0.45\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6620 20378 74.27  748             16384  18.81    .5034 3.096 1e-05 133.9 412.2 22.11      .3960   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10981 6754 20790 3.079\n",
            "\n",
            "19:36:55 | time:3962s total_exs:194768 epochs:0.45\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5863 17531 71.76  744             16384  17.47    .6490 3.056 1e-05 150.3 449.5 21.23      .4000   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                  11012 6013 17981 2.99\n",
            "\n",
            "19:36:59 | Overflow: setting loss scale to 16384.0\n",
            "19:37:05 | time:3972s total_exs:195336 epochs:0.45\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9667  6388 18856 55.89  568             16384  19.59    .4680 3.244 1e-05 131.4 387.9 25.64      .3656   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11042 6519 19244 2.952\n",
            "\n",
            "19:37:16 | time:3982s total_exs:195992 epochs:0.46\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6252 18920 64.04  656             16384  20.52    .4944 3.184 1e-05 135.7 410.8 24.15      .3814   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11073 6387 19331 3.027\n",
            "\n",
            "19:37:25 | Overflow: setting loss scale to 16384.0\n",
            "19:37:26 | time:3992s total_exs:196620 epochs:0.46\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9677  6597 19719 60.55  628             16384  20.63    .5004 3.139 1e-05 128.7 384.8 23.09      .3761   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11104 6726 20104 2.989\n",
            "\n",
            "19:37:27 | Overflow: setting loss scale to 16384.0\n",
            "19:37:36 | time:4003s total_exs:197268 epochs:0.46\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "   .9667  6499 19170 63.71  648             16384  18.87    .4785 3.223 1e-05 125.2 369.3 25.1      .3794   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                  11134 6625 19539 2.95\n",
            "\n",
            "19:37:46 | time:4013s total_exs:197888 epochs:0.46\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6238 18577 61.54  620             16384  19.55    .5034 3.032 1e-05 125.3 373.1 20.73      .4004   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11164 6363 18950 2.978\n",
            "\n",
            "19:37:57 | time:4023s total_exs:198432 epochs:0.46\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6464 19037 53.41  544             16384  20.88    .4944  3.25 1e-05 117.4 345.9 25.79      .3840   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11194 6581 19383 2.945\n",
            "\n",
            "19:38:07 | time:4033s total_exs:198936 epochs:0.46\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6526 19589 50.42  504             16384  21.86    .4877 3.082 1e-05 101.6   305 21.81      .3957   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11224 6628 19894 3.002\n",
            "\n",
            "19:38:17 | time:4043s total_exs:199444 epochs:0.46\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6360 18182 50.08  508             16384  21.19    .5101 3.222 1e-05 107.4   307 25.08      .3953   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11253 6468 18489 2.859\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-a5c1e6fc6fb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mskip_generation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mdynamic_batching\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'full'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36m_run_kwargs\u001b[0;34m(cls, kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_from_parser_and_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36m_run_from_parser_and_opt\u001b[0;34m(cls, opt, parser)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mscript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/scripts/train_model.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/scripts/train_model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    656\u001b[0m                 \u001b[0;31m# do one example / batch of examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m                     \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparley\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mStopTrainException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m                     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Stopping from {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/core/worlds.py\u001b[0m in \u001b[0;36mparley\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0;31m# great, this batch is good to go! let's run it!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m         \u001b[0macts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_acts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m         \u001b[0;31m# broadcast the results back to all the models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/core/torch_agent.py\u001b[0m in \u001b[0;36mbatch_act\u001b[0;34m(self, observations)\u001b[0m\n\u001b[1;32m   1967\u001b[0m             \u001b[0;31m# register the start of updates for later counting when they occur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1968\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ups'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGlobalTimerMetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1970\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/core/torch_generator_agent.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/core/torch_generator_agent.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, batch, return_output)\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_vec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot compute loss without a label.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m         \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0mscore_view\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/core/torch_generator_agent.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ys, prev_enc, maxlen, bsz, *xs)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;31m# use teacher forcing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_forced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/parlai/core/torch_generator_agent.py\u001b[0m in \u001b[0;36mdecode_forced\u001b[0;34m(self, encoder_states, ys)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mseqlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseqlen\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTART_IDX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m             raise AssertionError(\n\u001b[1;32m    191\u001b[0m                 \u001b[0;34m\"The Beginning of Sentence token is automatically added to the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}