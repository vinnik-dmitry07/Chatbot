{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPmQUru299/YdSx1vLBveMx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinnik-dmitry07/Chatbot/blob/main/train_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAbMlxO9DI4h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a8fdb10-975f-40d8-8d9e-9d8a660c17e4"
      },
      "source": [
        "!nvidia-smi\r\n",
        "!pip install --quiet parlai"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Feb 27 20:15:14 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 8.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 40kB 5.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 9.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 51.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 50.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.3MB 41.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 215kB 59.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 54.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.8MB 54.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 56.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 45.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 55.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 57.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 317kB 53.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 10.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.0MB 45.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 48.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 9.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 11.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 53.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 11.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 55.3MB/s \n",
            "\u001b[?25h  Building wheel for docformatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for websocket-server (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for untokenize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: sphinx-autodoc-typehints 1.11.1 has requirement Sphinx>=3.0, but you'll have sphinx 1.8.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: myst-parser 0.12.10 has requirement sphinx<4,>=2, but you'll have sphinx 1.8.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: omegaconf 2.0.6 has requirement PyYAML>=5.1.*, but you'll have pyyaml 3.13 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fvcore 0.1.3.post20210227 has requirement pyyaml>=5.1, but you'll have pyyaml 3.13 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY3EisQ766aP"
      },
      "source": [
        "from pathlib import Path\r\n",
        "\r\n",
        "GDRIVE_ROOT = Path('/content/drive/MyDrive/')\r\n",
        "SAVE_DIR = GDRIVE_ROOT / 'chatbot_model'\r\n",
        "DATA_DIR = GDRIVE_ROOT / 'chatbot_data'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9y2IWmWDFjU"
      },
      "source": [
        "from datetime import timedelta\r\n",
        "\r\n",
        "EPISODE_DT = timedelta(minutes=3)  # change to split messages in separate dialogues if time delta is greater than EPISODE_DT\r\n",
        "TRAIN_PART, TEST_PART, VALID_PART = 0.996, 0.002, 0.002\r\n",
        "\r\n",
        "assert TRAIN_PART + TEST_PART + VALID_PART == 1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr8wrt68ATZl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52cfc110-f493-40e5-89b4-9a2dee50b2f4"
      },
      "source": [
        "from google.colab import drive\r\n",
        "\r\n",
        "drive.mount(str(GDRIVE_ROOT.parent))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCM-AVZhHKFK"
      },
      "source": [
        "import json\r\n",
        "\r\n",
        "with open(DATA_DIR / 'result.json', 'r', encoding='utf8') as f:\r\n",
        "    raw_messages = json.load(f)['messages']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3CvK47rIujd"
      },
      "source": [
        "from datetime import datetime\r\n",
        "\r\n",
        "filtered_messages = []\r\n",
        "for msg in raw_messages:\r\n",
        "    if (\r\n",
        "            'from' in msg and\r\n",
        "            'from_id' in msg and\r\n",
        "            'mime_type' not in msg and\r\n",
        "            msg['text'] and\r\n",
        "            isinstance(msg['text'], str) and\r\n",
        "            len(msg['text']) < 50\r\n",
        "    ):\r\n",
        "        msg1 = msg.copy()\r\n",
        "        msg1['date'] = datetime.strptime(msg1['date'], '%Y-%m-%dT%H:%M:%S')\r\n",
        "        filtered_messages.append(msg1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz06qsAJJLda"
      },
      "source": [
        "import re\r\n",
        "\r\n",
        "joined_messages = []\r\n",
        "for i in range(len(filtered_messages)):\r\n",
        "    alphanum_text = re.sub(r'[^A-Za-z0-9 ]+', '', filtered_messages[i]['text']).strip()\r\n",
        "    if alphanum_text:\r\n",
        "        if (    \r\n",
        "                joined_messages and    \r\n",
        "                filtered_messages[i - 1]['from_id'] == filtered_messages[i]['from_id'] and\r\n",
        "                filtered_messages[i - 1]['date'] - filtered_messages[i]['date'] <= EPISODE_DT\r\n",
        "        ):\r\n",
        "            joined_messages[-1]['text'] += ' ' + alphanum_text\r\n",
        "        else:\r\n",
        "            new_message = filtered_messages[i].copy()\r\n",
        "            new_message['text'] = alphanum_text\r\n",
        "            joined_messages.append(new_message)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUphzoX3K_FF"
      },
      "source": [
        "def partition(alist, indices):\r\n",
        "    return [alist[a:b] for a, b in zip([0] + indices, indices + [None])]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXMAHZTxL-KN"
      },
      "source": [
        "def save_jsonl(messages, suffix, human_readable=False):\r\n",
        "    time_diffs = [messages[i + 1]['date'] - messages[i]['date'] for i in range(len(messages) - 1)]\r\n",
        "    split_positions = [i + 1 for i in range(len(time_diffs)) if time_diffs[i] > EPISODE_DT]\r\n",
        "    episodes = partition(messages, split_positions)\r\n",
        "    print(f'{suffix} episodes: {len(episodes)}, messages: {len(messages)}')\r\n",
        "\r\n",
        "    with open(DATA_DIR / f'data_{suffix}.jsonl', 'w', **({'encoding': 'utf8'} if human_readable else {})) as outfile:\r\n",
        "        for episode in episodes:\r\n",
        "            dialog = [{'id': i % 2, 'text': msg['text']} for i, msg in enumerate(episode)]\r\n",
        "            episode = {'dialog': [dialog]}\r\n",
        "            json.dump(episode, outfile, **({'ensure_ascii': False} if human_readable else {}))\r\n",
        "            outfile.write('\\n')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApxWsw2omwax",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83b52518-8941-42e6-f1a7-e329819d4a33"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "train, test, valid = np.split(joined_messages, [\r\n",
        "    int(TRAIN_PART * len(joined_messages)),\r\n",
        "    int((TRAIN_PART + TEST_PART) * len(joined_messages)),\r\n",
        "])\r\n",
        "\r\n",
        "save_jsonl(train, suffix='train')\r\n",
        "save_jsonl(test, suffix='test')\r\n",
        "save_jsonl(valid, suffix='valid')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train episodes: 424, messages: 754917\n",
            "test episodes: 1, messages: 1516\n",
            "valid episodes: 1, messages: 1516\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5JX4EM1Jb56"
      },
      "source": [
        "import shutil\r\n",
        "import subprocess\r\n",
        "import time\r\n",
        "import threading\r\n",
        "from pathlib import Path\r\n",
        "\r\n",
        "\r\n",
        "def dir_size_bytes(path):\r\n",
        "    return int(subprocess.check_output(['du','--bytes', '--summarize', path]).split()[0].decode())\r\n",
        "\r\n",
        "def check_chache(max_cache_size_gb=18, check_period_minutes=5):\r\n",
        "    this_id = str(threading.get_ident())\r\n",
        "    thread_path = Path('/threads')\r\n",
        "    thread_path.mkdir(exist_ok=True)\r\n",
        "\r\n",
        "    def threads_ids():\r\n",
        "        return [str(p.name) for p in thread_path.iterdir() if p.is_file()]\r\n",
        "    \r\n",
        "    if not(threads_ids()):\r\n",
        "        (thread_path / this_id).open(mode='w').close()\r\n",
        "        while True:\r\n",
        "            ids = threads_ids()\r\n",
        "            if not (len(ids) == 1 and ids[0] == this_id):\r\n",
        "                break\r\n",
        "\r\n",
        "            print(f'Thread {this_id} is checking chache.')\r\n",
        "\r\n",
        "            for cache_path in Path('/root/.config/Google/DriveFS').glob('**/content_cache'):\r\n",
        "                chache_path_str = str(cache_path)\r\n",
        "                chache_size_gb = dir_size_bytes(chache_path_str) / 10 ** 9\r\n",
        "                if chache_size_gb > max_cache_size_gb:\r\n",
        "                    print(f'Deleting {chache_path_str} with size {chache_size_gb} GB.')\r\n",
        "                    shutil.rmtree(chache_path_str)\r\n",
        "            time.sleep(check_period_minutes * 60)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HWEdG7hMboN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "01f52c68-0af5-4232-9d71-33c2bd4778ed"
      },
      "source": [
        "# threading.Thread(target=check_chache).start()\r\n",
        "\r\n",
        "import os\r\n",
        "\r\n",
        "os.environ['SAVE_DIR'] = str(SAVE_DIR)\r\n",
        "!rm --recursive --force $SAVE_DIR\r\n",
        "!mkdir --parents $SAVE_DIR\r\n",
        "\r\n",
        "\r\n",
        "from parlai.scripts.train_model import TrainModel\r\n",
        "\r\n",
        "TrainModel.main(\r\n",
        "    task='jsonfile',\r\n",
        "    jsonfile_datapath=str(DATA_DIR / 'data'),\r\n",
        "    jsonfile_datatype_extension=True,\r\n",
        "\r\n",
        "    model='transformer/generator',\r\n",
        "    model_file=str(SAVE_DIR / 'model'),\r\n",
        "    \r\n",
        "    init_model='zoo:tutorial_transformer_generator/model',\r\n",
        "\r\n",
        "    n_heads=16, n_layers=8, n_positions=512, text_truncate=512,\r\n",
        "    label_truncate=128, ffn_size=2048, embedding_size=512,\r\n",
        "    activation='gelu', variant='xlm',\r\n",
        "    dict_lower=True, dict_tokenizer='bpe',\r\n",
        "    dict_file='zoo:tutorial_transformer_generator/model.dict',\r\n",
        "    learn_positional_embeddings=True,\r\n",
        "    \r\n",
        "    lr=1e-5, optimizer='adam',\r\n",
        "    warmup_updates=5000,\r\n",
        "    validation_metric='ppl',\r\n",
        "    validation_every_n_secs=60 * 60,  # running eval: valid\r\n",
        "    # save_every_n_secs=60,  # saving model checkpoint\r\n",
        "\r\n",
        "    batchsize=12, fp16=True, fp16_impl='mem_efficient',\r\n",
        "    \r\n",
        "    skip_generation=True,\r\n",
        "    \r\n",
        "    dynamic_batching='full',\r\n",
        "\r\n",
        "    label_turns='both',  # https://parl.ai/docs/core/teachers.html#parlai.core.teachers.ConversationTeacher\r\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20:19:02 | building data: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz\n",
            "20:19:02 | Downloading http://parl.ai/downloads/_models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz to /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading tutorial_transformer_generator_v1.tar.gz: 100%|██████████| 1.12G/1.12G [00:15<00:00, 74.1MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:19:37 | building dictionary first...\n",
            "20:19:37 | No model with opt yet at: /content/drive/MyDrive/chatbot_model/model(.opt)\n",
            "20:19:37 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: full,verbose: False,datapath: /usr/local/lib/python3.7/dist-packages/data,eval_dynamic_batching: None,load_from_checkpoint: True,tensorboard_logdir: None,jsonfile_datapath: /content/drive/MyDrive/chatbot_data/data,jsonfile_datatype_extension: True,label_turns: both,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,beam_block_full_context: True,beam_length_penalty: 0.65,topk: 10,topp: 0.9,beam_delay: 30,beam_block_list_filename: None,temperature: 1.0,compute_tokenized_bleu: False,interactive_mode: False,fp16_impl: mem_efficient,force_fp16_tokens: False,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,max_lr_steps: -1,invsqrt_lr_decay_gamma: -1,parlai_home: /usr/local/lib/python3.7/dist-packages\u001b[0m\n",
            "20:19:37 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--show-advanced-args False --task internal:new_reddit:presorted --datatype train:stream --numthreads 1 --batchsize 48 --num-epochs 5.0 --validation-every-n-secs 1800.0 --save-every-n-secs -1 --save-after-valid True --validation-max-exs 9920 --short-final-eval True --validation-patience 0 --validation-metric-mode min --dict-build-first True --numworkers 4 --pytorch-preprocess False --pytorch-teacher-batch-sort False --batch-sort-cache-type pop --batch-length-range 5 --shuffle False --batch-sort-field text --pytorch-context-length -1 --pytorch-include-labels True --log-every-n-secs 30.0 --distributed-world-size 64 --port 61337 --dropout 0.1 --beam-size 8 --beam-min-n-best 3 --beam-min-length 10 --skip-generation False --inference beam --optimizer fused_adam --learningrate 0.0005 --gradient-clip 10.0 --adam-eps 1e-06 --betas 0.9,0.98 --weight-decay 0.01 --lr-scheduler invsqrt --warmup-updates 20000 --gpu 0 --beam-block-ngram 3 --beam-context-block-ngram 3\u001b[0m\n",
            "20:19:37 | Using CUDA\n",
            "20:19:37 | loading dictionary from /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "20:19:37 | num words = 54944\n",
            "20:19:38 | \u001b[33mDEPRECATED: XLM should only be used for backwards compatibility, as it involves a less-stable layernorm operation.\u001b[0m\n",
            "20:19:48 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "20:19:48 | Loading existing model params from /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "20:19:49 | \u001b[33mNot loading optim state since optim class changed.\u001b[0m\n",
            "20:19:49 | Opt:\n",
            "20:19:49 |     activation: gelu\n",
            "20:19:50 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "20:19:50 |     adam_eps: 1e-08\n",
            "20:19:50 |     add_p1_after_newln: False\n",
            "20:19:50 |     aggregate_micro: False\n",
            "20:19:50 |     allow_missing_init_opts: False\n",
            "20:19:50 |     attention_dropout: 0.0\n",
            "20:19:50 |     batchsize: 12\n",
            "20:19:50 |     beam_block_full_context: True\n",
            "20:19:50 |     beam_block_list_filename: None\n",
            "20:19:50 |     beam_block_ngram: -1\n",
            "20:19:50 |     beam_context_block_ngram: -1\n",
            "20:19:50 |     beam_delay: 30\n",
            "20:19:50 |     beam_length_penalty: 0.65\n",
            "20:19:50 |     beam_min_length: 1\n",
            "20:19:50 |     beam_size: 1\n",
            "20:19:50 |     betas: '(0.9, 0.999)'\n",
            "20:19:50 |     bpe_add_prefix_space: None\n",
            "20:19:50 |     bpe_debug: False\n",
            "20:19:50 |     bpe_dropout: None\n",
            "20:19:50 |     bpe_merge: None\n",
            "20:19:50 |     bpe_vocab: None\n",
            "20:19:50 |     compute_tokenized_bleu: False\n",
            "20:19:50 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "20:19:50 |     datatype: train\n",
            "20:19:50 |     delimiter: '\\n'\n",
            "20:19:50 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "20:19:50 |     dict_endtoken: __end__\n",
            "20:19:50 |     dict_file: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "20:19:50 |     dict_include_test: False\n",
            "20:19:50 |     dict_include_valid: False\n",
            "20:19:50 |     dict_initpath: None\n",
            "20:19:50 |     dict_language: english\n",
            "20:19:50 |     dict_loaded: True\n",
            "20:19:50 |     dict_lower: True\n",
            "20:19:50 |     dict_max_ngram_size: -1\n",
            "20:19:50 |     dict_maxexs: -1\n",
            "20:19:50 |     dict_maxtokens: -1\n",
            "20:19:50 |     dict_minfreq: 0\n",
            "20:19:50 |     dict_nulltoken: __null__\n",
            "20:19:50 |     dict_starttoken: __start__\n",
            "20:19:50 |     dict_textfields: text,labels\n",
            "20:19:50 |     dict_tokenizer: bpe\n",
            "20:19:50 |     dict_unktoken: __unk__\n",
            "20:19:50 |     display_examples: False\n",
            "20:19:50 |     download_path: None\n",
            "20:19:50 |     dropout: 0.0\n",
            "20:19:50 |     dynamic_batching: full\n",
            "20:19:50 |     embedding_projection: random\n",
            "20:19:50 |     embedding_size: 512\n",
            "20:19:50 |     embedding_type: random\n",
            "20:19:50 |     embeddings_scale: True\n",
            "20:19:50 |     eval_batchsize: None\n",
            "20:19:50 |     eval_dynamic_batching: None\n",
            "20:19:50 |     evaltask: None\n",
            "20:19:50 |     ffn_size: 2048\n",
            "20:19:50 |     force_fp16_tokens: False\n",
            "20:19:50 |     fp16: True\n",
            "20:19:50 |     fp16_impl: mem_efficient\n",
            "20:19:50 |     gpu: -1\n",
            "20:19:50 |     gradient_clip: 0.1\n",
            "20:19:50 |     hide_labels: False\n",
            "20:19:50 |     history_add_global_end_token: None\n",
            "20:19:50 |     history_reversed: False\n",
            "20:19:50 |     history_size: -1\n",
            "20:19:50 |     image_cropsize: 224\n",
            "20:19:50 |     image_mode: raw\n",
            "20:19:50 |     image_size: 256\n",
            "20:19:50 |     inference: greedy\n",
            "20:19:50 |     init_model: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "20:19:50 |     init_opt: None\n",
            "20:19:50 |     interactive_mode: False\n",
            "20:19:50 |     invsqrt_lr_decay_gamma: -1\n",
            "20:19:50 |     jsonfile_datapath: /content/drive/MyDrive/chatbot_data/data\n",
            "20:19:50 |     jsonfile_datatype_extension: True\n",
            "20:19:50 |     label_truncate: 128\n",
            "20:19:50 |     label_turns: both\n",
            "20:19:50 |     learn_positional_embeddings: True\n",
            "20:19:50 |     learningrate: 1e-05\n",
            "20:19:50 |     load_from_checkpoint: True\n",
            "20:19:50 |     log_every_n_secs: 10\n",
            "20:19:50 |     loglevel: info\n",
            "20:19:50 |     lr_scheduler: reduceonplateau\n",
            "20:19:50 |     lr_scheduler_decay: 0.5\n",
            "20:19:50 |     lr_scheduler_patience: 3\n",
            "20:19:50 |     max_lr_steps: -1\n",
            "20:19:50 |     max_train_time: -1\n",
            "20:19:50 |     metrics: default\n",
            "20:19:50 |     model: transformer/generator\n",
            "20:19:50 |     model_file: /content/drive/MyDrive/chatbot_model/model\n",
            "20:19:50 |     model_parallel: False\n",
            "20:19:50 |     momentum: 0\n",
            "20:19:50 |     multitask_weights: [1]\n",
            "20:19:50 |     n_decoder_layers: -1\n",
            "20:19:50 |     n_encoder_layers: -1\n",
            "20:19:50 |     n_heads: 16\n",
            "20:19:50 |     n_layers: 8\n",
            "20:19:50 |     n_positions: 512\n",
            "20:19:50 |     n_segments: 0\n",
            "20:19:50 |     nesterov: True\n",
            "20:19:50 |     no_cuda: False\n",
            "20:19:50 |     num_epochs: -1\n",
            "20:19:50 |     nus: (0.7,)\n",
            "20:19:50 |     optimizer: mem_eff_adam\n",
            "20:19:50 |     output_scaling: 1.0\n",
            "20:19:50 |     override: \"{'task': 'jsonfile', 'jsonfile_datapath': '/content/drive/MyDrive/chatbot_data/data', 'jsonfile_datatype_extension': True, 'model': 'transformer/generator', 'model_file': '/content/drive/MyDrive/chatbot_model/model', 'init_model': 'zoo:tutorial_transformer_generator/model', 'n_heads': 16, 'n_layers': 8, 'n_positions': 512, 'text_truncate': 512, 'label_truncate': 128, 'ffn_size': 2048, 'embedding_size': 512, 'activation': 'gelu', 'variant': 'xlm', 'dict_lower': True, 'dict_tokenizer': 'bpe', 'dict_file': '/usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model.dict', 'learn_positional_embeddings': True, 'learningrate': 1e-05, 'optimizer': 'adam', 'warmup_updates': 5000, 'validation_metric': 'ppl', 'validation_every_n_secs': 3600.0, 'save_every_n_secs': 60.0, 'batchsize': 12, 'fp16': True, 'fp16_impl': 'mem_efficient', 'skip_generation': True, 'dynamic_batching': 'full', 'label_turns': 'both'}\"\n",
            "20:19:50 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "20:19:50 |     person_tokens: False\n",
            "20:19:50 |     rank_candidates: False\n",
            "20:19:50 |     relu_dropout: 0.0\n",
            "20:19:50 |     save_after_valid: False\n",
            "20:19:50 |     save_every_n_secs: 60.0\n",
            "20:19:50 |     share_word_embeddings: True\n",
            "20:19:50 |     short_final_eval: False\n",
            "20:19:50 |     skip_generation: True\n",
            "20:19:50 |     special_tok_lst: None\n",
            "20:19:50 |     split_lines: False\n",
            "20:19:50 |     starttime: Feb27_20-19\n",
            "20:19:50 |     task: jsonfile\n",
            "20:19:50 |     temperature: 1.0\n",
            "20:19:50 |     tensorboard_log: False\n",
            "20:19:50 |     tensorboard_logdir: None\n",
            "20:19:50 |     text_truncate: 512\n",
            "20:19:50 |     topk: 10\n",
            "20:19:50 |     topp: 0.9\n",
            "20:19:50 |     truncate: -1\n",
            "20:19:50 |     update_freq: 1\n",
            "20:19:50 |     use_reply: label\n",
            "20:19:50 |     validation_cutoff: 1.0\n",
            "20:19:50 |     validation_every_n_epochs: -1\n",
            "20:19:50 |     validation_every_n_secs: 3600.0\n",
            "20:19:50 |     validation_max_exs: -1\n",
            "20:19:50 |     validation_metric: ppl\n",
            "20:19:50 |     validation_metric_mode: None\n",
            "20:19:50 |     validation_patience: 10\n",
            "20:19:50 |     validation_share_agent: False\n",
            "20:19:50 |     variant: xlm\n",
            "20:19:50 |     verbose: False\n",
            "20:19:50 |     warmup_rate: 0.0001\n",
            "20:19:50 |     warmup_updates: 5000\n",
            "20:19:50 |     weight_decay: None\n",
            "20:19:50 | creating task(s): jsonfile\n",
            "20:19:50 | [loading data from json file into task:/content/drive/MyDrive/chatbot_data/data_train.jsonl]\n",
            "20:19:53 | \u001b[31mMetadata does not exist. Please double check your datapath.\u001b[0m\n",
            "20:19:57 | training...\n",
            "20:19:58 | Overflow: setting loss scale to 65536.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/parlai/utils/fp16.py:487: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:19:59 | Overflow: setting loss scale to 32768.0\n",
            "20:20:00 | Overflow: setting loss scale to 16384.0\n",
            "20:20:08 | time:10s total_exs:6492 epochs:0.01\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9062  4241 13491 644.1 6492             19968  11.84    .4624 4.599 6.499e-08 882.7  2808 99.34      .2894   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                     32 5123 16300 3.182\n",
            "\n",
            "20:20:18 | time:20s total_exs:10012 epochs:0.01\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5769 20180 351.8 3520             16384  13.29    .3837 4.513 1.35e-07 456.3  1596 91.22      .3039   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                     67 6226 21776 3.498\n",
            "\n",
            "20:20:28 | time:31s total_exs:12752 epochs:0.02\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6114 21445   267 2740             16384  13.83    .3445 4.465 2.07e-07   354  1242 86.92      .3080   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    103 6468 22687 3.508\n",
            "\n",
            "20:20:38 | time:41s total_exs:15164 epochs:0.02\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6284 21982   241 2412             16384   14.1    .3064 4.471 2.77e-07 326.7  1143 87.43      .3138   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    138 6611 23124 3.498\n",
            "\n",
            "20:20:48 | time:51s total_exs:17168 epochs:0.02\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6323 22043 199.6 2004             16384  14.91    .3488 4.377 3.47e-07 268.8   937 79.58      .3200   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    173 6592 22980 3.486\n",
            "\n",
            "20:20:57 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n",
            "20:20:57 | Saving dictionary to /content/drive/MyDrive/chatbot_model/model.checkpoint.dict\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:21:04 | time:66s total_exs:18844 epochs:0.02\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6235 13188 107.4 1676             16384   15.6    .3056 4.432 4.13e-07 239.2   506 84.07      .3169   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    206 6474 13694 2.115\n",
            "\n",
            "20:21:14 | time:77s total_exs:20412 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6355 19918 153.6 1568             16384  15.35    .3340 4.362 4.77e-07 221.3 693.6 78.38      .3246   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    238 6576 20611 3.135\n",
            "\n",
            "20:21:24 | time:87s total_exs:21668 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6491 18168 125.6 1256             16384  16.21    .3371 4.489 5.329e-07 211.7 592.6 89.06      .3009   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    266 6702 18761 2.799\n",
            "\n",
            "20:21:34 | time:97s total_exs:23196 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6458 21625 150.5 1528             16384  15.52    .3108 4.514 6.009e-07 208.2   697 91.3      .3018   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    300 6666 22322 3.349\n",
            "\n",
            "20:21:44 | time:107s total_exs:24652 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6268 21284 145.4 1456             16384  16.07    .2972 4.526 6.689e-07 190.7 647.6 92.38      .3061   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    334 6459 21932 3.396\n",
            "\n",
            "20:21:54 | time:117s total_exs:25944 epochs:0.03\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6426 20997 127.9 1292             16384   17.1    .3237  4.61 7.349e-07 188.3 615.4 100.4      .2951   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    367 6614 21613 3.268\n",
            "\n",
            "20:21:56 | Overflow: setting loss scale to 16384.0\n",
            "20:22:03 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:22:10 | time:133s total_exs:27116 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9677  6353 12380 73.67 1172             16384  16.07    .3294  4.37 7.969e-07 165.3 322.2 79.06      .3192   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    398 6518 12702 1.949\n",
            "\n",
            "20:22:21 | time:143s total_exs:28200 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6614 19706 104.2 1084             16384  17.15    .3392 4.415 8.589e-07 157.8 470.1 82.66      .3175   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    429 6772 20176 2.98\n",
            "\n",
            "20:22:22 | Overflow: setting loss scale to 16384.0\n",
            "20:22:31 | time:154s total_exs:28892 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9500  6252 12201 67.52  692             16384   16.1    .3302 4.429 8.989e-07 158.4 309.2 83.85      .3206   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    449 6410 12511 1.952\n",
            "\n",
            "20:22:41 | time:164s total_exs:29692 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6868 17394 77.92  800             16384  18.08    .3302 4.435 9.509e-07 139.4 353.1 84.32      .3142   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    475 7008 17747 2.533\n",
            "\n",
            "20:22:52 | time:174s total_exs:30656 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6649 19308 93.31  964             16384  17.35    .3251 4.392 1.011e-06 151.8 440.8 80.8      .3160   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    505 6801 19749 2.904\n",
            "\n",
            "20:23:02 | time:185s total_exs:31720 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6392 19997   104 1064             16384  17.15    .3237 4.345 1.075e-06 143.1 447.7 77.11      .3227   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    537 6535 20444 3.129\n",
            "\n",
            "20:23:10 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:23:17 | time:199s total_exs:32416 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6793 12117 47.75  696             16384  17.71    .3318 4.515 1.127e-06 140.8 251.2 91.37      .3018   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    563 6934 12369 1.784\n",
            "\n",
            "20:23:27 | time:209s total_exs:33252 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6769 20001 82.34  836             16384  18.78    .3281 4.303 1.187e-06 123.7 365.5 73.91      .3288   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    593 6892 20367 2.956\n",
            "\n",
            "20:23:37 | time:220s total_exs:33780 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6829 12388 50.39  528             16384  19.22    .3528  4.41 1.225e-06 130.9 237.5 82.31      .3075   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    612 6960 12626 1.814\n",
            "\n",
            "20:23:48 | time:230s total_exs:34496 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6720 16298 69.46  716             16384   17.8    .3373 4.405 1.275e-06 139.5 338.4 81.87      .3048   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    637 6859 16636 2.426\n",
            "\n",
            "20:23:58 | time:240s total_exs:35296 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6670 19318 79.89  800             16384  18.02    .3318 4.358 1.333e-06 130.1 376.9 78.09      .3092   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    666 6801 19695 2.896\n",
            "\n",
            "20:24:06 | Overflow: setting loss scale to 16384.0\n",
            "20:24:08 | time:251s total_exs:36204 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9677  6244 18915 88.73  908             16384  17.37    .3556 4.405 1.395e-06 135.8 411.3 81.85      .3070   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    697 6380 19326 3.029\n",
            "\n",
            "20:24:16 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:24:23 | time:265s total_exs:36964 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6483 11980 52.02  760             16384  17.95    .3332 4.299 1.449e-06 132.6 245.1 73.61      .3203   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    724 6615 12225 1.848\n",
            "\n",
            "20:24:33 | time:276s total_exs:37720 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6595 18896  72.2  756             16384  18.23    .3681 4.328 1.509e-06 121.1 346.9 75.82      .3158   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    754 6716 19243 2.866\n",
            "\n",
            "20:24:43 | time:286s total_exs:38304 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6413 13648 56.49  584             16384  17.72    .3463 4.271 1.553e-06 127.5 271.3 71.6      .3283   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    776 6541 13919 2.128\n",
            "\n",
            "20:24:54 | time:296s total_exs:38920 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6751 15835  60.2  616             16384  17.88    .3556  4.33 1.601e-06 127.1 298.1 75.95      .3269   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    800 6878 16133 2.346\n",
            "\n",
            "20:25:04 | time:307s total_exs:39600 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6490 17226 66.84  680             16384  18.24    .3463  4.19 1.655e-06 110.9 294.3 66.02      .3497   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    827 6601 17520 2.654\n",
            "\n",
            "20:25:13 | Overflow: setting loss scale to 16384.0\n",
            "20:25:14 | time:317s total_exs:40424 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9677  6348 19234 80.53  824             16384  17.79    .3556  4.25 1.717e-06 116.9 354.2 70.08      .3306   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    858 6465 19588 3.03\n",
            "\n",
            "20:25:22 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:25:29 | time:331s total_exs:41032 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6774 12162 41.99  608             16384  18.19    .3556 4.206 1.769e-06 113.7 204.2 67.1      .3257   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    884 6888 12367 1.796\n",
            "\n",
            "20:25:39 | time:341s total_exs:41728 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6369 18161 68.43  696             16384  18.37    .3556 4.407 1.827e-06 120.6 343.8 82.06      .3088   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    913 6490 18505 2.852\n",
            "\n",
            "20:25:49 | time:352s total_exs:42140 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6564 11767 41.03  412             16384  17.88    .3554 4.121 1.863e-06 106.1 190.2 61.65      .3429   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    931 6670 11958 1.793\n",
            "\n",
            "20:25:59 | time:362s total_exs:42680 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6614 14522 53.89  540             16384  18.54    .3683 4.179 1.907e-06 115.8 254.3 65.3      .3454   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    953 6729 14776 2.196\n",
            "\n",
            "20:26:09 | time:372s total_exs:43368 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6481 16588 67.73  688             16384  17.97    .3716 4.161 1.959e-06 121.3 310.5 64.14      .3370   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    979 6602 16899 2.56\n",
            "\n",
            "20:26:19 | time:382s total_exs:44060 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6568 19206 67.45  692             16384  18.17    .3556 4.331 2.019e-06 108.2 316.3 76.05      .3239   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1009 6676 19523 2.924\n",
            "\n",
            "20:26:28 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:26:35 | time:397s total_exs:44700 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6467 11908 42.09  640             16384  18.21    .3556 4.186 2.075e-06 106.6 196.2 65.74      .3412   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1037 6573 12104 1.842\n",
            "\n",
            "20:26:42 | Overflow: setting loss scale to 16384.0\n",
            "20:26:45 | time:407s total_exs:45276 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9643  6813 18914 57.11  576             16384  17.79    .3683 4.302 2.131e-06 101.3 281.3 73.84      .3292   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1065 6914 19196 2.776\n",
            "\n",
            "20:26:55 | time:417s total_exs:45732 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6244 11849 45.54  456             16384  18.61    .3903  4.07 2.169e-06 110.4 209.4 58.58      .3529   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1084 6354 12058 1.898\n",
            "\n",
            "20:27:05 | time:428s total_exs:46264 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6535 15693 53.23  532             16384  19.13    .3857 4.093 2.217e-06 102.4 245.9 59.9      .3405   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1108 6638 15939 2.402\n",
            "\n",
            "20:27:15 | time:438s total_exs:46868 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6203 15246 59.38  604             16384  18.35    .3974 4.321 2.267e-06 117.5 288.8 75.26      .3298   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1133 6321 15534 2.458\n",
            "\n",
            "20:27:25 | time:448s total_exs:47556 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6446 18658 68.67  688             16384  18.81    .4125  4.21 2.325e-06 101.2   293 67.36      .3399   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1162 6547 18951 2.895\n",
            "\n",
            "20:27:35 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:27:41 | time:464s total_exs:48132 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6638 11719 36.32  576             16384  18.99    .3716 4.204 2.381e-06 104.9 185.2 66.95      .3373   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1190 6743 11904 1.766\n",
            "\n",
            "20:27:51 | time:474s total_exs:48696 epochs:0.06\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6422 17481 54.83  564             16384  19.96    .3998 4.154 2.437e-06 97.11 264.3 63.71      .3465   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1218 6519 17745 2.722\n",
            "\n",
            "20:28:02 | time:484s total_exs:49072 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6564 11576 36.84  376             16384  19.43    .3557 4.209 2.473e-06    97 171.1 67.32      .3471   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1236 6661 11747 1.764\n",
            "\n",
            "20:28:12 | time:495s total_exs:49592 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6602 14808 50.71  520             16384  19.76    .3998 4.049 2.519e-06 94.65 212.3 57.33      .3652   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1259 6697 15020 2.243\n",
            "\n",
            "20:28:22 | time:505s total_exs:50132 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6600 17559  53.2  540             16384  19.26    .4033 4.249 2.573e-06 96.11 255.7 70.06      .3118   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   1286 6696 17815 2.66\n",
            "\n",
            "20:28:32 | time:515s total_exs:50612 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6596 18788 47.15  480             16384   19.4    .4096 4.069 2.631e-06 84.86 241.7 58.48      .3409   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1315 6681 19030 2.849\n",
            "\n",
            "20:28:41 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:28:47 | time:530s total_exs:51060 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6616 11033 29.88  448             16384  19.68    .3717 4.238 2.681e-06 95.08 158.6 69.29      .3244   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1340 6711 11191 1.668\n",
            "\n",
            "20:28:58 | time:540s total_exs:51588 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6486 17758 51.63  528             16384  19.34    .3810 4.186 2.737e-06 95.04 260.2 65.78      .3209   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1368 6581 18018 2.738\n",
            "\n",
            "20:29:08 | time:550s total_exs:51944 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6392 11910 34.91  356             16384  19.97    .3904 4.264 2.775e-06 97.05 180.8 71.08      .3021   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1387 6490 12091 1.863\n",
            "\n",
            "20:29:12 | Overflow: setting loss scale to 16384.0\n",
            "20:29:13 | Overflow: setting loss scale to 16384.0\n",
            "20:29:18 | time:561s total_exs:52424 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9130  6491 14391 46.26  480             16384  17.07    .3904 4.031 2.821e-06 96.65 214.3 56.32      .3491   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1410 6588 14605 2.217\n",
            "\n",
            "20:29:28 | time:571s total_exs:52996 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6478 16594 56.35  572             16384  18.64    .4096 4.201 2.873e-06 100.7   258 66.72      .3307   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1436 6579 16852 2.562\n",
            "\n",
            "20:29:39 | time:581s total_exs:53548 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6481 17892 54.42  552             16384  20.11    .3999 4.247 2.929e-06 99.61   275 69.91      .3223   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1464 6580 18167 2.761\n",
            "\n",
            "20:29:47 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:29:54 | time:596s total_exs:54172 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6095  9718 41.45  624             16384  17.04    .4096 4.047 2.977e-06 118.1 188.3 57.24      .3591   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                   1488 6214 9906 1.594\n",
            "\n",
            "20:30:04 | time:607s total_exs:54744 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6492 17429 54.84  572             16384  18.99    .4093 4.141 3.033e-06 95.29 255.8 62.88      .3403   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1516 6588 17685 2.685\n",
            "\n",
            "20:30:14 | time:617s total_exs:55232 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6870 15952 47.21  488             16384     20    .4093 4.223 3.081e-06 96.83 224.9 68.22      .3257   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1540 6966 16177 2.322\n",
            "\n",
            "20:30:25 | time:627s total_exs:55620 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6771 14529 37.84  388             16384  19.24    .4031 4.069 3.125e-06 97.05 208.2 58.5      .3424   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1562 6868 14738 2.146\n",
            "\n",
            "20:30:35 | time:638s total_exs:56044 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6412 15199 41.87  424             16384  20.16    .4093  4.16 3.173e-06 85.12 201.8 64.08      .3348   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1586 6497 15401 2.371\n",
            "\n",
            "20:30:45 | time:648s total_exs:56500 epochs:0.07\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6501 15777 44.27  456             16384  20.16    .4093 4.231 3.223e-06 90.76 220.3 68.77      .3279   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1611 6592 15998 2.427\n",
            "\n",
            "20:30:54 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:31:00 | time:662s total_exs:56924 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6360 10561 29.33  424             16384  20.33    .4093 4.107 3.271e-06 82.92 137.7 60.77      .3452   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1635 6443 10699 1.661\n",
            "\n",
            "20:31:10 | time:672s total_exs:57432 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6189 17230 50.51  508             16384  19.98    .4093 4.165 3.327e-06 80.39 223.8 64.39      .3372   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1663 6269 17454 2.784\n",
            "\n",
            "20:31:20 | time:683s total_exs:57696 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6181 10364 26.04  264             16384  21.13    .4093 4.153 3.361e-06 76.24 127.8 63.64      .3364   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1680 6257 10492 1.677\n",
            "\n",
            "20:31:23 | Overflow: setting loss scale to 16384.0\n",
            "20:31:30 | time:693s total_exs:58072 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9565  6435 14274 36.26  376             16384  19.82    .4094 4.081 3.407e-06 77.57   172 59.23      .3447   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1703 6513 14446 2.218\n",
            "\n",
            "20:31:41 | time:703s total_exs:58412 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6350 14458 33.65  340             16384  21.73    .3808 4.154 3.453e-06 73.04 166.3 63.67      .3238   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1726 6423 14624 2.277\n",
            "\n",
            "20:31:51 | time:713s total_exs:58972 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6156 17463 54.78  560             16384  19.91    .4093 4.151 3.511e-06 91.41 259.3 63.52      .3353   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1755 6247 17722 2.837\n",
            "\n",
            "20:31:59 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:32:06 | time:728s total_exs:59416 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6414 10700 29.63  444             16384   20.3    .3996 3.968 3.561e-06 84.08 140.3 52.9      .3544   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1780 6499 10841 1.668\n",
            "\n",
            "20:32:16 | time:739s total_exs:59916 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6229 16728 47.95  500             16384  19.96    .4094 4.184 3.617e-06 88.21 236.9 65.64      .3332   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1808 6317 16965 2.686\n",
            "\n",
            "20:32:27 | time:749s total_exs:60224 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6372 12225 29.54  308             16384  19.94    .4094 4.033 3.657e-06 77.75 149.2 56.42      .3370   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1828 6450 12374 1.919\n",
            "\n",
            "20:32:37 | time:759s total_exs:60640 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6021 13203 41.46  416             16384  20.21    .3996 4.173 3.701e-06 92.64 203.1 64.92      .3248   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1850 6113 13406 2.193\n",
            "\n",
            "20:32:47 | time:770s total_exs:61056 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6189 15780 40.79  416             16384  19.59    .3996 4.054 3.753e-06 80.73 205.8 57.64      .3244   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   1876 6270 15986 2.55\n",
            "\n",
            "20:32:57 | time:780s total_exs:61708 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5926 16964 64.36  652             16384  17.83    .4142  4.15 3.811e-06 110.4   316 63.44      .3349   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1905 6036 17280 2.863\n",
            "\n",
            "20:33:06 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:33:12 | time:795s total_exs:62184 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6349 10728 32.17  476             16384  19.43    .4094 4.018 3.861e-06 90.52   153 55.57      .3336   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   1930 6439 10881 1.69\n",
            "\n",
            "20:33:22 | time:805s total_exs:62744 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5953 16326 54.85  560             16384  19.89    .4094 3.999 3.917e-06 87.21 239.2 54.52      .3546   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1958 6040 16565 2.743\n",
            "\n",
            "20:33:32 | time:815s total_exs:63124 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6139 11603  37.8  380             16384  18.67    .3855 4.165 3.955e-06 96.26 181.9 64.42      .3264   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   1977 6236 11785 1.89\n",
            "\n",
            "20:33:43 | time:825s total_exs:63444 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6188 13936 31.33  320             16384  21.04    .3715 4.155 4.001e-06 63.39 142.8 63.76      .3402   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2000 6252 14079 2.252\n",
            "\n",
            "20:33:51 | Overflow: setting loss scale to 16384.0\n",
            "20:33:53 | time:836s total_exs:63872 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9615  6134 15649 41.99  428             16384  19.17    .3762 4.059 4.053e-06 88.69 226.3 57.94      .3322   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2026 6223 15875 2.551\n",
            "\n",
            "20:34:03 | time:846s total_exs:64400 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6423 18315 51.91  528             16384     20    .4031 3.977 4.111e-06 85.97 245.1 53.36      .3610   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2055 6509 18560 2.852\n",
            "\n",
            "20:34:09 | Overflow: setting loss scale to 16384.0\n",
            "20:34:12 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:34:18 | time:861s total_exs:64852 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9615  6358 11093 30.33  452             16384  18.86    .4094 4.048 4.163e-06 80.77 140.9 57.26      .3448   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2081 6439 11234 1.745\n",
            "\n",
            "20:34:28 | time:871s total_exs:65316 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6127 17130 46.33  464             16384  19.74    .4094 3.949 4.219e-06 76.14 212.9 51.9      .3518   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2109 6203 17342 2.796\n",
            "\n",
            "20:34:39 | time:881s total_exs:65608 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6374 11520 27.77  292             16384  20.39    .3997 4.016 4.257e-06 70.11 126.7 55.5      .3574   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2128 6445 11646 1.807\n",
            "\n",
            "20:34:49 | time:891s total_exs:65940 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6472 14061 32.79  332             16384  20.14    .4094 3.972 4.301e-06 72.09 156.6 53.08      .3455   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2150 6544 14218 2.173\n",
            "\n",
            "20:34:59 | time:902s total_exs:66396 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5878 14924 44.53  456             16384  19.52    .4124 4.058 4.353e-06 85.54 217.2 57.88      .3368   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2176 5963 15141 2.539\n",
            "\n",
            "20:35:01 | Overflow: setting loss scale to 16384.0\n",
            "20:35:09 | time:912s total_exs:66768 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9643  6192 17285 37.08  372             16384  21.11    .3997 3.952 4.409e-06 60.21 168.1 52.02      .3606   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2204 6253 17453 2.792\n",
            "\n",
            "20:35:18 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:35:24 | time:927s total_exs:67148 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6112 10421 24.92  380             16384  20.64    .3855 4.134 4.461e-06 78.12 133.2 62.45      .3333   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2230 6190 10554 1.705\n",
            "\n",
            "20:35:35 | time:937s total_exs:67556 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6022 16035 40.23  408             16384  19.98    .3681 4.065 4.515e-06 78.19 208.2 58.25      .3373   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2257 6100 16243 2.663\n",
            "\n",
            "20:35:45 | time:947s total_exs:67964 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5911 14114 40.59  408             16384  19.82    .4124 3.973 4.563e-06 81.83 195.4 53.13      .3483   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2281 5992 14309 2.388\n",
            "\n",
            "20:35:55 | time:958s total_exs:68400 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5861 14152 42.11  436             16384   21.1    .4094 3.962 4.613e-06  80.2 193.7 52.57      .3426   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2306 5941 14346 2.415\n",
            "\n",
            "20:36:05 | time:968s total_exs:68808 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6010 14801 40.19  408             16384  19.46    .4031 3.882 4.663e-06 78.68 193.8 48.52      .3559   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2331 6088 14995 2.463\n",
            "\n",
            "20:36:15 | time:978s total_exs:69132 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6164 15144 31.84  324             16384  21.52    .3856 3.752 4.713e-06 60.32 148.2 42.6      .3687   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2356 6224 15292 2.457\n",
            "\n",
            "20:36:24 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:36:30 | time:993s total_exs:69584 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6068 10287 30.65  452             16384  19.44    .3996 3.819 4.763e-06 90.04 152.7 45.57      .3612   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2381 6158 10440 1.695\n",
            "\n",
            "20:36:40 | time:1003s total_exs:70124 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5830 16509 52.73  540             16384  18.69    .3682 3.995 4.821e-06 89.79 254.3 54.35      .3475   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2410 5920 16764 2.832\n",
            "\n",
            "20:36:51 | time:1013s total_exs:70340 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6064 10787 21.34  216             16384  21.64    .3682 3.994 4.857e-06 65.61 116.7 54.28      .3277   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2428 6130 10903 1.779\n",
            "\n",
            "20:36:57 | Overflow: setting loss scale to 16384.0\n",
            "20:37:01 | time:1023s total_exs:70652 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9545  6067 13121 30.66  312             16384  20.08    .4032 3.882 4.901e-06 67.41 145.8 48.51      .3479   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2450 6134 13266 2.163\n",
            "\n",
            "20:37:05 | Overflow: setting loss scale to 16384.0\n",
            "20:37:11 | time:1034s total_exs:71020 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9615  6205 15746 35.92  368             16384   19.7    .3762 4.033 4.953e-06 70.85 179.8 56.45      .3415   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2476 6276 15926 2.538\n",
            "\n",
            "20:37:21 | Overflow: setting loss scale to 16384.0\n",
            "20:37:21 | time:1044s total_exs:71492 epochs:0.09\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9655  6072 17382 46.59  472             16384  19.19    .3682 4.079 5.01e-06 74.28 212.6 59.09      .3370   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2505 6146 17594 2.863\n",
            "\n",
            "20:37:30 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:37:36 | time:1059s total_exs:71928 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5767 10284  28.8  436             16384  20.62    .4094 4.021 5.064e-06 74.85 133.5 55.75      .3503   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2532 5842 10417 1.783\n",
            "\n",
            "20:37:46 | time:1069s total_exs:72320 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6122 17120 39.15  392             16384  21.28    .3715 3.967 5.12e-06 67.29 188.2 52.81      .3386   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2560 6189 17308 2.797\n",
            "\n",
            "20:37:57 | time:1079s total_exs:72604 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6253 12429 28.23  284             16384  20.44    .3944 3.987 5.16e-06    63 125.2 53.91      .3508   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2580 6316 12554 1.988\n",
            "\n",
            "20:38:07 | time:1089s total_exs:72908 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6174 13935 29.83  304             16384  21.23    .3682  3.98 5.206e-06 65.57   148 53.54      .3462   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2603 6240 14083 2.257\n",
            "\n",
            "20:38:17 | time:1099s total_exs:73304 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6184 15467 39.62  396             16384  20.01    .4058 4.052 5.256e-06 75.52 188.9 57.49      .3528   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2628 6260 15656 2.501\n",
            "\n",
            "20:38:23 | Overflow: setting loss scale to 16384.0\n",
            "20:38:27 | time:1110s total_exs:73772 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9667  5988 17400 45.33  468             16384  19.52    .4143 4.008 5.316e-06  76.1 221.1 55.02      .3425   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2658 6064 17621 2.906\n",
            "\n",
            "20:38:36 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:38:43 | time:1125s total_exs:74168 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6067 10589  25.6  396             16384  20.39    .3682 3.956 5.37e-06 73.93   129 52.22      .3427   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2685 6141 10718 1.745\n",
            "\n",
            "20:38:47 | Overflow: setting loss scale to 16384.0\n",
            "20:38:51 | Overflow: setting loss scale to 16384.0\n",
            "20:38:53 | time:1136s total_exs:74556 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9286  6006 16424 37.89  388             16384  19.77    .3682 3.871 5.426e-06 57.32 156.7 47.98      .3688   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2713 6064 16581 2.735\n",
            "\n",
            "20:38:56 | Overflow: setting loss scale to 16384.0\n",
            "20:39:03 | time:1146s total_exs:74836 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9500  6156 12220 27.79  280             16384  20.63    .4094 3.951 5.466e-06  64.2 127.4 52.01      .3497   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2733 6220 12347 1.985\n",
            "\n",
            "20:39:10 | Overflow: setting loss scale to 16384.0\n",
            "20:39:13 | time:1156s total_exs:75196 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "   .9565  5969 13632 35.74  360             16384  19.25    .3763 4.048 5.512e-06    85 194.1 57.3      .3330   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2756 6054 13826 2.284\n",
            "\n",
            "20:39:24 | time:1166s total_exs:75500 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6231 13839 29.35  304             16384  21.35    .4032 3.911 5.558e-06 62.04 137.8 49.97      .3476   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2779 6294 13977 2.221\n",
            "\n",
            "20:39:34 | time:1176s total_exs:75884 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6277 17506 38.25  384             16384  21.13    .4124 4.049 5.614e-06 68.71 191.6 57.33      .3358   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2807 6345 17698 2.789\n",
            "\n",
            "20:39:39 | Overflow: setting loss scale to 16384.0\n",
            "20:39:43 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:39:49 | time:1191s total_exs:76288 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9630  5893 10580 26.86  404             16384  19.44    .3715 4.046 5.668e-06 76.48 137.3 57.14      .3356   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2834 5969 10718 1.796\n",
            "\n",
            "20:39:59 | time:1202s total_exs:76728 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5938 16158 42.76  440             16384  20.73    .3944 3.785 5.724e-06 68.21 185.6 44.03      .3759   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2862 6007 16344 2.721\n",
            "\n",
            "20:40:09 | time:1212s total_exs:76988 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6194 10470 25.85  260             16384   20.1    .3973 3.907 5.758e-06 73.24 123.8 49.75      .3574   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2879 6267 10594 1.691\n",
            "\n",
            "20:40:20 | time:1222s total_exs:77408 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5836 13430 40.27  420             16384  19.57    .3877 3.845 5.806e-06 74.83 172.2 46.74      .3736   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2903 5911 13602 2.301\n",
            "\n",
            "20:40:30 | time:1232s total_exs:77724 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6235 15452 31.32  316             16384  22.13    .3997 3.936 5.856e-06 56.36 139.7 51.21      .3563   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2928 6291 15591 2.478\n",
            "\n",
            "20:40:40 | time:1242s total_exs:78080 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6242 17297 35.23  356             16384  21.26    .4124 4.227 5.912e-06 69.86 193.6 68.54      .3134   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2956 6312 17490 2.771\n",
            "\n",
            "20:40:48 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:40:55 | time:1258s total_exs:78496 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6022 10376 27.57  416             16384  19.68    .3973 3.804 5.964e-06 73.69   127 44.88      .3638   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   2982 6096 10503 1.723\n",
            "\n",
            "20:41:05 | time:1268s total_exs:78912 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5946 15936 41.29  416             16384  20.58    .3682 3.588 6.018e-06 64.48 172.8 36.17      .3963   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   3009 6011 16109 2.68\n",
            "\n",
            "20:41:15 | time:1278s total_exs:79168 epochs:0.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6108 11601 25.59  256             16384  21.41    .3682 3.829 6.056e-06 63.74 121.1 46.02      .3576   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   3028 6171 11722  1.9\n",
            "\n",
            "20:41:25 | time:1288s total_exs:79484 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6072 13504 30.55  316             16384  21.67    .4124 3.972 6.102e-06 65.74 146.2 53.06      .3532   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3051 6138 13651 2.224\n",
            "\n",
            "20:41:36 | time:1298s total_exs:79856 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6016 15451 36.74  372             16384  19.58    .3682 3.941 6.154e-06 74.15 190.4 51.48      .3667   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3077 6091 15642 2.568\n",
            "\n",
            "20:41:46 | time:1309s total_exs:80276 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6219 17547 40.86  420             16384  20.23    .4032   3.7 6.212e-06 68.62 193.6 40.45      .3698   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3106 6288 17741 2.822\n",
            "\n",
            "20:41:55 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:42:01 | time:1324s total_exs:80636 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6146 10330 23.27  360             16384  20.38    .4124 3.978 6.264e-06  69.5 116.8 53.42      .3365   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3132 6216 10446 1.681\n",
            "\n",
            "20:42:12 | time:1334s total_exs:80980 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6210 16478 33.81  344             16384  21.09    .3944   3.7 6.318e-06 57.41 152.3 40.45      .3703   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3159 6267 16631 2.654\n",
            "\n",
            "20:42:22 | time:1344s total_exs:81232 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6281 11751 24.81  252             16384  20.69    .3944 3.769 6.356e-06 68.42   128 43.33      .3577   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3178 6349 11879 1.871\n",
            "\n",
            "20:42:32 | time:1355s total_exs:81600 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5842 14113 35.56  368             16384  20.73    .3689 3.929 6.406e-06 67.76 163.7 50.85      .3571   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3203 5910 14276 2.416\n",
            "\n",
            "20:42:42 | time:1365s total_exs:81952 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6023 14297 34.82  352             16384   19.9    .4032 3.994 6.454e-06    73 173.3 54.28      .3453   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3227 6096 14471 2.374\n",
            "\n",
            "20:42:52 | time:1375s total_exs:82376 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6019 17360 42.17  424             16384  20.51    .3763 4.012 6.512e-06 75.69 218.3 55.28      .3380   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3256 6095 17578 2.884\n",
            "\n",
            "20:43:01 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:43:08 | time:1390s total_exs:82736 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6244 10635 23.58  360             16384  20.49    .3682  3.77 6.564e-06    59 100.5 43.38      .3664   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3282 6303 10735 1.703\n",
            "\n",
            "20:43:18 | time:1400s total_exs:83156 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6000 16620 41.55  420             16384  20.04    .3716 3.858 6.62e-06 71.18 197.2 47.39      .3663   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   3310 6071 16817 2.77\n",
            "\n",
            "20:43:28 | time:1411s total_exs:83404 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6249 10857 23.94  248             16384  20.42    .3763  3.85 6.656e-06 66.61 115.7 46.99      .3411   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3328 6316 10973 1.737\n",
            "\n",
            "20:43:38 | time:1421s total_exs:83696 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss      lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6187 13406 28.76  292             16384  21.03    .3790 3.755 6.7e-06 58.77 127.3 42.75      .3735   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3350 6246 13533 2.167\n",
            "\n",
            "20:43:49 | time:1431s total_exs:84120 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5925 15434  40.9  424             16384  19.37    .3768 3.883 6.754e-06  78.3   204 48.59      .3401   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3377 6003 15638 2.605\n",
            "\n",
            "20:43:59 | time:1441s total_exs:84596 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5689 17009 47.43  476             16384     20    .3810 3.861 6.814e-06 71.37 213.4 47.53      .3671   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   3407 5761 17223 2.99\n",
            "\n",
            "20:44:08 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:44:14 | time:1457s total_exs:84976 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5985 10236    25  380             16384  20.63    .4125 3.746 6.866e-06 62.77 107.3 42.33      .3762   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   3433 6048 10343 1.71\n",
            "\n",
            "20:44:24 | time:1467s total_exs:85396 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6113 16791  41.2  420             16384  20.13    .3997 3.737 6.922e-06 66.07 181.5 41.96      .3784   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3461 6179 16972 2.749\n",
            "\n",
            "20:44:35 | time:1477s total_exs:85708 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5872 11960 30.26  312             16384  20.36    .3857 3.916 6.964e-06 74.19 151.1 50.22      .3293   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3482 5946 12111 2.037\n",
            "\n",
            "20:44:45 | time:1487s total_exs:85976 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6278 13191 26.81  268             16384  20.78    .3944 3.846 7.006e-06 65.29 137.2 46.8      .3487   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3503 6343 13328 2.102\n",
            "\n",
            "20:44:55 | time:1498s total_exs:86332 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5985 14514 34.53  356             16384  19.82    .4095  3.94 7.056e-06 72.48 175.8 51.41      .3383   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3528 6057 14690 2.425\n",
            "\n",
            "20:45:05 | time:1508s total_exs:86800 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  5785 16593 46.29  468             16384  19.71    .4125 3.896 7.114e-06 81.72 234.4 49.2      .3612   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3557 5867 16828 2.868\n",
            "\n",
            "20:45:14 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:45:20 | time:1523s total_exs:87172 epochs:0.12\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6095 10361 24.32  372             16384   20.9    .3683 3.787 7.166e-06 61.58 104.7 44.13      .3704   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   3583 6157 10465  1.7\n",
            "\n",
            "20:45:31 | time:1533s total_exs:87532 epochs:0.12\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6097 16565 34.93  360             16384  21.24    .3768 3.947 7.222e-06 58.75 159.6 51.76      .3471   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3611 6156 16725 2.717\n",
            "\n",
            "20:45:41 | time:1544s total_exs:87796 epochs:0.12\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6044 10848 26.32  264             16384  19.95    .3857 3.926 7.258e-06 77.44   139 50.69      .3465   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3629 6121 10987 1.795\n",
            "\n",
            "20:45:51 | time:1554s total_exs:88120 epochs:0.12\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6023 13226 32.33  324             16384  18.81    .3683 4.042 7.302e-06 83.05 182.3 56.95      .3240   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3651 6106 13408 2.196\n",
            "\n",
            "20:46:01 | time:1564s total_exs:88472 epochs:0.12\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6012 15545    35  352             16384  20.61    .3683 3.838 7.354e-06 61.73 159.6 46.43      .3445   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3677 6074 15705 2.586\n",
            "\n",
            "20:46:11 | time:1574s total_exs:88908 epochs:0.12\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5904 16840 42.88  436             16384   19.8    .3903 3.874 7.412e-06  68.9 196.5 48.12      .3564   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3706 5973 17036 2.852\n",
            "\n",
            "20:46:20 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:46:27 | time:1589s total_exs:89232 epochs:0.12\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6122 10777 21.12  324             16384  20.64    .3683 3.681 7.466e-06 52.96 93.23 39.7      .3720   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   3733 6175 10871 1.76\n",
            "\n",
            "20:46:37 | time:1600s total_exs:89580 epochs:0.12\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6178 14787 33.31  348             16384  20.98    .3683 3.777 7.516e-06 60.44 144.7 43.7      .3594   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3758 6239 14932 2.394\n",
            "\n",
            "20:46:47 | time:1610s total_exs:89940 epochs:0.12\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6060 16012 35.22  360             16384  19.99    .3998 3.892 7.57e-06 72.67   192 49.02      .3379   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3785 6133 16204 2.642\n",
            "\n",
            "20:46:58 | time:1620s total_exs:90312 epochs:0.12\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  5918 15115 36.54  372             16384  19.55    .3857 3.861 7.622e-06 75.46 192.7 47.5      .3364   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3811 5993 15307 2.554\n",
            "\n",
            "20:47:08 | time:1631s total_exs:90664 epochs:0.12\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  5918 13855 34.34  352             16384  19.82    .4059  3.93 7.67e-06 73.92 173.1 50.9      .3523   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3835 5992 14028 2.341\n",
            "\n",
            "20:47:18 | time:1641s total_exs:90972 epochs:0.12\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6220 15131 29.97  308             16384  21.44    .3998 3.701 7.72e-06 52.64 128.1 40.5      .3701   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3860 6272 15259 2.433\n",
            "\n",
            "20:47:26 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:47:33 | time:1656s total_exs:91268 epochs:0.12\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6065  9463 20.08  296             16384  20.95    .3857 3.774 7.766e-06 63.65 99.31 43.54      .3552   \n",
            "    total_train_updates  tpb  tps  ups  \n",
            "                   3883 6129 9562 1.56\n",
            "\n",
            "20:47:43 | time:1666s total_exs:91664 epochs:0.12\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6013 16045 39.14  396             16384  19.61    .4095 4.113 7.82e-06 76.19 203.3 61.14      .3296   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   3910 6089 16249 2.67\n",
            "\n",
            "20:47:53 | time:1676s total_exs:91988 epochs:0.12\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5990 12825 31.53  324             16384  21.15    .4033 3.752 7.864e-06 62.18 133.1 42.59      .3713   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3932 6052 12959 2.141\n",
            "\n",
            "20:48:04 | time:1686s total_exs:92292 epochs:0.12\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6147 13220 29.72  304             16384  21.12    .3845 3.817 7.908e-06 66.23 142.4 45.48      .3631   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3954 6213 13362 2.151\n",
            "\n",
            "20:48:14 | time:1697s total_exs:92572 epochs:0.12\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6128 13845  27.5  280             16384  21.09    .3945 3.755 7.954e-06 58.43   132 42.72      .3460   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   3977 6186 13977 2.259\n",
            "\n",
            "20:48:24 | time:1707s total_exs:92976 epochs:0.12\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5886 16537 39.14  404             16384  20.89    .3683 3.926 8.012e-06 73.97 207.8 50.72      .3375   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   4006 5960 16744 2.81\n",
            "\n",
            "20:48:33 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:48:39 | time:1722s total_exs:93288 epochs:0.12\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6120 10094 20.58  312             16384  20.39    .3878 3.941 8.062e-06 64.76 106.8 51.49      .3360   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   4031 6185 10201 1.65\n",
            "\n",
            "20:48:50 | time:1732s total_exs:93648 epochs:0.12\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6156 16347  35.4  360             16384  19.64    .4033  3.72 8.116e-06 65.78 174.7 41.26      .3654   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4058 6222 16522 2.656\n",
            "\n",
            "20:49:00 | time:1742s total_exs:93888 epochs:0.12\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6143 12198 23.83  240             16384  22.07    .3683  3.63 8.156e-06  51.8 102.9 37.7      .3774   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4078 6195 12301 1.986\n",
            "\n",
            "20:49:10 | time:1752s total_exs:94164 epochs:0.12\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss      lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6213 13610 27.48  276             16384  20.66    .4095 3.798 8.2e-06 56.68 124.2 44.61      .3512   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4100 6270 13734 2.191\n",
            "\n",
            "20:49:18 | Overflow: setting loss scale to 16384.0\n",
            "20:49:20 | time:1763s total_exs:94448 epochs:0.13\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9524  5961 12089 27.43  284             16384  19.39    .3683 3.855 8.242e-06    66 133.9 47.23      .3499   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4121 6027 12223 2.028\n",
            "\n",
            "20:49:30 | time:1773s total_exs:94876 epochs:0.13\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5883 16131 41.91  428             16384  19.92    .4125 3.873 8.298e-06 73.18 200.6 48.11      .3509   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4149 5956 16331 2.742\n",
            "\n",
            "20:49:39 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:49:46 | time:1788s total_exs:95204 epochs:0.13\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6122 10395 21.42  328             16384  20.83    .3764 3.838 8.35e-06 63.12 107.2 46.45      .3662   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4175 6185 10502 1.698\n",
            "\n",
            "20:49:56 | time:1799s total_exs:95552 epochs:0.13\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  5990 15395 33.12  348             16384  20.72    .3683 3.627 8.404e-06 54.11 139.1 37.6      .3970   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   4202 6044 15534 2.57\n",
            "\n",
            "20:50:06 | time:1809s total_exs:95812 epochs:0.13\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5978 11335 25.95  260             16384  21.42    .3683 3.733 8.442e-06 57.32 108.7 41.82      .3701   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4221 6035 11443 1.896\n",
            "\n",
            "20:50:17 | time:1820s total_exs:96168 epochs:0.13\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5863 13998    34  356             16384  20.64    .3683 3.772 8.492e-06 63.72 152.1 43.48      .3635   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4246 5927 14150 2.388\n",
            "\n",
            "20:50:27 | time:1830s total_exs:96456 epochs:0.13\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6083 14474 28.55  288             16384  22.36    .3683 3.917 8.54e-06 56.21 133.7 50.23      .3477   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   4270 6139 14608 2.38\n",
            "\n",
            "20:50:37 | time:1840s total_exs:96864 epochs:0.13\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5981 16032 40.51  408             16384  19.65    .3683 3.736 8.594e-06 71.59 191.9 41.95      .3735   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4297 6052 16224 2.681\n",
            "\n",
            "20:50:46 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:50:52 | time:1855s total_exs:97240 epochs:0.13\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5796  9851 25.56  376             16384  20.03    .3764 4.072 8.644e-06 79.08 134.4 58.65      .3313   \n",
            "    total_train_updates  tpb  tps  ups  \n",
            "                   4322 5875 9985  1.7\n",
            "\n",
            "20:50:55 | Overflow: setting loss scale to 16384.0\n",
            "20:51:02 | time:1865s total_exs:97644 epochs:0.13\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9655  5861 16465 39.13  404             16384  20.36    .3683 3.608 8.702e-06 58.72   165 36.91      .3964   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   4351 5920 16630 2.81\n",
            "\n",
            "20:51:13 | time:1875s total_exs:97900 epochs:0.13\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6077 11782 24.81  256             16384  20.88    .3683  3.84 8.742e-06 57.45 111.4 46.5      .3812   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4371 6135 11894 1.939\n",
            "\n",
            "20:51:23 | time:1885s total_exs:98240 epochs:0.13\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5895 13318  33.4  340             16384  19.95    .3683 3.709 8.788e-06 69.26 156.5 40.82      .3785   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4394 5964 13475 2.259\n",
            "\n",
            "20:51:33 | time:1896s total_exs:98664 epochs:0.13\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5873 14989 41.62  424             16384  19.61    .3683 3.934 8.84e-06 79.65 203.3 51.12      .3501   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4420 5953 15193 2.552\n",
            "\n",
            "20:51:43 | time:1906s total_exs:99040 epochs:0.13\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144 17322 36.55  376             16384  20.37    .3683  3.81 8.898e-06 64.07 180.6 45.14      .3445   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   4449 6208 17503 2.82\n",
            "\n",
            "20:51:52 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:51:58 | time:1921s total_exs:99420 epochs:0.13\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  5881 10059    26  380             16384  19.54    .3790 3.886 8.948e-06 79.48   136 48.7      .3563   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4474 5960 10195 1.711\n",
            "\n",
            "20:52:09 | time:1931s total_exs:99756 epochs:0.13\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6118 16268  31.9  336             16384  21.05    .3683 3.738 9.004e-06 55.18 146.7 42.03      .3663   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4502 6174 16415 2.659\n",
            "\n",
            "20:52:19 | time:1942s total_exs:100016 epochs:0.13\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6134 12526 25.28  260             16384  20.78    .3683 3.839 9.046e-06 58.57 119.6 46.47      .3691   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4523 6192 12646 2.042\n",
            "\n",
            "20:52:29 | time:1952s total_exs:100344 epochs:0.13\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5992 13529  32.2  328             16384   20.8    .3769 3.618 9.092e-06 61.35 138.5 37.25      .3870   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4546 6053 13667 2.258\n",
            "\n",
            "20:52:39 | time:1962s total_exs:100632 epochs:0.13\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5999 12910 28.17  288             16384  20.35    .3683 3.762 9.136e-06 68.18 146.7 43.03      .3767   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4568 6068 13057 2.152\n",
            "\n",
            "20:52:49 | time:1972s total_exs:101040 epochs:0.13\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6049 16840 40.56  408             16384   20.1    .4126 3.777 9.192e-06 67.43 187.7 43.69      .3554   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4596 6117 17028 2.784\n",
            "\n",
            "20:52:58 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:53:04 | time:1987s total_exs:101408 epochs:0.13\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5920  9923 24.67  368             16384  19.41    .3717 3.952 9.242e-06 77.04 129.1 52.02      .3437   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4621 5997 10052 1.676\n",
            "\n",
            "20:53:15 | time:1998s total_exs:101784 epochs:0.13\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6029 16133 35.93  376             16384  20.64    .3683 3.715 9.298e-06  64.5 172.6 41.04      .3726   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4649 6093 16306 2.676\n",
            "\n",
            "20:53:25 | time:2008s total_exs:102004 epochs:0.14\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6134 10775 21.47  220             16384  21.68    .3764 3.925 9.334e-06 68.11 119.6 50.66      .3377   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4667 6202 10895 1.757\n",
            "\n",
            "20:53:35 | time:2018s total_exs:102356 epochs:0.14\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5789 13101 34.63  352             16384   20.4    .3769 3.689 9.38e-06 68.22 154.4 39.99      .3665   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4690 5857 13256 2.263\n",
            "\n",
            "20:53:45 | time:2028s total_exs:102756 epochs:0.14\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5847 15106 39.75  400             16384  19.51    .3858 3.747 9.432e-06 74.35 192.1 42.38      .3570   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4716 5921 15298 2.584\n",
            "\n",
            "20:53:54 | Overflow: setting loss scale to 16384.0\n",
            "20:53:56 | time:2038s total_exs:103100 epochs:0.14\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9630  6253 16840 34.31  344             16384  19.58    .4033 3.597 9.486e-06 61.48 165.6 36.49      .3759   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4743 6314 17005 2.693\n",
            "\n",
            "20:54:04 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:54:11 | time:2053s total_exs:103436 epochs:0.14\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6172 10511 22.01  336             16384  20.79    .4126 3.694 9.538e-06    56 95.37 40.2      .3805   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4769 6228 10606 1.703\n",
            "\n",
            "20:54:21 | time:2064s total_exs:103796 epochs:0.14\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6179 17272 35.94  360             16384  20.58    .3683 3.794 9.594e-06 59.11 165.2 44.41      .3553   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4797 6238 17437 2.796\n",
            "\n",
            "20:54:31 | time:2074s total_exs:104028 epochs:0.14\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6136 11610  23.1  232             16384   21.1    .3684 3.623 9.632e-06 53.16 100.6 37.45      .3832   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4816 6189 11710 1.892\n",
            "\n",
            "20:54:41 | time:2084s total_exs:104336 epochs:0.14\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5998 14247 30.48  308             16384  20.33    .3684 3.743 9.68e-06 60.75 144.3 42.24      .3745   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4840 6059 14391 2.376\n",
            "\n",
            "20:54:51 | time:2094s total_exs:104648 epochs:0.14\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6022 13577 30.58  312             16384  21.35    .3684 3.929 9.726e-06  65.3 147.2 50.88      .3475   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4863 6087 13724 2.255\n",
            "\n",
            "20:55:01 | time:2104s total_exs:104996 epochs:0.14\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss       lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6019 16104 34.48  348             16384  20.66    .3684 3.791 9.78e-06 60.74 162.5 44.32      .3561   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4890 6080 16267 2.676\n",
            "\n",
            "20:55:11 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:55:17 | time:2120s total_exs:105340 epochs:0.14\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6023 10418 22.04  344             16384  20.32    .3764 3.783 9.834e-06 76.26 131.9 43.93      .3409   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   4917 6099 10550 1.73\n",
            "\n",
            "20:55:27 | time:2130s total_exs:105684 epochs:0.14\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6174 16561 34.17  344             16384   20.5    .4033 3.731 9.888e-06 66.96 179.6 41.73      .3496   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4944 6241 16741 2.683\n",
            "\n",
            "20:55:37 | time:2140s total_exs:105908 epochs:0.14\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6190 11015 22.14  224             16384   21.3    .3683 3.685 9.924e-06 58.28 103.7 39.83      .3746   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4962 6249 11118 1.779\n",
            "\n",
            "20:55:38 | Overflow: setting loss scale to 16384.0\n",
            "20:55:48 | time:2150s total_exs:106252 epochs:0.14\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9600  5839 14119 33.27  344             16384  19.74    .3684 3.845 9.974e-06 70.52 170.5 46.74      .3364   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   4987 5910 14290 2.418\n",
            "\n",
            "20:55:58 | time:2161s total_exs:106588 epochs:0.14\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6023 14888 33.22  336             16384  20.24    .3764 3.702 1e-05 68.16 168.5 40.54      .3862   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5012 6091 15057 2.472\n",
            "\n",
            "20:56:08 | time:2171s total_exs:106988 epochs:0.14\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5939 16291 39.18  400             16384  19.76    .3769   3.7 1e-05 65.96 180.9 40.46      .3795   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5040 6005 16472 2.743\n",
            "\n",
            "20:56:17 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:56:23 | time:2186s total_exs:107316 epochs:0.14\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6050 10263  21.4  328             16384  20.69    .3684  3.71 1e-05 59.65 101.2 40.86      .3630   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5066 6110 10364 1.696\n",
            "\n",
            "20:56:34 | time:2196s total_exs:107672 epochs:0.14\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6034 16008 34.98  356             16384  19.72    .3845 3.719 1e-05 69.63 184.7 41.24      .3527   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5093 6103 16193 2.653\n",
            "\n",
            "20:56:44 | time:2207s total_exs:107972 epochs:0.14\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5894 11595  29.5  300             16384  20.16    .3684 3.593 1e-05  62.9 123.7 36.34      .3808   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5113 5957 11719 1.967\n",
            "\n",
            "20:56:54 | time:2217s total_exs:108316 epochs:0.14\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5751 12593 34.24  344             16384  20.02    .3684 3.514 1e-05 65.59 143.6 33.57      .3860   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   5135 5816 12737 2.19\n",
            "\n",
            "20:57:04 | time:2227s total_exs:108624 epochs:0.14\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5972 12969  30.4  308             16384  20.07    .3858 3.684 1e-05 67.32 146.2 39.79      .3808   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5157 6039 13115 2.172\n",
            "\n",
            "20:57:12 | Overflow: setting loss scale to 16384.0\n",
            "20:57:14 | time:2237s total_exs:108964 epochs:0.14\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9643  6151 17177 33.91  340             16384  19.47    .3684  3.55 1e-05 56.18 156.9 34.81      .3840   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5185 6207 17334 2.793\n",
            "\n",
            "20:57:23 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:57:30 | time:2252s total_exs:109336 epochs:0.14\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6034 10125 24.01  372             16384  19.48    .4034 3.886 1e-05 70.92   119 48.72      .3590   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5211 6105 10244 1.678\n",
            "\n",
            "20:57:40 | time:2262s total_exs:109680 epochs:0.15\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6001 16235 34.47  344             16384  19.81    .3684 3.808 1e-05    65 175.9 45.08      .3544   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5238 6066 16411 2.706\n",
            "\n",
            "20:57:50 | time:2273s total_exs:109940 epochs:0.15\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5981 10508 25.37  260             16384  19.96    .4034 3.787 1e-05    69 121.2 44.11      .3671   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5256 6050 10630 1.757\n",
            "\n",
            "20:58:00 | time:2283s total_exs:110192 epochs:0.15\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6125 12660  24.8  252             16384  20.47    .3684 3.403 1e-05 49.33   102 30.05      .4102   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5277 6175 12762 2.067\n",
            "\n",
            "20:58:10 | time:2293s total_exs:110532 epochs:0.15\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6007 14371 33.89  340             16384  19.78    .3684 3.749 1e-05 70.88 169.6 42.49      .3674   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5301 6078 14541 2.393\n",
            "\n",
            "20:58:21 | time:2303s total_exs:110936 epochs:0.15\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6072 17236 39.54  404             16384  19.93    .3684 3.775 1e-05 69.83 198.2 43.59      .3689   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5330 6142 17434 2.839\n",
            "\n",
            "20:58:29 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:58:36 | time:2319s total_exs:111308 epochs:0.15\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6088 10350 24.33  372             16384  19.62    .3684 3.639 1e-05 70.27 119.5 38.07      .3727   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   5356 6158 10470  1.7\n",
            "\n",
            "20:58:46 | time:2329s total_exs:111684 epochs:0.15\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6114 16262 35.72  376             16384  20.04    .3858 3.655 1e-05 57.43 152.7 38.68      .3905   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   5384 6171 16414 2.66\n",
            "\n",
            "20:58:57 | time:2339s total_exs:111944 epochs:0.15\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6029 11326 25.71  260             16384  19.72    .3684 3.823 1e-05 68.68   129 45.76      .3594   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5403 6097 11455 1.879\n",
            "\n",
            "20:59:07 | time:2350s total_exs:112240 epochs:0.15\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6235 14246 28.18  296             16384  20.67    .4126 3.773 1e-05 53.04 121.2 43.52      .3708   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5427 6288 14368 2.285\n",
            "\n",
            "20:59:17 | time:2360s total_exs:112556 epochs:0.15\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  5875 13404 31.34  316             16384  20.23    .3684 3.704 1e-05 63.35 144.5 40.6      .3727   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5450 5938 13548 2.282\n",
            "\n",
            "20:59:28 | time:2370s total_exs:112972 epochs:0.15\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5916 16660 40.39  416             16384  20.31    .3764 3.821 1e-05 71.38   201 45.65      .3589   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5479 5988 16862 2.816\n",
            "\n",
            "20:59:36 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20:59:42 | time:2385s total_exs:113284 epochs:0.15\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6141 10262 21.72  312             16384  20.11    .3684 3.832 1e-05 65.88 110.1 46.15      .3586   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5503 6207 10372 1.671\n",
            "\n",
            "20:59:52 | time:2395s total_exs:113668 epochs:0.15\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5947 15970 36.83  384             16384  20.52    .3684 3.641 1e-05 64.14 172.2 38.14      .3814   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5531 6011 16142 2.686\n",
            "\n",
            "21:00:03 | time:2405s total_exs:113952 epochs:0.15\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6104 11892 27.65  284             16384  19.54    .3684 3.659 1e-05  66.4 129.4 38.83      .3818   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5551 6170 12021 1.948\n",
            "\n",
            "21:00:13 | time:2416s total_exs:114232 epochs:0.15\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6064 12385 27.23  280             16384  19.93    .3764 3.898 1e-05    82 167.5 49.32      .3391   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5572 6146 12552 2.043\n",
            "\n",
            "21:00:23 | time:2426s total_exs:114512 epochs:0.15\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6154 14160 28.01  280             16384  20.91    .4126 3.689 1e-05 56.61 130.2 40.02      .3671   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5595 6211 14290 2.301\n",
            "\n",
            "21:00:33 | time:2436s total_exs:114864 epochs:0.15\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6011 16733 34.99  352             16384  20.76    .3684 3.718 1e-05 59.68 166.1 41.17      .3692   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5623 6071 16899 2.784\n",
            "\n",
            "21:00:42 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:00:48 | time:2451s total_exs:115180 epochs:0.15\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6145 10219 21.02  316             16384  21.06    .3684 3.612 1e-05 58.56 97.38 37.03      .3654   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5648 6203 10316 1.663\n",
            "\n",
            "21:00:59 | time:2461s total_exs:115536 epochs:0.15\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6181 15851 33.81  356             16384  20.14    .3946 3.703 1e-05 63.07 161.8 40.56      .3705   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5675 6244 16013 2.565\n",
            "\n",
            "21:01:09 | time:2472s total_exs:115804 epochs:0.15\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6059 11926 26.37  268             16384  20.42    .3684 3.706 1e-05 63.25 124.5 40.7      .3818   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5695 6122 12050 1.968\n",
            "\n",
            "21:01:19 | time:2482s total_exs:116076 epochs:0.15\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5907 12220  26.8  272             16384  21.17    .3684 3.541 1e-05 62.76 129.8 34.51      .3915   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5716 5969 12350 2.069\n",
            "\n",
            "21:01:29 | time:2492s total_exs:116400 epochs:0.15\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5848 13339 32.13  324             16384  20.49    .3684 3.686 1e-05 64.48 147.1 39.87      .3682   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5739 5912 13486 2.281\n",
            "\n",
            "21:01:33 | Overflow: setting loss scale to 16384.0\n",
            "21:01:40 | time:2502s total_exs:116788 epochs:0.15\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "   .9655  6021 17040 37.86  388             16384  20.45    .3936 3.517 1e-05 71.86 203.4 33.7      .3796   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   5768 6093 17243 2.83\n",
            "\n",
            "21:01:48 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:01:54 | time:2517s total_exs:117132 epochs:0.16\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5856  9956  23.4  344             16384  20.48    .3684 3.525 1e-05 60.68 103.2 33.95      .3942   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   5793 5916 10060  1.7\n",
            "\n",
            "21:02:05 | time:2527s total_exs:117456 epochs:0.16\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6149 15713 31.84  324             16384  21.39    .3859 3.694 1e-05 52.96 135.3 40.2      .3863   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5819 6202 15849 2.556\n",
            "\n",
            "21:02:15 | time:2537s total_exs:117748 epochs:0.16\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6062 13903 29.12  292             16384  21.27    .3765 3.683 1e-05 65.26 149.7 39.78      .3671   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5842 6127 14053 2.294\n",
            "\n",
            "21:02:25 | time:2548s total_exs:118100 epochs:0.16\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5849 14340 34.52  352             16384  19.76    .3684 3.577 1e-05 63.64   156 35.75      .4048   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5867 5912 14496 2.452\n",
            "\n",
            "21:02:35 | time:2558s total_exs:118440 epochs:0.16\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5815 13736 33.46  340             16384  20.27    .3684 3.786 1e-05 71.75 169.5 44.09      .3630   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5891 5887 13906 2.362\n",
            "\n",
            "21:02:45 | time:2568s total_exs:118744 epochs:0.16\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6006 13694 30.14  304             16384  20.67    .3684 3.541 1e-05 57.87   132 34.51      .3802   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   5914 6063 13826 2.28\n",
            "\n",
            "21:02:54 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:03:01 | time:2583s total_exs:119060 epochs:0.16\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5862  8831  20.7  316             16384  20.47    .3684   3.7 1e-05  64.3 96.88 40.46      .3658   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                   5937 5926 8928 1.507\n",
            "\n",
            "21:03:11 | time:2594s total_exs:119444 epochs:0.16\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5935 16124 37.26  384             16384  20.22    .3681  3.83 1e-05 70.57 191.7 46.07      .3654   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5965 6005 16316 2.717\n",
            "\n",
            "21:03:21 | time:2604s total_exs:119688 epochs:0.16\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6142 11908 23.65  244             16384  21.08    .3681 3.832 1e-05 58.25 112.9 46.15      .3708   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   5985 6200 12021 1.939\n",
            "\n",
            "21:03:32 | time:2614s total_exs:119936 epochs:0.16\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6181 11969 24.01  248             16384  19.72    .3761 3.908 1e-05 72.45 140.3 49.82      .3354   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6005 6253 12109 1.937\n",
            "\n",
            "21:03:42 | time:2624s total_exs:120212 epochs:0.16\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144 13875  27.1  276             16384  20.74    .3681 3.519 1e-05 52.96 119.6 33.74      .3974   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6028 6197 13994 2.258\n",
            "\n",
            "21:03:52 | time:2635s total_exs:120564 epochs:0.16\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6161 17148 34.99  352             16384  20.41    .3681  3.62 1e-05 58.68 163.3 37.35      .3719   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6056 6219 17311 2.784\n",
            "\n",
            "21:04:00 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:04:07 | time:2649s total_exs:120876 epochs:0.16\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6043  9776 21.03  312             16384  20.59    .3681 3.589 1e-05 60.12 97.27 36.18      .3818   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                   6080 6103 9873 1.618\n",
            "\n",
            "21:04:11 | Overflow: setting loss scale to 16384.0\n",
            "21:04:17 | time:2660s total_exs:121220 epochs:0.16\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9643  6145 16474 32.94  344             16384   20.4    .3761 3.748 1e-05 59.14 158.6 42.45      .3623   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6108 6204 16633 2.681\n",
            "\n",
            "21:04:27 | time:2670s total_exs:121508 epochs:0.16\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6017 13022 28.33  288             16384  20.62    .3681 3.715 1e-05 62.27 134.8 41.06      .3620   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6130 6079 13156 2.164\n",
            "\n",
            "21:04:38 | time:2681s total_exs:121760 epochs:0.16\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6116 11605 23.91  252             16384  19.56    .3681 3.918 1e-05 72.05 136.7 50.31      .3407   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6150 6188 11742 1.898\n",
            "\n",
            "21:04:45 | Overflow: setting loss scale to 16384.0\n",
            "21:04:48 | time:2691s total_exs:122056 epochs:0.16\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9545  6050 12997  28.9  296             16384  19.39    .3681 3.654 1e-05 68.18 146.5 38.63      .3847   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6172 6118 13143 2.148\n",
            "\n",
            "21:04:49 | Overflow: setting loss scale to 16384.0\n",
            "21:04:59 | time:2701s total_exs:122432 epochs:0.16\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "   .9655  6027 16932 36.42  376             16384  19.48    .3681 3.671 1e-05 60.83 170.9 39.3      .3719   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   6201 6088 17103 2.81\n",
            "\n",
            "21:05:07 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:05:13 | time:2716s total_exs:122732 epochs:0.16\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6080  9750 20.91  300             16384  19.89    .3901  3.75 1e-05 63.26 101.4 42.53      .3574   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                   6224 6144 9851 1.604\n",
            "\n",
            "21:05:23 | time:2726s total_exs:123112 epochs:0.16\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6090 16465 36.69  380             16384  20.33    .3681  3.75 1e-05 65.32 176.6 42.52      .3652   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6252 6155 16642 2.705\n",
            "\n",
            "21:05:33 | time:2736s total_exs:123348 epochs:0.16\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6139 11666  23.6  236             16384  21.04    .3681 3.671 1e-05 48.89 92.92 39.3      .3994   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6271 6187 11759 1.901\n",
            "\n",
            "21:05:44 | time:2746s total_exs:123660 epochs:0.16\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5983 13507 30.62  312             16384  20.28    .3681 3.751 1e-05 62.74 141.6 42.58      .3708   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6294 6045 13648 2.258\n",
            "\n",
            "21:05:54 | time:2757s total_exs:123904 epochs:0.16\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6137 11874  23.6  244             16384  20.25    .3681 3.678 1e-05 63.55   123 39.56      .3690   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6314 6200 11997 1.935\n",
            "\n",
            "21:06:04 | time:2767s total_exs:124272 epochs:0.16\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5934 16338 36.18  368             16384  20.27    .3681 3.698 1e-05 61.68 169.8 40.35      .3729   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6342 5996 16508 2.753\n",
            "\n",
            "21:06:13 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:06:19 | time:2782s total_exs:124616 epochs:0.17\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6096  9850 23.16  344             16384  19.62    .3943 3.635 1e-05 70.58   114 37.91      .3808   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                   6366 6167 9964 1.616\n",
            "\n",
            "21:06:30 | time:2792s total_exs:124948 epochs:0.17\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6173 15937 31.75  332             16384  20.38    .3855 3.881 1e-05  70.3 181.5 48.47      .3477   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6393 6243 16119 2.582\n",
            "\n",
            "21:06:40 | time:2803s total_exs:125152 epochs:0.17\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6142  9773 20.29  204             16384  21.67    .3681  3.78 1e-05 59.12 94.08 43.83      .3753   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                   6409 6201 9867 1.592\n",
            "\n",
            "21:06:50 | time:2813s total_exs:125432 epochs:0.17\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6118 13443 27.96  280             16384  20.64    .3681 3.516 1e-05    55 120.8 33.66      .4050   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6431 6173 13563 2.197\n",
            "\n",
            "21:07:00 | time:2823s total_exs:125752 epochs:0.17\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6188 15146 31.33  320             16384  21.12    .3766 3.702 1e-05 55.64 136.2 40.53      .3681   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6456 6244 15282 2.448\n",
            "\n",
            "21:07:10 | time:2833s total_exs:126132 epochs:0.17\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5996 17076 37.32  380             16384  20.11    .3681 3.596 1e-05 61.31 174.6 36.45      .3909   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6485 6057 17251 2.848\n",
            "\n",
            "21:07:19 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:07:25 | time:2848s total_exs:126456 epochs:0.17\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5976 10087 21.87  324             16384  20.69    .3761 3.789 1e-05 74.16 125.2 44.21      .3554   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6510 6050 10212 1.688\n",
            "\n",
            "21:07:36 | time:2859s total_exs:126788 epochs:0.17\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6004 14725 31.31  332             16384  20.75    .3681 3.534 1e-05 55.08 135.1 34.27      .3911   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6536 6060 14860 2.453\n",
            "\n",
            "21:07:46 | time:2869s total_exs:127140 epochs:0.17\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5728 14174 34.84  352             16384  20.24    .3681 3.588 1e-05 59.96 148.4 36.16      .3956   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6561 5788 14322 2.474\n",
            "\n",
            "21:07:56 | time:2879s total_exs:127440 epochs:0.17\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6211 14616 29.41  300             16384  20.61    .3943 3.689 1e-05 59.33 139.6 39.99      .3624   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6585 6271 14756 2.353\n",
            "\n",
            "21:08:06 | time:2889s total_exs:127836 epochs:0.17\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5640 14095 39.59  396             16384  19.57    .3681 3.485 1e-05 71.92 179.7 32.63      .4021   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   6610 5711 14275  2.5\n",
            "\n",
            "21:08:17 | time:2899s total_exs:128140 epochs:0.17\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6180 15061 29.63  304             16384   20.5    .3943 3.503 1e-05 59.08   144 33.21      .3758   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6635 6240 15205 2.437\n",
            "\n",
            "21:08:25 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:08:32 | time:2914s total_exs:128472 epochs:0.17\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6046  9641 22.06  332             16384  19.73    .3996 3.644 1e-05 63.71 101.6 38.23      .3787   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                   6659 6109 9743 1.595\n",
            "\n",
            "21:08:42 | time:2925s total_exs:128836 epochs:0.17\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6045 15951 35.57  364             16384  20.26    .3681 3.548 1e-05 63.63 167.9 34.74      .3912   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6686 6108 16119 2.639\n",
            "\n",
            "21:08:52 | time:2935s total_exs:129080 epochs:0.17\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6264 11643 23.87  244             16384  20.21    .4094 3.682 1e-05 60.89 113.2 39.71      .3691   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6705 6325 11756 1.859\n",
            "\n",
            "21:09:03 | time:2945s total_exs:129416 epochs:0.17\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5744 12960 32.96  336             16384  20.22    .3843 3.747 1e-05 77.48 174.8 42.38      .3704   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6728 5822 13135 2.256\n",
            "\n",
            "21:09:13 | time:2955s total_exs:129692 epochs:0.17\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6173 12753 27.15  276             16384  19.42    .3767 3.778 1e-05 66.43 137.2 43.71      .3728   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6749 6240 12890 2.066\n",
            "\n",
            "21:09:23 | time:2966s total_exs:130076 epochs:0.17\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6068 17062 37.23  384             16384  20.13    .3681 3.519 1e-05 62.17 174.8 33.75      .3905   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6778 6130 17237 2.812\n",
            "\n",
            "21:09:32 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:09:38 | time:2981s total_exs:130392 epochs:0.17\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5996  9953 20.98  316             16384  19.67    .3681 3.563 1e-05 65.96 109.5 35.26      .4008   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   6803 6062 10062 1.66\n",
            "\n",
            "21:09:48 | time:2991s total_exs:130772 epochs:0.17\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5814 15562 37.67  380             16384  19.36    .3943 3.623 1e-05 71.37   191 37.47      .3726   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6830 5885 15753 2.677\n",
            "\n",
            "21:09:52 | Overflow: setting loss scale to 16384.0\n",
            "21:09:59 | time:3002s total_exs:130980 epochs:0.17\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9412  6132  9986 19.92  208             16384  19.24    .3681 3.868 1e-05 61.88 100.8 47.83      .3593   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   6847 6194 10087 1.63\n",
            "\n",
            "21:10:09 | time:3012s total_exs:131248 epochs:0.17\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6135 13509 26.82  268             16384  19.69    .3681 3.611 1e-05 61.55 135.5 36.99      .3804   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6869 6196 13645 2.202\n",
            "\n",
            "21:10:19 | time:3022s total_exs:131564 epochs:0.17\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6215 15402 31.32  316             16384   19.7    .4031 3.538 1e-05 57.32   142 34.38      .3852   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6894 6273 15544 2.478\n",
            "\n",
            "21:10:29 | time:3032s total_exs:131928 epochs:0.17\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6042 16899 36.36  364             16384  20.28    .4031 3.588 1e-05 57.93   162 36.17      .3958   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6922 6099 17061 2.797\n",
            "\n",
            "21:10:38 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:10:44 | time:3047s total_exs:132248 epochs:0.18\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6236 10481 21.51  320             16384  20.35    .4094 3.703 1e-05 60.72   102 40.57      .3650   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6947 6297 10583 1.681\n",
            "\n",
            "21:10:55 | time:3057s total_exs:132600 epochs:0.18\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6013 16013 33.48  352             16384  20.06    .3762 3.638 1e-05 65.68 174.9 38.02      .3709   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6975 6079 16188 2.663\n",
            "\n",
            "21:11:05 | time:3067s total_exs:132888 epochs:0.18\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5797 11546 28.68  288             16384  19.31    .3682 3.596 1e-05 59.35 118.2 36.46      .4027   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   6995 5856 11664 1.992\n",
            "\n",
            "21:11:15 | time:3078s total_exs:133152 epochs:0.18\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6124 13089 25.65  264             16384   21.3    .3762 3.884 1e-05 65.09 139.1 48.63      .3443   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7017 6189 13229 2.138\n",
            "\n",
            "21:11:25 | time:3088s total_exs:133440 epochs:0.18\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6222 14214  28.6  288             16384  19.92    .4124 3.719 1e-05 63.78 145.7 41.22      .3572   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7040 6286 14360 2.284\n",
            "\n",
            "21:11:35 | time:3098s total_exs:133792 epochs:0.18\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6017 16546 34.57  352             16384  20.43    .3682 3.702 1e-05 65.64 180.5 40.54      .3672   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   7068 6083 16727 2.75\n",
            "\n",
            "21:11:44 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:11:50 | time:3113s total_exs:134132 epochs:0.18\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5890  9954 22.98  340             16384  19.43    .3682 3.594 1e-05  61.8 104.4 36.39      .3858   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   7093 5952 10059 1.69\n",
            "\n",
            "21:12:01 | time:3123s total_exs:134500 epochs:0.18\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5899 15728 35.04  368             16384  19.92    .3681 3.555 1e-05 60.89 162.4 34.98      .3865   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7121 5960 15891 2.667\n",
            "\n",
            "21:12:11 | time:3134s total_exs:134716 epochs:0.18\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6131 10716 20.97  216             16384  21.05    .3681  3.69 1e-05 58.06 101.5 40.04      .3608   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7139 6189 10818 1.748\n",
            "\n",
            "21:12:17 | Overflow: setting loss scale to 16384.0\n",
            "21:12:21 | time:3144s total_exs:135028 epochs:0.18\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9545  5792 12458  30.5  312             16384  19.13    .3682 3.645 1e-05 68.59 147.5 38.29      .3625   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7161 5861 12605 2.151\n",
            "\n",
            "21:12:31 | time:3154s total_exs:135332 epochs:0.18\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5998 14368 30.34  304             16384  20.11    .3681 3.621 1e-05 57.38 137.4 37.37      .3813   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7185 6056 14505 2.395\n",
            "\n",
            "21:12:42 | time:3164s total_exs:135704 epochs:0.18\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6029 17026 36.23  372             16384   20.1    .3682 3.632 1e-05 58.83 166.1 37.8      .3769   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7214 6088 17192 2.824\n",
            "\n",
            "21:12:50 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:12:57 | time:3179s total_exs:135992 epochs:0.18\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6132  9856 19.29  288             16384  20.14    .3682 3.485 1e-05 49.25 79.16 32.61      .4188   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                   7238 6181 9935 1.607\n",
            "\n",
            "21:13:07 | time:3190s total_exs:136284 epochs:0.18\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6145 14552 28.81  292             16384  19.67    .3682 3.836 1e-05 69.29 164.1 46.34      .3506   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7262 6215 14716 2.368\n",
            "\n",
            "21:13:17 | time:3200s total_exs:136636 epochs:0.18\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5858 14597 35.08  352             16384  19.63    .3972 3.649 1e-05  69.8 173.9 38.45      .3713   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7287 5928 14771 2.492\n",
            "\n",
            "21:13:27 | time:3210s total_exs:136968 epochs:0.18\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5970 14531 32.32  332             16384  19.37    .3682 3.487 1e-05    62 150.9 32.69      .3916   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7312 6032 14682 2.434\n",
            "\n",
            "21:13:37 | time:3220s total_exs:137284 epochs:0.18\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6193 15314 31.26  316             16384  20.49    .4124   3.6 1e-05 66.36 164.1 36.61      .3966   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7337 6259 15478 2.473\n",
            "\n",
            "21:13:48 | time:3230s total_exs:137604 epochs:0.18\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5816 13071 31.27  320             16384  19.81    .3682 3.582 1e-05  60.7 136.4 35.95      .3926   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7360 5876 13207 2.248\n",
            "\n",
            "21:13:56 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:14:03 | time:3246s total_exs:137904 epochs:0.18\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6086  9693 19.91  300             16384  19.79    .3682 3.569 1e-05 59.96  95.5 35.48      .3760   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                   7384 6146 9788 1.593\n",
            "\n",
            "21:14:13 | time:3256s total_exs:138256 epochs:0.18\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6054 16095 33.42  352             16384  20.44    .3682 3.644 1e-05 58.82 156.4 38.23      .3868   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7412 6113 16251 2.659\n",
            "\n",
            "21:14:24 | time:3266s total_exs:138500 epochs:0.18\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6183 11475 23.83  244             16384  20.05    .3682 3.779 1e-05 72.16 133.9 43.76      .3494   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7431 6255 11609 1.856\n",
            "\n",
            "21:14:34 | time:3277s total_exs:138728 epochs:0.18\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144 11269 22.01  228             16384   20.6    .3682 3.842 1e-05 61.32 112.5 46.62      .3511   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7450 6205 11381 1.834\n",
            "\n",
            "21:14:44 | time:3287s total_exs:139040 epochs:0.18\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6141 15024 30.53  312             16384  20.82    .3682 3.483 1e-05 54.32 132.9 32.56      .3859   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7475 6195 15157 2.447\n",
            "\n",
            "21:14:55 | time:3297s total_exs:139384 epochs:0.18\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6169 17114 34.08  344             16384  20.32    .3682 3.633 1e-05 59.89 166.2 37.81      .3709   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7503 6229 17280 2.774\n",
            "\n",
            "21:15:03 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:15:09 | time:3312s total_exs:139684 epochs:0.19\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5982  9757 20.39  300             16384  20.19    .3682  3.53 1e-05 54.88  89.5 34.13      .3850   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                   7527 6037 9846 1.631\n",
            "\n",
            "21:15:20 | time:3323s total_exs:140008 epochs:0.19\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6115 15630 30.67  324             16384  21.34    .3682 3.713 1e-05 53.81 137.5 40.96      .3696   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7554 6169 15767 2.556\n",
            "\n",
            "21:15:30 | time:3333s total_exs:140268 epochs:0.19\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6064 11950 25.62  260             16384   20.5    .3762 3.844 1e-05  69.4 136.8 46.72      .3465   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7574 6134 12087 1.971\n",
            "\n",
            "21:15:40 | time:3343s total_exs:140524 epochs:0.19\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6071 12020 25.34  256             16384  19.76    .3944 3.756 1e-05 66.15   131 42.77      .3575   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   7594 6137 12151 1.98\n",
            "\n",
            "21:15:51 | time:3353s total_exs:140796 epochs:0.19\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6164 13186 26.45  272             16384  20.12    .3682  3.71 1e-05 56.36 120.6 40.85      .3806   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7616 6221 13307 2.139\n",
            "\n",
            "21:16:01 | time:3364s total_exs:141160 epochs:0.19\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6086 17127 35.32  364             16384  20.12    .3682 3.475 1e-05 58.62   165 32.3      .3871   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7645 6144 17292 2.814\n",
            "\n",
            "21:16:09 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:16:16 | time:3378s total_exs:141456 epochs:0.19\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6181 10010 19.97  296             16384  20.78    .3856 3.702 1e-05 58.46 94.66 40.51      .3778   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7669 6240 10105 1.619\n",
            "\n",
            "21:16:26 | time:3389s total_exs:141812 epochs:0.19\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6024 16209 34.21  356             16384  20.97    .3809 3.584 1e-05 59.96 161.3 36.02      .3842   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7697 6084 16370 2.691\n",
            "\n",
            "21:16:37 | time:3399s total_exs:142044 epochs:0.19\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6201 11388 22.42  232             16384  20.85    .3902  3.71 1e-05 59.37   109 40.86      .3679   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7716 6260 11497 1.837\n",
            "\n",
            "21:16:47 | time:3409s total_exs:142292 epochs:0.19\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5939 11306 24.85  248             16384  20.37    .3997 3.853 1e-05 74.89 142.6 47.11      .3584   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7735 6014 11449 1.904\n",
            "\n",
            "21:16:57 | time:3419s total_exs:142588 epochs:0.19\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5917 13504 29.37  296             16384  20.14    .3682 3.669 1e-05 62.57 142.8 39.23      .3787   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7758 5980 13647 2.282\n",
            "\n",
            "21:17:07 | time:3430s total_exs:142944 epochs:0.19\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5992 16554 35.12  356             16384  19.84    .3682 3.601 1e-05 65.11 179.9 36.63      .3840   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7786 6057 16734 2.763\n",
            "\n",
            "21:17:16 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:17:22 | time:3445s total_exs:143272 epochs:0.19\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5919  9767 21.65  328             16384  20.24    .3682 3.626 1e-05 60.76 100.3 37.55      .3831   \n",
            "    total_train_updates  tpb  tps  ups  \n",
            "                   7811 5980 9867 1.65\n",
            "\n",
            "21:17:33 | time:3455s total_exs:143640 epochs:0.19\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5924 15712 34.86  368             16384  20.37    .3682 3.529 1e-05 60.14 159.5 34.09      .3777   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7839 5984 15871 2.652\n",
            "\n",
            "21:17:43 | time:3466s total_exs:143892 epochs:0.19\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6144 12764 24.93  252             16384  21.12    .3682 3.529 1e-05    54 112.2 34.1      .3792   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7860 6198 12876 2.078\n",
            "\n",
            "21:17:53 | time:3476s total_exs:144176 epochs:0.19\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5960 12622 27.34  284             16384  21.15    .3682 3.758 1e-05 64.14 135.8 42.86      .3692   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7882 6024 12758 2.118\n",
            "\n",
            "21:18:03 | time:3486s total_exs:144416 epochs:0.19\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6144 12153 23.74  240             16384  19.78    .3682 3.526 1e-05  54.9 108.6   34      .4016   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7902 6199 12262 1.978\n",
            "\n",
            "21:18:14 | time:3496s total_exs:144840 epochs:0.19\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5584 16101 42.16  424             16384  19.89    .3682 3.473 1e-05 66.69 192.3 32.22      .3976   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7931 5650 16293 2.884\n",
            "\n",
            "21:18:22 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:18:28 | time:3511s total_exs:145164 epochs:0.19\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5918  9796 22.35  324             16384  19.83    .3682 3.565 1e-05 63.62 105.3 35.35      .3779   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                   7955 5982 9901 1.655\n",
            "\n",
            "21:18:39 | time:3521s total_exs:145536 epochs:0.19\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5750 14943 35.81  372             16384  19.85    .3682 3.689 1e-05 70.74 183.9 40.02      .3733   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7982 5820 15127 2.599\n",
            "\n",
            "21:18:49 | time:3532s total_exs:145744 epochs:0.19\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6221 10240 20.14  208             16384  21.42    .4124 3.618 1e-05 54.41 89.56 37.26      .3676   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   7999 6276 10330 1.646\n",
            "\n",
            "21:18:59 | time:3542s total_exs:146048 epochs:0.19\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5846 12631 29.86  304             16384  19.72    .3682 3.596 1e-05 61.59 133.1 36.45      .3786   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8021 5907 12764 2.161\n",
            "\n",
            "21:19:09 | time:3552s total_exs:146348 epochs:0.19\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6127 14964 29.31  300             16384  20.82    .3682 3.721 1e-05 59.24 144.7 41.32      .3599   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8046 6186 15108 2.443\n",
            "\n",
            "21:19:20 | time:3562s total_exs:146712 epochs:0.19\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6038 16719 35.99  364             16384  20.46    .3763 3.744 1e-05 67.32 186.4 42.26      .3565   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8074 6106 16905 2.769\n",
            "\n",
            "21:19:26 | Overflow: setting loss scale to 16384.0\n",
            "21:19:28 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:19:34 | time:3577s total_exs:147076 epochs:0.19\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9583  5629  9173 24.71  364             16384  19.12    .3682 3.619 1e-05    71 115.7 37.32      .3744   \n",
            "    total_train_updates  tpb  tps  ups  \n",
            "                   8098 5700 9289 1.63\n",
            "\n",
            "21:19:45 | time:3588s total_exs:147416 epochs:0.20\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6084 15660 32.41  340             16384  19.38    .3682 3.719 1e-05  68.3 175.8 41.2      .3639   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8125 6153 15836 2.574\n",
            "\n",
            "21:19:55 | time:3598s total_exs:147676 epochs:0.20\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6094 12167 25.95  260             16384  19.96    .3682 3.532 1e-05  59.6   119 34.21      .3985   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8145 6154 12286 1.997\n",
            "\n",
            "21:19:58 | time:3600s total_exs:147752 epochs:0.20\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6126 14071 29.09   76             16384  20.17    .3682 3.699 1e-05 63.67 146.2 40.41      .3927   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8151 6190 14217 2.298\n",
            "\n",
            "21:19:58 | creating task(s): jsonfile\n",
            "21:19:59 | [loading data from json file into task:/content/drive/MyDrive/chatbot_data/data_valid.jsonl]\n",
            "21:19:59 | \u001b[31mMetadata does not exist. Please double check your datapath.\u001b[0m\n",
            "21:20:02 | running eval: valid\n",
            "21:20:02 | \u001b[33m--skip-generation true produces limited metrics\u001b[0m\n",
            "21:20:53 | eval completed in 51.29s\n",
            "21:20:53 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates   tpb   tps\n",
            "   988.3 14628  29.6 1516   .08898 3.668 1e-05 8.489 125.7 39.16      .3941                 8151 996.8 14754\n",
            "\u001b[0m\n",
            "21:20:53 | \u001b[1;32mnew best ppl: 39.16\u001b[0m\n",
            "21:20:53 | saving best valid model: /content/drive/MyDrive/chatbot_model/model\n",
            "21:20:53 | Saving dictionary to /content/drive/MyDrive/chatbot_model/model.dict\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:20:59 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n",
            "21:21:27 | time:3689s total_exs:147764 epochs:0.20\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144 13726 26.79   12             16384  18.77    .3682 4.572 1e-05   125 279.2 96.73      .2560   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8152 6269 14005 2.238\n",
            "\n",
            "21:21:37 | time:3700s total_exs:148060 epochs:0.20\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5848 11982 28.88  296             16384  19.89    .3682 3.754 1e-05 69.33 142.1 42.67      .3640   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   8173 5917 12124 2.05\n",
            "\n",
            "21:21:47 | time:3710s total_exs:148376 epochs:0.20\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6032 14621 30.63  316             16384  20.06    .3682 3.492 1e-05 56.76 137.6 32.84      .4010   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8198 6089 14759 2.424\n",
            "\n",
            "21:21:57 | time:3720s total_exs:148676 epochs:0.20\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6080 14562 29.94  300             16384  19.88    .3763 3.729 1e-05 64.21 153.8 41.63      .3621   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8222 6144 14716 2.395\n",
            "\n",
            "21:22:08 | time:3730s total_exs:148984 epochs:0.20\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6015 14411 30.74  308             16384  21.49    .3763  3.67 1e-05 62.29 149.2 39.25      .3739   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8246 6078 14560 2.396\n",
            "\n",
            "21:22:18 | time:3740s total_exs:149328 epochs:0.20\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5883 14596 34.14  344             16384  19.14    .3682 3.792 1e-05 70.84 175.8 44.36      .3574   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8271 5954 14772 2.481\n",
            "\n",
            "21:22:27 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:22:33 | time:3756s total_exs:149620 epochs:0.20\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6010  8802 18.59  292             16384  20.42    .3682  3.78 1e-05 60.48 88.58 43.83      .3587   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                   8294 6070 8891 1.465\n",
            "\n",
            "21:22:44 | time:3767s total_exs:149980 epochs:0.20\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6009 16049 34.34  360             16384  20.45    .3682 3.396 1e-05 55.36 147.8 29.84      .4194   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8322 6065 16197 2.671\n",
            "\n",
            "21:22:54 | time:3777s total_exs:150232 epochs:0.20\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6030 11372 25.01  252             16384  20.71    .3877 3.379 1e-05 55.68   105 29.33      .4008   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8341 6085 11477 1.886\n",
            "\n",
            "21:23:04 | time:3787s total_exs:150544 epochs:0.20\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5807 13309 31.09  312             16384  20.65    .3763 3.681 1e-05 66.52 152.4 39.68      .3699   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8364 5874 13462 2.292\n",
            "\n",
            "21:23:14 | time:3797s total_exs:150872 epochs:0.20\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5998 14351  32.7  328             16384  20.13    .4125 3.665 1e-05 63.96   153 39.04      .3687   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8388 6062 14504 2.393\n",
            "\n",
            "21:23:24 | Overflow: setting loss scale to 16384.0\n",
            "21:23:24 | time:3807s total_exs:151196 epochs:0.20\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9630  6144 16551 32.33  324             16384  19.97    .3763 3.791 1e-05    65 175.1 44.31      .3590   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8415 6209 16726 2.694\n",
            "\n",
            "21:23:33 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:23:40 | time:3822s total_exs:151524 epochs:0.20\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6074 10324 21.44  328             16384  20.54    .3683 3.607 1e-05 54.92 93.36 36.86      .3845   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   8441 6129 10417  1.7\n",
            "\n",
            "21:23:50 | time:3832s total_exs:151856 epochs:0.20\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6235 16202 33.18  332             16384  20.33    .4124 3.827 1e-05 64.12 166.6 45.93      .3707   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8467 6299 16368 2.599\n",
            "\n",
            "21:24:00 | time:3843s total_exs:152088 epochs:0.20\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5938 10280 22.31  232             16384   19.9    .3682 3.569 1e-05 59.39 102.8 35.49      .3891   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8485 5997 10383 1.731\n",
            "\n",
            "21:24:10 | time:3853s total_exs:152368 epochs:0.20\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6166 14186 28.01  280             16384  20.26    .3810 3.764 1e-05  63.7 146.5 43.12      .3631   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8508 6230 14332 2.301\n",
            "\n",
            "21:24:20 | time:3863s total_exs:152664 epochs:0.20\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6162 14751 29.52  296             16384  19.76    .4125 3.702 1e-05 63.17 151.2 40.52      .3674   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8532 6226 14902 2.394\n",
            "\n",
            "21:24:22 | Overflow: setting loss scale to 16384.0\n",
            "21:24:30 | time:3873s total_exs:152996 epochs:0.20\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9630  6121 16527  33.2  332             16384  19.55    .3683  3.56 1e-05 52.81 142.6 35.15      .3941   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   8559 6174 16669  2.7\n",
            "\n",
            "21:24:39 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:24:46 | time:3888s total_exs:153308 epochs:0.20\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144 10351 20.22  312             16384  20.56    .3683 3.283 1e-05 47.85 80.61 26.66      .4140   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8585 6192 10432 1.685\n",
            "\n",
            "21:24:56 | time:3899s total_exs:153660 epochs:0.20\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5993 15915 33.38  352             16384  20.17    .3763 3.763 1e-05 68.86 182.8 43.07      .3522   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8613 6062 16098 2.656\n",
            "\n",
            "21:25:07 | time:3909s total_exs:153888 epochs:0.20\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6144 11392 22.25  228             16384     21    .3683 3.606 1e-05 52.95 98.17 36.8      .3907   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8632 6197 11491 1.854\n",
            "\n",
            "21:25:17 | time:3919s total_exs:154128 epochs:0.20\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144 12177 23.78  240             16384   20.9    .3683 3.694 1e-05  56.2 111.4 40.22      .3514   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8652 6200 12288 1.982\n",
            "\n",
            "21:25:27 | time:3930s total_exs:154428 epochs:0.20\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6131 14870  29.1  300             16384  20.15    .3683 3.488 1e-05 59.72 144.8 32.71      .3878   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8677 6191 15015 2.426\n",
            "\n",
            "21:25:37 | time:3940s total_exs:154764 epochs:0.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6122 16883 33.09  336             16384  19.94    .3683 3.555 1e-05 56.68 156.3 34.99      .3787   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8705 6179 17039 2.758\n",
            "\n",
            "21:25:46 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:25:52 | time:3954s total_exs:155064 epochs:0.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6105 10189 20.86  300             16384  21.13    .3683 3.482 1e-05 55.38 92.42 32.54      .3875   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8729 6160 10282 1.669\n",
            "\n",
            "21:26:02 | time:3965s total_exs:155448 epochs:0.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  5770 15426 36.67  384             16384  19.11    .3857  3.52 1e-05 63.32 169.3 33.8      .3897   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8757 5833 15596 2.674\n",
            "\n",
            "21:26:07 | Overflow: setting loss scale to 16384.0\n",
            "21:26:13 | time:3975s total_exs:155704 epochs:0.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9524  6117 12665 25.24  256             16384  18.44    .3683 3.838 1e-05 69.43 143.8 46.45      .3519   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8778 6186 12809 2.071\n",
            "\n",
            "21:26:23 | time:3985s total_exs:155980 epochs:0.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144 13998 27.34  276             16384  20.87    .3683   3.7 1e-05 61.87   141 40.46      .3570   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8801 6206 14139 2.279\n",
            "\n",
            "21:26:33 | time:3995s total_exs:156256 epochs:0.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5910 12271 27.29  276             16384  20.47    .3683 3.675 1e-05 57.86 120.1 39.46      .3654   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8822 5968 12391 2.077\n",
            "\n",
            "21:26:43 | time:4006s total_exs:156600 epochs:0.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6014 15961 33.81  344             16384  19.53    .3763 3.881 1e-05 74.19 196.9 48.49      .3410   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8849 6089 16158 2.654\n",
            "\n",
            "21:26:52 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:26:58 | time:4021s total_exs:156916 epochs:0.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6078 10101 21.01  316             16384  19.56    .4032 3.657 1e-05 59.16 98.32 38.73      .3773   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8874 6137 10199 1.662\n",
            "\n",
            "21:27:08 | time:4031s total_exs:157240 epochs:0.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6121 16004 31.38  324             16384  20.91    .3683 3.529 1e-05    52   136 34.08      .3846   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8901 6173 16140 2.615\n",
            "\n",
            "21:27:19 | time:4041s total_exs:157500 epochs:0.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5745 10329 25.97  260             16384  19.53    .4125 3.415 1e-05 64.72 116.4 30.43      .4017   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8919 5810 10445 1.798\n",
            "\n",
            "21:27:29 | time:4051s total_exs:157772 epochs:0.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6056 13245 27.04  272             16384  20.07    .3683 3.744 1e-05 61.41 134.3 42.27      .3671   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8941 6117 13379 2.187\n",
            "\n",
            "21:27:39 | time:4061s total_exs:158116 epochs:0.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5727 14315 34.39  344             16384  19.69    .3683 3.549 1e-05 63.04 157.6 34.79      .3775   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   8966 5790 14473  2.5\n",
            "\n",
            "21:27:49 | time:4071s total_exs:158468 epochs:0.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5980 16654 35.01  352             16384  19.21    .3763 3.658 1e-05 67.79 188.8 38.78      .3825   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   8994 6048 16843 2.785\n",
            "\n",
            "21:27:58 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:28:04 | time:4087s total_exs:158812 epochs:0.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5958 10545 22.55  344             16384  19.76    .3683 3.549 1e-05 61.74 109.3 34.77      .3941   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   9021 6020 10654 1.77\n",
            "\n",
            "21:28:15 | time:4097s total_exs:159180 epochs:0.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5851 15766 35.42  368             16384  18.87    .3683 3.662 1e-05 66.25 178.5 38.93      .3612   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9049 5917 15945 2.695\n",
            "\n",
            "21:28:25 | time:4108s total_exs:159400 epochs:0.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5921  9693 21.19  220             16384  19.53    .3683 3.363 1e-05 58.06 95.08 28.86      .3951   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                   9066 5979 9788 1.638\n",
            "\n",
            "21:28:35 | time:4118s total_exs:159668 epochs:0.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6031 12475 26.39  268             16384  20.46    .3683 3.501 1e-05 55.24 114.3 33.15      .4190   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9087 6086 12589 2.069\n",
            "\n",
            "21:28:45 | time:4128s total_exs:159968 epochs:0.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6119 15098 29.61  300             16384  19.87    .3763 3.725 1e-05 60.52 149.3 41.46      .3596   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9112 6180 15247 2.467\n",
            "\n",
            "21:28:56 | time:4138s total_exs:160328 epochs:0.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5997 16549 35.48  360             16384  19.64    .3945  3.43 1e-05 63.86 176.2 30.89      .3887   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   9140 6061 16726 2.76\n",
            "\n",
            "21:29:04 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:29:10 | time:4153s total_exs:160632 epochs:0.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5982 10064 21.31  304             16384  19.98    .3683 3.541 1e-05 54.25 91.27 34.49      .4017   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9164 6036 10155 1.683\n",
            "\n",
            "21:29:20 | time:4163s total_exs:160956 epochs:0.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6132 16101 31.51  324             16384  19.47    .3763 3.654 1e-05  63.3 166.2 38.62      .3733   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9191 6196 16267 2.626\n",
            "\n",
            "21:29:30 | time:4173s total_exs:161148 epochs:0.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144  9763 19.06  192             16384  19.98    .3683 3.458 1e-05  54.5  86.6 31.76      .4048   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                   9207 6198 9850 1.589\n",
            "\n",
            "21:29:41 | time:4183s total_exs:161464 epochs:0.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5991 14717 31.05  316             16384  19.47    .3683 3.493 1e-05 55.08 135.3 32.87      .3936   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9232 6047 14853 2.457\n",
            "\n",
            "21:29:51 | time:4193s total_exs:161760 epochs:0.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6012 13540 28.98  296             16384  20.09    .3763 3.787 1e-05 63.52 143.1 44.1      .3641   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9255 6075 13683 2.252\n",
            "\n",
            "21:30:01 | time:4204s total_exs:162084 epochs:0.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6114 16181 31.75  324             16384  20.04    .3683  3.73 1e-05 62.74   166 41.7      .3613   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9282 6177 16347 2.646\n",
            "\n",
            "21:30:10 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:30:16 | time:4219s total_exs:162424 epochs:0.22\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5918  9984 22.94  340             16384  18.79    .3683 3.604 1e-05 71.04 119.8 36.74      .3773   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9307 5989 10103 1.687\n",
            "\n",
            "21:30:26 | time:4229s total_exs:162800 epochs:0.22\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5994 16246 36.39  376             16384  20.14    .4033 3.705 1e-05    67 181.6 40.63      .3614   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9335 6061 16428 2.711\n",
            "\n",
            "21:30:36 | time:4239s total_exs:163036 epochs:0.22\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6022 10874 23.67  236             16384  19.63    .3683  3.43 1e-05 62.89 113.5 30.89      .3949   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9353 6085 10988 1.806\n",
            "\n",
            "21:30:47 | time:4249s total_exs:163288 epochs:0.22\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144 12688 24.78  252             16384  20.37    .3683 3.844 1e-05 58.43 120.7 46.72      .3725   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9374 6202 12809 2.065\n",
            "\n",
            "21:30:57 | time:4259s total_exs:163588 epochs:0.22\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6127 14609  29.8  300             16384  20.31    .3683 3.495 1e-05 51.83 123.6 32.95      .4011   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9398 6179 14733 2.385\n",
            "\n",
            "21:31:07 | time:4269s total_exs:163964 epochs:0.22\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5961 16496 37.16  376             16384  18.85    .3683 3.565 1e-05 70.29 194.5 35.34      .3755   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9426 6032 16691 2.767\n",
            "\n",
            "21:31:16 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:31:22 | time:4285s total_exs:164276 epochs:0.22\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6154 10016 20.31  312             16384  19.51    .3683 3.634 1e-05    64 104.2 37.85      .3631   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9451 6218 10120 1.628\n",
            "\n",
            "21:31:33 | time:4295s total_exs:164616 epochs:0.22\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6025 15776 32.97  340             16384  19.72    .3683 3.614 1e-05    64 167.6 37.13      .3727   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9478 6089 15944 2.619\n",
            "\n",
            "21:31:43 | time:4306s total_exs:164856 epochs:0.22\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6116 11345 23.43  240             16384  20.03    .3764 3.861 1e-05 64.89 120.4 47.49      .3520   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9497 6181 11465 1.855\n",
            "\n",
            "21:31:53 | time:4316s total_exs:165120 epochs:0.22\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6111 12937  25.4  264             16384  20.38    .3683 3.568 1e-05 47.86 101.3 35.45      .3894   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9519 6158 13038 2.117\n",
            "\n",
            "21:31:54 | Overflow: setting loss scale to 16384.0\n",
            "21:32:03 | time:4326s total_exs:165388 epochs:0.22\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "   .9545  6115 13476 26.84  268             16384  19.74    .3683 3.487 1e-05 52.82 116.4 32.7      .3959   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9541 6168 13593 2.204\n",
            "\n",
            "21:32:14 | time:4336s total_exs:165784 epochs:0.22\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5790 16276 38.38  396             16384  19.33    .3683 3.505 1e-05 59.03 165.9 33.29      .3984   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9570 5849 16442 2.811\n",
            "\n",
            "21:32:22 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:32:29 | time:4351s total_exs:166108 epochs:0.22\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5767  9316 21.81  324             16384  18.82    .3683 3.486 1e-05 64.12 103.6 32.66      .3873   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                   9594 5831 9420 1.616\n",
            "\n",
            "21:32:39 | time:4361s total_exs:166436 epochs:0.22\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6041 15758 32.91  328             16384  19.85    .3683 3.649 1e-05 61.04 159.2 38.43      .3869   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9620 6102 15917 2.609\n",
            "\n",
            "21:32:49 | time:4372s total_exs:166664 epochs:0.22\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6111 10621 22.01  228             16384  19.83    .3683 3.718 1e-05  60.5 105.1 41.18      .3673   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9638 6172 10726 1.738\n",
            "\n",
            "21:32:52 | Overflow: setting loss scale to 16384.0\n",
            "21:32:59 | time:4382s total_exs:166936 epochs:0.22\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "   .9524  5969 12168  26.4  272             16384  18.95    .3683 3.463 1e-05 54.48 111.1 31.9      .3907   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9659 6024 12279 2.039\n",
            "\n",
            "21:33:10 | time:4392s total_exs:167260 epochs:0.22\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6000 15152 31.47  324             16384  20.38    .3683 3.579 1e-05 57.69 145.7 35.82      .3967   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9685 6058 15298 2.525\n",
            "\n",
            "21:33:20 | time:4402s total_exs:167600 epochs:0.22\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6124 17051 33.81  340             16384  19.93    .4125 3.517 1e-05 56.64 157.7 33.7      .3821   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9713 6181 17209 2.784\n",
            "\n",
            "21:33:28 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:33:34 | time:4417s total_exs:167900 epochs:0.22\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144 10181 20.71  300             16384   19.7    .3683  3.45 1e-05  60.5 100.3 31.51      .4022   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9737 6204 10281 1.657\n",
            "\n",
            "21:33:45 | time:4428s total_exs:168236 epochs:0.22\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144 16216 31.67  336             16384  20.79    .3683 3.673 1e-05 57.29 151.2 39.35      .3728   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   9765 6201 16368 2.64\n",
            "\n",
            "21:33:55 | time:4438s total_exs:168452 epochs:0.22\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144 10755    21  216             16384  20.49    .3683 3.795 1e-05 66.06 115.6 44.48      .3482   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9783 6210 10870 1.751\n",
            "\n",
            "21:34:06 | time:4448s total_exs:168732 epochs:0.22\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6113 13029 27.13  280             16384  19.45    .3904 3.523 1e-05 52.95 112.9 33.89      .4043   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9805 6166 13142 2.132\n",
            "\n",
            "21:34:16 | time:4459s total_exs:169040 epochs:0.22\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  5998 14236 30.46  308             16384   19.3    .4125 3.669 1e-05 62.67 148.7 39.2      .3816   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9829 6061 14385 2.374\n",
            "\n",
            "21:34:26 | time:4469s total_exs:169376 epochs:0.22\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6135 16985 33.22  336             16384  20.34    .3683 3.505 1e-05 52.93 146.5 33.27      .3893   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9857 6188 17131 2.769\n",
            "\n",
            "21:34:34 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:34:40 | time:4483s total_exs:169664 epochs:0.22\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5969  9707 20.36  288             16384   19.8    .3683 3.584 1e-05 61.57 100.1 36.02      .3686   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                   9880 6030 9807 1.626\n",
            "\n",
            "21:34:51 | time:4493s total_exs:170020 epochs:0.23\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6022 15792 34.57  356             16384  18.78    .3683 3.717 1e-05 62.85 164.8 41.13      .3765   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9907 6085 15957 2.623\n",
            "\n",
            "21:35:01 | time:4503s total_exs:170248 epochs:0.23\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5973 10631 22.55  228             16384  19.77    .3683  3.51 1e-05 59.33 105.6 33.45      .3708   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   9925 6032 10737 1.78\n",
            "\n",
            "21:35:11 | time:4514s total_exs:170520 epochs:0.23\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6159 13050 26.19  272             16384  20.46    .3683 3.836 1e-05 67.27 142.5 46.34      .3635   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9947 6227 13192 2.119\n",
            "\n",
            "21:35:21 | time:4524s total_exs:170836 epochs:0.23\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5822 13766 31.13  316             16384  19.52    .3684 3.664 1e-05 64.83 153.3 39.01      .3715   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9971 5887 13919 2.365\n",
            "\n",
            "21:35:31 | time:4534s total_exs:171200 epochs:0.23\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5941 16526 36.16  364             16384  19.53    .3683 3.451 1e-05 56.71 157.8 31.52      .3936   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   9999 5998 16684 2.782\n",
            "\n",
            "21:35:40 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:35:47 | time:4550s total_exs:171500 epochs:0.23\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144  9988 19.51  300             16384  19.87    .3684 3.543 1e-05 59.12 96.11 34.56      .3823   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10024 6203 10085 1.626\n",
            "\n",
            "21:35:57 | time:4560s total_exs:171828 epochs:0.23\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6004 15371 32.29  328             16384  20.49    .3683 3.616 1e-05 53.65 137.4 37.19      .3814   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                  10050 6058 15509 2.56\n",
            "\n",
            "21:36:07 | time:4570s total_exs:172084 epochs:0.23\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  5973 11916 25.53  256             16384  20.03    .3684 3.619 1e-05 61.95 123.6 37.3      .3818   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10070 6035 12040 1.995\n",
            "\n",
            "21:36:17 | time:4580s total_exs:172336 epochs:0.23\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144 12826 25.05  252             16384  20.62    .3684 3.554 1e-05 51.67 107.9 34.95      .3982   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10091 6196 12933 2.088\n",
            "\n",
            "21:36:28 | time:4590s total_exs:172600 epochs:0.23\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6018 11775 25.83  264             16384  19.42    .3684 3.576 1e-05 63.35   124 35.74      .3702   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10111 6081 11899 1.957\n",
            "\n",
            "21:36:38 | time:4600s total_exs:172936 epochs:0.23\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144 16305 33.02  336             16384  20.35    .3684 3.598 1e-05 57.67   153 36.54      .3815   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10138 6202 16458 2.654\n",
            "\n",
            "21:36:47 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:36:53 | time:4615s total_exs:173244 epochs:0.23\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6080 10153 20.57  308             16384  20.34    .3846 3.936 1e-05 71.64 119.6 51.24      .3356   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                  10163 6152 10272 1.67\n",
            "\n",
            "21:37:03 | time:4626s total_exs:173568 epochs:0.23\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144 15759 30.78  324             16384  19.38    .3684 3.543 1e-05 60.74 155.8 34.58      .3774   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10190 6205 15915 2.565\n",
            "\n",
            "21:37:14 | time:4636s total_exs:173780 epochs:0.23\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6186 10463 21.09  212             16384  19.76    .3684 3.602 1e-05 61.18 103.5 36.68      .3731   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10207 6247 10567 1.692\n",
            "\n",
            "21:37:24 | time:4646s total_exs:174068 epochs:0.23\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6016 13128 28.56  288             16384  19.81    .3935 3.459 1e-05 60.77 132.6 31.79      .3874   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10229 6077 13260 2.182\n",
            "\n",
            "21:37:34 | time:4656s total_exs:174336 epochs:0.23\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6142 13508 26.79  268             16384  20.09    .3684 3.503 1e-05 54.36 119.6 33.21      .3972   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10251 6196 13627 2.199\n",
            "\n",
            "21:37:44 | time:4666s total_exs:174660 epochs:0.23\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6117 16502 32.37  324             16384   20.7    .3681 3.529 1e-05  51.3 138.4 34.11      .3928   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10278 6169 16640 2.698\n",
            "\n",
            "21:37:53 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:37:59 | time:4682s total_exs:174976 epochs:0.23\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  5989  9867 20.83  316             16384  20.19    .3681 3.635 1e-05 61.96 102.1 37.9      .3538   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                  10303 6051 9969 1.648\n",
            "\n",
            "21:38:09 | time:4692s total_exs:175316 epochs:0.23\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5971 15750 33.22  340             16384  19.73    .3681 3.613 1e-05 67.22 177.3 37.06      .3785   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10330 6038 15927 2.638\n",
            "\n",
            "21:38:19 | time:4702s total_exs:175516 epochs:0.23\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6149  9852 20.03  200             16384  19.05    .3761 3.697 1e-05 73.25 117.4 40.31      .3677   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                  10346 6222 9969 1.602\n",
            "\n",
            "21:38:30 | time:4712s total_exs:175788 epochs:0.23\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5974 12146 26.33  272             16384  19.75    .3681 3.643 1e-05 63.67 129.4 38.22      .3598   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10367 6038 12276 2.033\n",
            "\n",
            "21:38:40 | time:4723s total_exs:176092 epochs:0.23\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6153 14939 29.52  304             16384  19.67    .3681 3.596 1e-05 62.52 151.8 36.44      .3781   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10392 6216 15090 2.428\n",
            "\n",
            "21:38:50 | time:4733s total_exs:176444 epochs:0.23\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6023 16795 35.05  352             16384  20.94    .3761 3.634 1e-05 54.82 152.9 37.87      .3785   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10420 6078 16948 2.788\n",
            "\n",
            "21:38:59 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:39:05 | time:4748s total_exs:176740 epochs:0.23\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6062  9850 20.04  296             16384  20.29    .3681 3.434 1e-05 52.12  84.7 31.01      .4077   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                  10444 6114 9935 1.625\n",
            "\n",
            "21:39:15 | time:4758s total_exs:177080 epochs:0.23\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6034 15935 33.26  340             16384  20.91    .3681 3.743 1e-05 58.78 155.2 42.24      .3623   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10471 6092 16091 2.641\n",
            "\n",
            "21:39:25 | time:4768s total_exs:177300 epochs:0.23\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6108 10917 21.84  220             16384  19.42    .3681 3.554 1e-05 56.44 100.9 34.96      .3780   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10489 6164 11018 1.788\n",
            "\n",
            "21:39:36 | time:4778s total_exs:177612 epochs:0.24\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  5761 12220 30.08  312             16384  19.07    .3681 3.745 1e-05 67.23 142.6 42.3      .3556   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10511 5828 12363 2.121\n",
            "\n",
            "21:39:46 | time:4789s total_exs:177904 epochs:0.24\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6162 14727 29.08  292             16384  20.44    .3681 3.561 1e-05 57.96 138.5 35.21      .3868   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                  10535 6220 14866 2.39\n",
            "\n",
            "21:39:56 | time:4799s total_exs:178244 epochs:0.24\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6162 16741 32.99  340             16384  19.69    .3681 3.791 1e-05 63.89 173.6 44.3      .3505   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10563 6226 16915 2.717\n",
            "\n",
            "21:40:05 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:40:11 | time:4814s total_exs:178548 epochs:0.24\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6104  9817 20.37  304             16384  20.33    .3972 3.818 1e-05    64 102.9 45.53      .3509   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                  10587 6168 9920 1.608\n",
            "\n",
            "21:40:22 | time:4824s total_exs:178892 epochs:0.24\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6059 15297  33.4  344             16384  18.55    .4123 3.812 1e-05 70.81 178.8 45.23      .3552   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10613 6130 15476 2.525\n",
            "\n",
            "21:40:32 | time:4834s total_exs:179120 epochs:0.24\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144 11539 22.53  228             16384  20.82    .3681 3.695 1e-05 54.74 102.8 40.25      .3712   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10632 6199 11641 1.878\n",
            "\n",
            "21:40:42 | time:4845s total_exs:179376 epochs:0.24\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6019 11861 25.22  256             16384  19.64    .3681 3.641 1e-05 64.55 127.2 38.11      .3617   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10652 6084 11988 1.971\n",
            "\n",
            "21:40:52 | time:4855s total_exs:179724 epochs:0.24\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5575 13302 34.59  348             16384  18.43    .3681 3.295 1e-05 58.54 139.7 26.96      .4270   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10676 5633 13441 2.386\n",
            "\n",
            "21:41:02 | time:4865s total_exs:180088 epochs:0.24\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5915 16208 35.62  364             16384   19.5    .4031 3.553 1e-05 60.04 164.5 34.92      .4021   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                  10704 5975 16372 2.74\n",
            "\n",
            "21:41:11 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:41:17 | time:4880s total_exs:180404 epochs:0.24\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6006  9988 21.02  316             16384  20.41    .3766 3.412 1e-05 52.08 86.61 30.33      .4071   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10729 6058 10075 1.663\n",
            "\n",
            "21:41:24 | Overflow: setting loss scale to 16384.0\n",
            "21:41:28 | time:4890s total_exs:180728 epochs:0.24\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9630  6144 16182 31.61  324             16384  20.57    .3681 3.543 1e-05 47.19 124.3 34.58      .3838   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10756 6191 16307 2.634\n",
            "\n",
            "21:41:38 | time:4901s total_exs:180956 epochs:0.24\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5942 10375 22.12  228             16384  19.22    .3681 3.641 1e-05 64.61 112.8 38.12      .3758   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10774 6006 10488 1.746\n",
            "\n",
            "21:41:48 | time:4911s total_exs:181216 epochs:0.24\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5676 10544 25.42  260             16384   19.3    .3681 3.786 1e-05 67.95 126.2 44.08      .3486   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10793 5744 10671 1.858\n",
            "\n",
            "21:41:59 | time:4921s total_exs:181492 epochs:0.24\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5972 13066 27.45  276             16384  20.27    .3681 3.628 1e-05 57.09 124.9 37.65      .3822   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10815 6029 13191 2.188\n",
            "\n",
            "21:42:09 | time:4931s total_exs:181832 epochs:0.24\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6143 16805 33.22  340             16384  19.92    .3681 3.479 1e-05 59.68 163.3 32.42      .3902   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10843 6202 16968 2.736\n",
            "\n",
            "21:42:17 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:42:24 | time:4946s total_exs:182144 epochs:0.24\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5828  9462 21.11  312             16384  19.65    .3681 3.442 1e-05 52.92 85.91 31.23      .4087   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                  10867 5881 9548 1.624\n",
            "\n",
            "21:42:34 | time:4956s total_exs:182460 epochs:0.24\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6129 15707 31.14  316             16384  20.16    .3681 3.637 1e-05 52.46 134.4 37.98      .3915   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10893 6182 15842 2.563\n",
            "\n",
            "21:42:44 | time:4967s total_exs:182684 epochs:0.24\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6147 10952 22.17  224             16384  19.05    .3681 3.543 1e-05 64.22 114.4 34.56      .3746   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10911 6211 11067 1.782\n",
            "\n",
            "21:42:54 | time:4977s total_exs:182952 epochs:0.24\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6184 13436 26.47  268             16384  19.81    .4094 3.715 1e-05 59.82   130 41.04      .3640   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10933 6243 13566 2.173\n",
            "\n",
            "21:43:04 | time:4987s total_exs:183240 epochs:0.24\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144 14725 28.76  288             16384  19.42    .3681 3.654 1e-05 58.88 141.1 38.63      .3836   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10957 6203 14866 2.397\n",
            "\n",
            "21:43:14 | time:4997s total_exs:183588 epochs:0.24\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5979 16700 34.72  348             16384  20.03    .3681 3.677 1e-05 61.29 171.2 39.54      .3788   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  10985 6040 16871 2.793\n",
            "\n",
            "21:43:23 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:43:29 | time:5012s total_exs:183916 epochs:0.24\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5976 10340 21.83  328             16384   19.3    .3681 3.584 1e-05 58.27 100.8 36.02      .3848   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                  11011 6034 10441 1.73\n",
            "\n",
            "21:43:40 | time:5023s total_exs:184264 epochs:0.24\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6058 15612 33.21  348             16384   19.1    .4031 3.529 1e-05  60.3 155.4 34.09      .3870   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11038 6119 15767 2.577\n",
            "\n",
            "21:43:50 | time:5033s total_exs:184480 epochs:0.24\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6004 10201 21.59  216             16384  20.19    .3681 3.577 1e-05 58.59 99.54 35.75      .3835   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11055 6063 10301 1.699\n",
            "\n",
            "21:44:00 | time:5043s total_exs:184756 epochs:0.24\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144 13688 26.73  276             16384  20.48    .3681 3.441 1e-05 55.91 124.6 31.22      .3958   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11078 6200 13812 2.228\n",
            "\n",
            "21:44:11 | time:5053s total_exs:185060 epochs:0.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6051 14342 30.02  304             16384  19.88    .3681 3.536 1e-05 63.08 149.5 34.34      .3719   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                  11102 6114 14491 2.37\n",
            "\n",
            "21:44:21 | time:5064s total_exs:185412 epochs:0.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6017 16355 34.17  352             16384  20.33    .3681 3.703 1e-05 60.46 164.4 40.55      .3686   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11130 6077 16519 2.718\n",
            "\n",
            "21:44:29 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:44:36 | time:5079s total_exs:185696 epochs:0.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6116  9381 18.94  284             16384  20.43    .3762 3.459 1e-05 61.48 94.29 31.78      .3876   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                  11153 6178 9475 1.534\n",
            "\n",
            "21:44:46 | time:5089s total_exs:186024 epochs:0.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5942 15501 32.91  328             16384  19.98    .3762 3.762 1e-05 63.04 164.4 43.04      .3569   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11179 6005 15666 2.609\n",
            "\n",
            "21:44:56 | time:5099s total_exs:186244 epochs:0.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6201 10980 21.64  220             16384  20.89    .4094 3.375 1e-05  49.5 87.65 29.21      .4063   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11197 6250 11068 1.771\n",
            "\n",
            "21:45:07 | time:5109s total_exs:186516 epochs:0.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6093 12734 25.84  272             16384  19.83    .3681 3.496 1e-05 52.73 110.2 32.98      .4034   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                  11219 6146 12845 2.09\n",
            "\n",
            "21:45:17 | time:5119s total_exs:186860 epochs:0.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5406 12451 34.44  344             16384  19.19    .3681 3.785 1e-05 70.35   162 44.03      .3690   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11242 5477 12613 2.303\n",
            "\n",
            "21:45:27 | time:5130s total_exs:187228 epochs:0.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5924 16101 35.72  368             16384  19.69    .3681 3.618 1e-05    63 171.2 37.27      .3713   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11270 5987 16272 2.718\n",
            "\n",
            "21:45:36 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:45:42 | time:5145s total_exs:187520 epochs:0.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6146  9882 19.56  292             16384  20.46    .3762 3.568 1e-05 62.17 99.95 35.45      .3747   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                  11294 6209 9982 1.608\n",
            "\n",
            "21:45:52 | time:5155s total_exs:187840 epochs:0.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6131 15583 31.28  320             16384  20.17    .3715 3.716 1e-05 63.12 160.4 41.08      .3601   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11320 6194 15743 2.543\n",
            "\n",
            "21:46:03 | time:5165s total_exs:188080 epochs:0.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  5892 11029 23.64  240             16384  19.62    .3762 3.747 1e-05 66.16 123.8 42.4      .3580   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11339 5958 11153 1.872\n",
            "\n",
            "21:46:13 | time:5176s total_exs:188360 epochs:0.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6162 13693 27.05  280             16384  20.54    .3932 3.535 1e-05 60.39 134.2 34.3      .3952   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11362 6222 13827 2.222\n",
            "\n",
            "21:46:24 | time:5186s total_exs:188616 epochs:0.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5971 11499 24.65  256             16384  20.69    .4031 3.735 1e-05  58.1 111.9 41.87      .3752   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11382 6029 11611 1.926\n",
            "\n",
            "21:46:34 | time:5196s total_exs:188948 epochs:0.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6055 15912 32.31  332             16384  19.61    .3682 3.655 1e-05 62.41   164 38.68      .3632   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11409 6117 16076 2.628\n",
            "\n",
            "21:46:42 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:46:48 | time:5211s total_exs:189228 epochs:0.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6101  9782 19.52  280             16384  20.03    .3681 3.316 1e-05 49.43 79.27 27.54      .4230   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                  11432 6150 9862 1.604\n",
            "\n",
            "21:46:58 | time:5221s total_exs:189592 epochs:0.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5814 15378 35.66  364             16384  19.39    .3681 3.484 1e-05 56.93 150.6 32.58      .4034   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11459 5871 15529 2.645\n",
            "\n",
            "21:47:09 | time:5231s total_exs:189772 epochs:0.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144  9200 17.97  180             16384  20.38    .3682 3.435 1e-05 49.27 73.77 31.05      .3884   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                  11474 6193 9274 1.498\n",
            "\n",
            "21:47:19 | time:5242s total_exs:190068 epochs:0.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5800 12356 28.66  296             16384  19.83    .3681 3.739 1e-05 71.41 152.1 42.06      .3590   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                  11496 5871 12508 2.13\n",
            "\n",
            "21:47:29 | time:5252s total_exs:190356 epochs:0.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144 14645  28.6  288             16384  20.82    .3682 3.714 1e-05 55.46 132.2 41.01      .3674   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11520 6199 14778 2.384\n",
            "\n",
            "21:47:36 | Overflow: setting loss scale to 16384.0\n",
            "21:47:39 | time:5262s total_exs:190708 epochs:0.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "   .9643  5997 16528 34.64  352             16384  18.48    .3682 3.809 1e-05 69.57 191.7 45.1      .3501   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11548 6067 16720 2.756\n",
            "\n",
            "21:47:48 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:47:54 | time:5277s total_exs:190996 epochs:0.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144  9888 19.31  288             16384  19.87    .3682 3.641 1e-05 58.21 93.68 38.12      .3822   \n",
            "    total_train_updates  tpb  tps  ups  \n",
            "                  11572 6202 9982 1.61\n",
            "\n",
            "21:48:05 | time:5287s total_exs:191312 epochs:0.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6129 15394 30.52  316             16384  20.49    .3762 3.559 1e-05 55.04 138.2 35.13      .3781   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11598 6184 15532 2.512\n",
            "\n",
            "21:48:15 | time:5297s total_exs:191588 epochs:0.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144 14026 27.39  276             16384  19.43    .3682 3.528 1e-05 56.61 129.2 34.04      .3955   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11621 6201 14155 2.283\n",
            "\n",
            "21:48:25 | time:5308s total_exs:191888 epochs:0.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  5996 13699  29.8  300             16384  19.09    .4124 3.514 1e-05 65.57 149.8 33.6      .3919   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11644 6062 13849 2.285\n",
            "\n",
            "21:48:35 | time:5318s total_exs:192176 epochs:0.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6137 14529 28.41  288             16384  20.18    .3682 3.629 1e-05 59.38 140.6 37.66      .3768   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11668 6196 14670 2.368\n",
            "\n",
            "21:48:45 | time:5328s total_exs:192464 epochs:0.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144 14636 28.59  288             16384  20.57    .3682 3.319 1e-05    49 116.7 27.62      .4260   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11692 6193 14753 2.383\n",
            "\n",
            "21:48:54 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:49:00 | time:5343s total_exs:192740 epochs:0.26\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6119  9367 18.37  276             16384  19.99    .3762 3.786 1e-05 65.04 99.57 44.1      .3529   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                  11715 6184 9466 1.531\n",
            "\n",
            "21:49:11 | time:5353s total_exs:193064 epochs:0.26\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6136 16178 31.64  324             16384  19.95    .3682 3.696 1e-05 61.41 161.9 40.3      .3685   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11742 6197 16340 2.637\n",
            "\n",
            "21:49:21 | time:5363s total_exs:193288 epochs:0.26\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5933 10094 22.42  224             16384  19.48    .3682 3.429 1e-05 56.94 96.87 30.83      .4153   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11759 5990 10191 1.701\n",
            "\n",
            "21:49:31 | Overflow: setting loss scale to 16384.0\n",
            "21:49:31 | time:5374s total_exs:193564 epochs:0.26\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9524  5904 11946 26.59  276             16384  18.83    .3682 3.633 1e-05 58.19 117.7 37.83      .3813   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11780 5963 12064 2.023\n",
            "\n",
            "21:49:42 | time:5384s total_exs:193880 epochs:0.26\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5957 14406 30.57  316             16384  20.37    .3682 3.565 1e-05 57.52 139.1 35.35      .3797   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11805 6014 14545 2.419\n",
            "\n",
            "21:49:52 | time:5394s total_exs:194244 epochs:0.26\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5915 15946 36.34  364             16384  19.66    .3682 3.467 1e-05 61.67 166.2 32.04      .3856   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11832 5977 16112 2.696\n",
            "\n",
            "21:50:00 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:50:07 | time:5409s total_exs:194548 epochs:0.26\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5998  9661  20.4  304             16384  19.55    .3682 3.571 1e-05 54.92 88.46 35.54      .3938   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                  11856 6053 9750 1.611\n",
            "\n",
            "21:50:17 | time:5419s total_exs:194900 epochs:0.26\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5906 15384 35.27  352             16384  19.92    .3682 3.719 1e-05 63.38 165.1 41.22      .3750   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11882 5969 15549 2.605\n",
            "\n",
            "21:50:27 | time:5430s total_exs:195160 epochs:0.26\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5719 10082 25.46  260             16384  18.92    .3689 3.499 1e-05 76.44 134.8 33.09      .3866   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11900 5795 10217 1.763\n",
            "\n",
            "21:50:37 | time:5440s total_exs:195416 epochs:0.26\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6022 11992 25.49  256             16384   19.9    .3944 3.462 1e-05    57 113.5 31.88      .3956   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11920 6079 12106 1.992\n",
            "\n",
            "21:50:47 | time:5450s total_exs:195704 epochs:0.26\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144 14771 28.85  288             16384  19.74    .3682 3.522 1e-05 56.58   136 33.84      .3940   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11944 6201 14907 2.404\n",
            "\n",
            "21:50:57 | time:5460s total_exs:196064 epochs:0.26\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6029 16511 35.21  360             16384  19.15    .3762 3.721 1e-05 64.89 177.7 41.29      .3605   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11972 6094 16689 2.739\n",
            "\n",
            "21:51:06 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:51:13 | time:5475s total_exs:196408 epochs:0.26\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5937 10059 22.42  344             16384  18.86    .3682 3.748 1e-05 78.46 132.9 42.42      .3525   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  11998 6015 10192 1.694\n",
            "\n",
            "21:51:23 | time:5486s total_exs:196772 epochs:0.26\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5880 15226 34.91  364             16384  18.75    .3682 3.583 1e-05 68.63 177.7 35.97      .3697   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                  12025 5948 15404 2.59\n",
            "\n",
            "21:51:33 | time:5496s total_exs:197052 epochs:0.26\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5743 10242 27.74  280             16384  17.81    .4032 3.359 1e-05 68.83 122.8 28.76      .4003   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  12043 5812 10365 1.784\n",
            "\n",
            "21:51:44 | time:5506s total_exs:197336 epochs:0.26\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6046 12574 28.12  284             16384  18.54    .3682 3.372 1e-05    63   131 29.15      .3953   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                  12064 6109 12705 2.08\n",
            "\n",
            "21:51:54 | time:5516s total_exs:197668 epochs:0.26\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  5836 13817 32.75  332             16384  19.06    .3682 3.453 1e-05 60.71 143.7 31.6      .4070   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  12088 5897 13961 2.368\n",
            "\n",
            "21:52:04 | time:5526s total_exs:198020 epochs:0.26\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6008 16227 35.21  352             16384  19.15    .3682 3.678 1e-05 67.78 183.1 39.58      .3557   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  12115 6075 16410 2.701\n",
            "\n",
            "21:52:12 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:52:19 | time:5541s total_exs:198336 epochs:0.26\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6000 10105 21.29  316             16384  20.14    .3682 3.435 1e-05 54.44 91.68 31.03      .4129   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  12140 6054 10197 1.684\n",
            "\n",
            "21:52:29 | time:5552s total_exs:198672 epochs:0.26\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6082 15319 32.55  336             16384  19.95    .3682 3.514 1e-05 63.46 159.8 33.59      .3903   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  12166 6145 15479 2.519\n",
            "\n",
            "21:52:39 | time:5562s total_exs:198888 epochs:0.26\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144 10869 21.23  216             16384  19.73    .3762 3.959 1e-05 74.33 131.5 52.42      .3438   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  12184 6218 11001 1.769\n",
            "\n",
            "21:52:50 | time:5572s total_exs:199168 epochs:0.26\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5880 12084  27.4  280             16384  19.48    .3682 3.615 1e-05 62.52 128.5 37.17      .3907   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  12205 5942 12212 2.055\n",
            "\n",
            "21:53:00 | time:5582s total_exs:199460 epochs:0.26\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5964 12866 28.63  292             16384  19.46    .3682 3.412 1e-05 55.18   119 30.31      .4012   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  12227 6019 12985 2.158\n",
            "\n",
            "21:53:10 | time:5593s total_exs:199816 epochs:0.26\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6140 16560 35.56  356             16384  19.06    .3682 3.571 1e-05 62.48 168.5 35.57      .3829   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  12254 6203 16728 2.697\n",
            "\n",
            "21:53:18 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:53:25 | time:5608s total_exs:200124 epochs:0.27\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6176  9902 20.58  308             16384  19.06    .3944 3.509 1e-05 60.79 97.47 33.42      .3859   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  12278 6237 10000 1.603\n",
            "\n",
            "21:53:35 | time:5618s total_exs:200468 epochs:0.27\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5951 15054 33.47  344             16384  19.78    .3682 3.473 1e-05 59.73 151.1 32.22      .3928   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                  12304 6011 15205 2.53\n",
            "\n",
            "21:53:45 | time:5628s total_exs:200688 epochs:0.27\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  5888  9953 21.87  220             16384  19.78    .3682 3.511 1e-05 56.94 96.24 33.5      .3926   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                  12321 5945 10049 1.69\n",
            "\n",
            "21:53:56 | time:5638s total_exs:200932 epochs:0.27\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6115 11903 23.75  244             16384  19.93    .3682 3.573 1e-05 60.15 117.1 35.61      .3874   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  12341 6176 12020 1.947\n",
            "\n",
            "21:54:06 | time:5649s total_exs:201240 epochs:0.27\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6064 14971 30.41  308             16384  20.66    .3682 3.464 1e-05 51.04   126 31.93      .3966   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  12366 6115 15097 2.469\n",
            "\n",
            "21:54:16 | time:5659s total_exs:201640 epochs:0.27\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5716 15455 38.63  400             16384  18.38    .3682 3.574 1e-05  70.5 190.6 35.67      .3825   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  12394 5786 15646 2.704\n",
            "\n",
            "21:54:25 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:54:31 | time:5674s total_exs:201916 epochs:0.27\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144  9570 18.69  276             16384  19.63    .3682 3.385 1e-05 50.04 77.95 29.53      .4075   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                  12417 6194 9648 1.558\n",
            "\n",
            "21:54:42 | time:5684s total_exs:202240 epochs:0.27\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144 15930 31.11  324             16384   20.1    .3682 3.705 1e-05 61.04 158.2 40.65      .3592   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  12444 6205 16088 2.593\n",
            "\n",
            "21:54:52 | time:5695s total_exs:202436 epochs:0.27\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6123  9503 19.01  196             16384  19.61    .3682 3.547 1e-05 60.38 93.71 34.69      .3758   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                  12460 6183 9597 1.552\n",
            "\n",
            "21:55:03 | time:5705s total_exs:202720 epochs:0.27\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5854 12328 27.18  284             16384  19.01    .3682 3.394 1e-05 60.36 127.1 29.78      .3833   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  12482 5914 12455 2.106\n",
            "\n",
            "21:55:13 | time:5715s total_exs:203032 epochs:0.27\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5741 13625 30.85  312             16384  20.26    .3682 3.503 1e-05 52.92 125.6 33.23      .4024   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  12506 5794 13751 2.373\n",
            "\n",
            "21:55:23 | time:5726s total_exs:203364 epochs:0.27\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6161 16458 32.84  332             16384  19.83    .3682 3.581 1e-05 60.41 161.4 35.9      .3844   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  12533 6222 16619 2.671\n",
            "\n",
            "21:55:31 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:55:37 | time:5740s total_exs:203664 epochs:0.27\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5911  9606  21.2  300             16384   19.6    .3682 3.697 1e-05 57.13 92.85 40.34      .3676   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                  12556 5968 9699 1.625\n",
            "\n",
            "21:55:47 | time:5750s total_exs:203988 epochs:0.27\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144 16165 31.57  324             16384  19.83    .3682 3.515 1e-05 58.04 152.7 33.61      .3778   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  12583 6202 16318 2.631\n",
            "\n",
            "21:55:58 | time:5760s total_exs:204228 epochs:0.27\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5822  9796 23.75  240             16384  19.52    .3682 3.385 1e-05 57.41 96.59 29.52      .4098   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                  12600 5880 9892 1.683\n",
            "\n",
            "21:56:08 | time:5770s total_exs:204472 epochs:0.27\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6156 12158 24.09  244             16384  20.69    .4059 3.375 1e-05  55.7   110 29.22      .3959   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  12620 6211 12268 1.975\n",
            "\n",
            "21:56:18 | time:5781s total_exs:204772 epochs:0.27\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6144 15098 29.49  300             16384  20.41    .3682 3.716 1e-05 64.08 157.5 41.1      .3539   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  12645 6208 15256 2.458\n",
            "\n",
            "21:56:28 | time:5791s total_exs:205104 epochs:0.27\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6086 15913 32.15  332             16384  19.51    .3682 3.508 1e-05 63.96 167.2 33.38      .3758   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  12672 6150 16080 2.615\n",
            "\n",
            "21:56:37 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:56:44 | time:5806s total_exs:205408 epochs:0.27\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5956  9414 20.02  304             16384  18.95    .3763 3.518 1e-05 64.83 102.5 33.72      .3907   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                  12696 6021 9517 1.581\n",
            "\n",
            "21:56:54 | time:5817s total_exs:205744 epochs:0.27\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6142 15968 32.35  336             16384  19.48    .3682 3.619 1e-05 61.48 159.8 37.28      .3783   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                  12723 6203 16128  2.6\n",
            "\n",
            "21:57:04 | time:5827s total_exs:205968 epochs:0.27\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6037 10843 22.35  224             16384  19.14    .3682 3.323 1e-05 56.44 101.4 27.74      .4124   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  12741 6093 10944 1.796\n",
            "\n",
            "21:57:14 | time:5837s total_exs:206216 epochs:0.27\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5937 11280  24.8  248             16384  18.93    .3763 3.651 1e-05 65.79   125 38.52      .3736   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                  12760 6003 11405  1.9\n",
            "\n",
            "21:57:24 | time:5847s total_exs:206484 epochs:0.27\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6147 13342 26.44  268             16384  19.13    .3682 3.657 1e-05    66 143.3 38.76      .3629   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  12782 6213 13486 2.171\n",
            "\n",
            "21:57:35 | time:5857s total_exs:206852 epochs:0.27\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5804 15832 35.85  368             16384   20.1    .3682 3.562 1e-05 60.07 163.8 35.25      .3841   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  12810 5864 15996 2.728\n",
            "\n",
            "21:57:43 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:57:50 | time:5872s total_exs:207180 epochs:0.27\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5824  9826 22.14  328             16384  19.94    .3683 3.615 1e-05 62.72 105.8 37.14      .3795   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                  12835 5886 9932 1.687\n",
            "\n",
            "21:58:00 | time:5883s total_exs:207508 epochs:0.27\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6120 15978 31.71  328             16384  19.61    .3682 3.601 1e-05 59.15 154.4 36.64      .3688   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  12862 6179 16132 2.611\n",
            "\n",
            "21:58:10 | time:5893s total_exs:207776 epochs:0.28\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5850 11534 26.41  268             16384  19.26    .3763 3.583 1e-05 67.75 133.6 35.98      .3860   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  12882 5918 11668 1.972\n",
            "\n",
            "21:58:20 | time:5903s total_exs:208032 epochs:0.28\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5967 11793  25.3  256             16384   19.2    .3683 3.643 1e-05 65.55 129.6 38.21      .3875   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  12902 6032 11923 1.977\n",
            "\n",
            "21:58:31 | time:5913s total_exs:208296 epochs:0.28\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5861 11618 26.17  264             16384  19.05    .3683 3.544 1e-05 67.15 133.1 34.62      .3768   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  12922 5928 11751 1.982\n",
            "\n",
            "21:58:41 | time:5924s total_exs:208612 epochs:0.28\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6161 15582 30.74  316             16384  18.92    .3683 3.485 1e-05 65.04 164.5 32.61      .3820   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  12948 6226 15746 2.529\n",
            "\n",
            "21:58:49 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:58:56 | time:5939s total_exs:208912 epochs:0.28\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6065  9769 20.13  300             16384  19.63    .3683 3.567 1e-05 52.38 84.36 35.41      .3978   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                  12972 6117 9853 1.611\n",
            "\n",
            "21:59:06 | time:5949s total_exs:209200 epochs:0.28\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144 14272 27.88  288             16384  20.15    .3682 3.314 1e-05 48.08 111.7 27.49      .4142   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  12996 6192 14384 2.323\n",
            "\n",
            "21:59:17 | time:5959s total_exs:209512 epochs:0.28\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  5987 14190 30.81  312             16384  18.99    .3683  3.44 1e-05 59.42 140.8 31.2      .3927   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                  13020 6047 14331 2.37\n",
            "\n",
            "21:59:27 | time:5970s total_exs:209836 epochs:0.28\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5831 13710 31.74  324             16384  19.28    .3683 3.473 1e-05 64.71 152.1 32.23      .3799   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13044 5896 13862 2.351\n",
            "\n",
            "21:59:37 | time:5980s total_exs:210112 epochs:0.28\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144 14104 27.55  276             16384   19.6    .3683 3.405 1e-05 53.78 123.5 30.13      .3897   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13067 6198 14228 2.296\n",
            "\n",
            "21:59:47 | time:5990s total_exs:210412 epochs:0.28\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6033 14103 29.22  300             16384  20.03    .3683 3.535 1e-05 57.75   135 34.3      .3947   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13091 6091 14238 2.338\n",
            "\n",
            "21:59:56 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "22:00:02 | time:6005s total_exs:210700 epochs:0.28\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5745  8197 19.56  288             16384  19.34    .3683 3.061 1e-05 55.33 78.94 21.34      .4372   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                  13112 5801 8276 1.427\n",
            "\n",
            "22:00:12 | time:6015s total_exs:211020 epochs:0.28\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6104 15757 31.77  320             16384  19.72    .3683 3.412 1e-05 57.46 148.3 30.32      .3902   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13138 6161 15906 2.582\n",
            "\n",
            "22:00:22 | time:6025s total_exs:211256 epochs:0.28\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5940 10522 23.22  236             16384  19.74    .3683 3.559 1e-05 65.11 115.3 35.13      .3814   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13156 6005 10637 1.771\n",
            "\n",
            "22:00:33 | time:6035s total_exs:211532 epochs:0.28\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5889 12199 27.22  276             16384  18.97    .3683 3.613 1e-05 72.81 150.8 37.08      .3656   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13177 5962 12349 2.071\n",
            "\n",
            "22:00:43 | time:6045s total_exs:211828 epochs:0.28\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5907 13437 29.27  296             16384  19.29    .3683 3.357 1e-05 57.52 130.8 28.71      .4036   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13200 5964 13568 2.275\n",
            "\n",
            "22:00:53 | time:6056s total_exs:212192 epochs:0.28\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5918 16291 35.78  364             16384  19.36    .3683 3.332 1e-05 55.89 153.8 27.99      .4185   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13228 5974 16444 2.753\n",
            "\n",
            "22:01:02 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "22:01:08 | time:6071s total_exs:212536 epochs:0.28\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5750  9373 22.43  344             16384  19.25    .3683 3.403 1e-05 65.32 106.5 30.06      .3889   \n",
            "    total_train_updates  tpb  tps  ups  \n",
            "                  13253 5815 9480 1.63\n",
            "\n",
            "22:01:19 | time:6081s total_exs:212896 epochs:0.28\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  5840 14950 35.44  360             16384  19.08    .3683 3.532 1e-05 66.65 170.6 34.2      .3907   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                  13279 5907 15120 2.56\n",
            "\n",
            "22:01:29 | time:6092s total_exs:213188 epochs:0.28\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5871 11924 28.24  292             16384  19.06    .3683 3.516 1e-05 65.43 132.9 33.65      .3945   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13300 5937 12057 2.031\n",
            "\n",
            "22:01:39 | time:6102s total_exs:213488 epochs:0.28\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6036 13494 29.16  300             16384  19.44    .3716 3.375 1e-05 59.13 132.2 29.23      .3985   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13323 6095 13626 2.236\n",
            "\n",
            "22:01:49 | time:6112s total_exs:213736 epochs:0.28\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6076 12233 24.96  248             16384  19.78    .3683 3.369 1e-05 52.85 106.4 29.05      .4040   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13343 6129 12339 2.013\n",
            "\n",
            "22:02:00 | time:6122s total_exs:214116 epochs:0.28\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5891 15819 37.79  380             16384  19.43    .3763 3.613 1e-05 69.11 185.6 37.07      .3896   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13370 5960 16005 2.685\n",
            "\n",
            "22:02:08 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "22:02:15 | time:6137s total_exs:214424 epochs:0.28\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6030  9612 20.45  308             16384  19.23    .3768 3.463 1e-05 59.04 94.11 31.92      .3888   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                  13394 6089 9706 1.594\n",
            "\n",
            "22:02:25 | time:6147s total_exs:214760 epochs:0.28\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5953 14737 33.27  336             16384  18.74    .3683 3.439 1e-05 66.24   164 31.15      .3859   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13419 6019 14901 2.476\n",
            "\n",
            "22:02:35 | time:6157s total_exs:214992 epochs:0.28\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6171 11732 23.21  232             16384  19.82    .3768  3.72 1e-05 62.32 118.5 41.25      .3691   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13438 6233 11850 1.901\n",
            "\n",
            "22:02:45 | time:6168s total_exs:215292 epochs:0.29\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5864 12722 29.58  300             16384  19.56    .3945 3.426 1e-05 59.68 129.5 30.76      .3960   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                  13460 5924 12851 2.17\n",
            "\n",
            "22:02:55 | time:6178s total_exs:215548 epochs:0.29\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5971 11731 25.15  256             16384  18.87    .3763 3.591 1e-05 69.05 135.7 36.27      .3903   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13480 6040 11867 1.965\n",
            "\n",
            "22:03:05 | time:6188s total_exs:215896 epochs:0.29\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5993 15843 34.07  348             16384  19.11    .3845 3.439 1e-05 57.41 151.8 31.14      .4026   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13507 6050 15995 2.644\n",
            "\n",
            "22:03:14 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "22:03:21 | time:6204s total_exs:216204 epochs:0.29\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6156  9948 19.91  308             16384  19.41    .3857 3.549 1e-05 57.72 93.28 34.78      .3791   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13532 6213 10041 1.616\n",
            "\n",
            "22:03:32 | time:6214s total_exs:216520 epochs:0.29\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  6136 15220 30.14  316             16384   19.6    .3857 3.463 1e-05 52.35 129.8 31.9      .4012   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13558 6188 15349 2.481\n",
            "\n",
            "22:03:42 | time:6225s total_exs:216752 epochs:0.29\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6205 11405 22.44  232             16384  20.12    .3998 3.698 1e-05 60.32 110.8 40.38      .3534   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13577 6266 11516 1.838\n",
            "\n",
            "22:03:53 | time:6235s total_exs:217024 epochs:0.29\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5990 11457 26.01  272             16384  18.97    .4059 3.585 1e-05  67.2 128.5 36.04      .3810   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13597 6057 11586 1.913\n",
            "\n",
            "22:04:03 | time:6245s total_exs:217320 epochs:0.29\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5982 12531 29.52  296             16384  19.24    .4125 3.502 1e-05 64.95   136 33.18      .4098   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13618 6047 12667 2.095\n",
            "\n",
            "22:04:13 | time:6255s total_exs:217676 epochs:0.29\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5920 15933 35.49  356             16384  19.23    .3683 3.655 1e-05 64.04 172.4 38.65      .3696   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13645 5984 16105 2.692\n",
            "\n",
            "22:04:21 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "22:04:27 | time:6270s total_exs:217988 epochs:0.29\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6008  9719 21.94  312             16384  17.97    .4033  3.62 1e-05 73.65 119.1 37.35      .3589   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                  13668 6082 9838 1.618\n",
            "\n",
            "22:04:37 | time:6280s total_exs:218304 epochs:0.29\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5776 12905  30.7  316             16384  18.82    .3683 3.403 1e-05 57.78 129.1 30.05      .4003   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13691 5833 13034 2.235\n",
            "\n",
            "22:04:47 | time:6290s total_exs:218592 epochs:0.29\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144 14816 28.94  288             16384  18.92    .3683 3.794 1e-05 73.12 176.3 44.42      .3499   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13715 6217 14992 2.412\n",
            "\n",
            "22:04:58 | time:6300s total_exs:218888 epochs:0.29\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  5978 13605 29.29  296             16384  19.35    .3683 3.411 1e-05 60.91 138.6 30.3      .3862   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13738 6039 13744 2.276\n",
            "\n",
            "22:05:08 | time:6310s total_exs:219192 epochs:0.29\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5958 13577 30.12  304             16384  17.97    .3683  3.54 1e-05 73.52 167.5 34.48      .3950   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13761 6032 13745 2.279\n",
            "\n",
            "22:05:18 | time:6321s total_exs:219532 epochs:0.29\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5878 13148 33.06  340             19946  17.68    .4125  3.41 1e-05 68.74 153.8 30.26      .3941   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13784 5947 13302 2.237\n",
            "\n",
            "22:05:25 | Overflow: setting loss scale to 16384.0\n",
            "22:05:27 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "22:05:33 | time:6335s total_exs:219836 epochs:0.29\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9565  5909  9331 20.87  304             29206  18.11    .3683 3.295 1e-05 58.48 92.35 26.99      .4104   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                  13807 5967 9423 1.579\n",
            "\n",
            "22:05:43 | time:6346s total_exs:220180 epochs:0.29\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6013 15439 32.71  344             16384  20.06    .3683 3.545 1e-05 58.78 150.9 34.64      .3825   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13834 6072 15590 2.568\n",
            "\n",
            "22:05:54 | time:6356s total_exs:220440 epochs:0.29\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5741 10005 25.17  260             16384  18.73    .3945  3.45 1e-05 62.94 109.7 31.51      .3954   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13852 5804 10115 1.743\n",
            "\n",
            "22:06:04 | time:6367s total_exs:220700 epochs:0.29\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5974 11726 25.52  260             16384   19.4    .3683 3.517 1e-05  61.9 121.5 33.69      .3893   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13872 6036 11848 1.963\n",
            "\n",
            "22:06:14 | time:6377s total_exs:220984 epochs:0.29\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5993 12912 27.81  284             16384  19.19    .3683 3.259 1e-05 55.36 119.3 26.02      .4212   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13894 6048 13031 2.155\n",
            "\n",
            "22:06:24 | time:6387s total_exs:221372 epochs:0.29\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5845 15589 38.32  388             16384  18.97    .3683 3.613 1e-05 67.85 180.9 37.08      .3772   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13921 5913 15770 2.667\n",
            "\n",
            "22:06:33 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "22:06:39 | time:6402s total_exs:221664 epochs:0.29\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5983  9563 20.29  292             16384  19.58    .3764 3.627 1e-05  65.3 104.4 37.59      .3722   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                  13944 6048 9667 1.598\n",
            "\n",
            "22:06:49 | time:6412s total_exs:222024 epochs:0.29\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6063 15705 34.54  360             16384  19.25    .3683 3.389 1e-05 55.22   143 29.64      .4199   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  13971 6118 15848 2.591\n",
            "\n",
            "22:07:00 | time:6422s total_exs:222216 epochs:0.29\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144  9608 18.77  192             16384  19.53    .3683 3.661 1e-05 63.94 99.99 38.88      .3705   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                  13987 6208 9708 1.564\n",
            "\n",
            "22:07:10 | time:6433s total_exs:222536 epochs:0.29\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5812 12335 30.87  320             16384  18.59    .3683 3.427 1e-05 67.45 143.2 30.78      .3969   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  14009 5879 12479 2.123\n",
            "\n",
            "22:07:20 | time:6443s total_exs:222848 epochs:0.30\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6089 15123 30.99  312             16384  19.67    .3683 3.483 1e-05 55.24 137.2 32.56      .4012   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  14034 6144 15260 2.484\n",
            "\n",
            "22:07:31 | time:6453s total_exs:223200 epochs:0.30\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6022 15919 34.46  352             16384  19.29    .3683  3.58 1e-05 66.07 174.7 35.87      .3761   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  14061 6088 16093 2.644\n",
            "\n",
            "22:07:39 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "22:07:45 | time:6468s total_exs:223484 epochs:0.30\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6141  9747  19.6  284             16384  19.22    .3764  3.47 1e-05 60.09 95.37 32.14      .3821   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                  14084 6201 9843 1.587\n",
            "\n",
            "22:07:56 | time:6478s total_exs:223844 epochs:0.30\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6003 15512 34.45  360             16384  18.46    .3683 3.507 1e-05 62.63 161.8 33.34      .3909   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  14111 6065 15674 2.585\n",
            "\n",
            "22:08:06 | time:6488s total_exs:224072 epochs:0.30\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144 11680 22.81  228             16384  18.87    .3683 3.517 1e-05 60.89 115.8 33.67      .3941   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  14130 6205 11796 1.901\n",
            "\n",
            "22:08:16 | time:6499s total_exs:224324 epochs:0.30\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144 12828 25.05  252             16384  19.31    .3683 3.559 1e-05 60.57 126.5 35.12      .3664   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  14151 6204 12954 2.088\n",
            "\n",
            "22:08:26 | time:6509s total_exs:224592 epochs:0.30\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6140 12845  26.7  268             16384  18.97    .3684 3.541 1e-05 61.57 128.8 34.49      .3898   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  14172 6201 12974 2.092\n",
            "\n",
            "22:08:36 | time:6519s total_exs:224920 epochs:0.30\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5971 15201 32.12  328             16384  18.91    .3764  3.48 1e-05    61 155.3 32.46      .3884   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  14198 6032 15356 2.546\n",
            "\n",
            "22:08:45 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "22:08:51 | time:6534s total_exs:225196 epochs:0.30\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144  9456 18.47  276             16384  19.44    .3683 3.604 1e-05    59  90.8 36.76      .3788   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                  14221 6203 9547 1.539\n",
            "\n",
            "22:09:01 | time:6544s total_exs:225528 epochs:0.30\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  5993 15343 32.69  332             16384  19.13    .3683 3.747 1e-05 70.46 180.4 42.4      .3543   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  14247 6063 15523 2.561\n",
            "\n",
            "22:09:12 | time:6554s total_exs:225784 epochs:0.30\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5709 10161 25.31  256             16384  18.39    .3764 3.479 1e-05    72 128.1 32.41      .3981   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                  14265 5781 10289 1.78\n",
            "\n",
            "22:09:22 | time:6564s total_exs:226048 epochs:0.30\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6144 13438 26.25  264             16384  19.25    .3684 3.509 1e-05 56.18 122.9 33.41      .3900   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  14287 6200 13561 2.187\n",
            "\n",
            "22:09:32 | time:6575s total_exs:226320 epochs:0.30\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5954 12297 26.75  272             16384  19.41    .3684 3.633 1e-05 59.38 122.6 37.81      .3721   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  14308 6013 12420 2.066\n",
            "\n",
            "22:09:42 | time:6585s total_exs:226648 epochs:0.30\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6134 16167 32.02  328             16384  19.58    .3816 3.422 1e-05 56.67 149.3 30.63      .3961   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  14335 6191 16316 2.636\n",
            "\n",
            "22:09:51 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "22:09:57 | time:6600s total_exs:226968 epochs:0.30\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5916  9554 21.53  320             16384   19.3    .3684 3.421 1e-05 58.46 94.41 30.61      .4056   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                  14359 5975 9649 1.615\n",
            "\n",
            "22:10:08 | time:6610s total_exs:227252 epochs:0.30\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6151 13862 27.82  284             16384  19.78    .3684 3.486 1e-05 59.35 133.7 32.66      .3890   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  14382 6211 13996 2.254\n",
            "\n",
            "22:10:18 | time:6621s total_exs:227580 epochs:0.30\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6006 14585 31.86  328             16384  18.85    .3684 3.395 1e-05 61.88 150.3 29.83      .3943   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  14407 6068 14735 2.429\n",
            "\n",
            "22:10:28 | time:6631s total_exs:227904 epochs:0.30\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  5866 13526 32.48  324             16384  19.83    .3684 3.544 1e-05 61.74 142.4 34.6      .3915   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  14430 5927 13668 2.306\n",
            "\n",
            "22:10:38 | time:6641s total_exs:228252 epochs:0.30\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5810 13874 34.62  348             16384  18.43    .3684  3.53 1e-05 71.46 170.6 34.13      .3913   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  14454 5882 14045 2.388\n",
            "\n",
            "22:10:48 | time:6651s total_exs:228560 epochs:0.30\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  6025 12880 29.93  308             16384  18.32    .3684 3.614 1e-05 69.05 147.6 37.12      .3706   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  14476 6094 13028 2.138\n",
            "\n",
            "22:10:57 | saving model checkpoint: /content/drive/MyDrive/chatbot_model/model.checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "22:11:03 | time:6666s total_exs:228844 epochs:0.30\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5916  8787 19.17  284             16384  20.02    .3684 3.424 1e-05    58 86.14 30.69      .4122   \n",
            "    total_train_updates  tpb  tps   ups  \n",
            "                  14498 5974 8873 1.485\n",
            "\n",
            "22:11:14 | time:6677s total_exs:229224 epochs:0.30\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  5849 15152 36.46  380             16384  18.14    .3858 3.375 1e-05 67.15   174 29.23      .4175   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                  14525 5916 15326 2.591\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-12-13afe375ff26>\", line 41, in <module>\n",
            "    label_turns='both',  # https://parl.ai/docs/core/teachers.html#parlai.core.teachers.ConversationTeacher\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/parlai/core/script.py\", line 108, in main\n",
            "    return cls._run_kwargs(kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/parlai/core/script.py\", line 74, in _run_kwargs\n",
            "    return cls._run_from_parser_and_opt(opt, parser)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/parlai/core/script.py\", line 89, in _run_from_parser_and_opt\n",
            "    return script.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/parlai/scripts/train_model.py\", line 789, in run\n",
            "    return self.train_loop.train()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/parlai/scripts/train_model.py\", line 679, in train\n",
            "    world.parley()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/parlai/core/worlds.py\", line 1160, in parley\n",
            "    acts = self.world.get_model_agent().batch_act([self._obs[i] for i in batch])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/parlai/core/torch_agent.py\", line 2006, in batch_act\n",
            "    output = self.train_step(batch)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/parlai/core/torch_generator_agent.py\", line 750, in train_step\n",
            "    self.update_params()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/parlai/core/torch_agent.py\", line 2141, in update_params\n",
            "    self.optimizer.step()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/parlai/utils/fp16.py\", line 372, in step\n",
            "    self.optimizer.step(closure)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py\", line 67, in wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/parlai/utils/fp16.py\", line 488, in step\n",
            "    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 739, in getmodule\n",
            "    f = getabsfile(module)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 708, in getabsfile\n",
            "    _filename = getsourcefile(object) or getfile(object)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 693, in getsourcefile\n",
            "    if os.path.exists(filename):\n",
            "  File \"/usr/lib/python3.7/genericpath.py\", line 19, in exists\n",
            "    os.stat(path)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9JwQut7wLgz"
      },
      "source": [
        "!ls /content/drive/MyDrive/chatbot_model/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}